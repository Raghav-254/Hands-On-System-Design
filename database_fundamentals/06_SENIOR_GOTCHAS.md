# Level 6: Senior "Gotcha" Questions

> These are the edge-case questions that distinguish senior engineers. They probe failure modes, distributed systems complexities, and operational realities.

---

## Table of Contents
1. [Connection & State Management](#1-connection--state-management)
2. [Distributed Systems Edge Cases](#2-distributed-systems-edge-cases)
3. [Data Integrity & Recovery](#3-data-integrity--recovery)
4. [Performance & Scaling](#4-performance--scaling)
5. [Operational Realities](#5-operational-realities)
6. [Deep Dive Answers](#6-deep-dive-answers)

---

## The Six "Gotcha" Questions

### 1. "What happens to your WebSocket connection if the server restarts?"

**Why This Is Tricky:**
- WebSocket connections are stateful
- Server restart = all connections dropped
- Clients may be in the middle of operations
- State stored in server memory is lost

**Key Points to Cover:**
```
1. CONNECTION HANDLING
   â€¢ Client must detect disconnect (onclose event)
   â€¢ Implement exponential backoff reconnection
   â€¢ Don't retry immediately (thundering herd)

2. STATE RECOVERY
   â€¢ Don't store critical state in WebSocket server memory
   â€¢ Use external store (Redis) for session state
   â€¢ Client should request "catch-up" on reconnect

3. MESSAGE DELIVERY
   â€¢ In-flight messages may be lost
   â€¢ Client should track last received message ID
   â€¢ Request missed messages on reconnect
   
   WHERE TO GET MISSED MESSAGES FROM?
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Option 1: Redis Message Buffer                                â”‚
   â”‚  â€¢ Store last N messages per user/channel in Redis List        â”‚
   â”‚  â€¢ On reconnect: Client sends lastMessageId                    â”‚
   â”‚  â€¢ Server replays messages after that ID from Redis            â”‚
   â”‚                                                                 â”‚
   â”‚  Option 2: Database (for persistence)                          â”‚
   â”‚  â€¢ Messages stored in DB with timestamp/sequence               â”‚
   â”‚  â€¢ On reconnect: Query messages WHERE id > lastMessageId       â”‚
   â”‚                                                                 â”‚
   â”‚  Option 3: Kafka (for ordered replay)                          â”‚
   â”‚  â€¢ Messages persisted in Kafka topic                           â”‚
   â”‚  â€¢ On reconnect: Consumer seeks to last offset + 1             â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   
   COMMON PATTERN:
   â€¢ Real-time delivery: Redis Pub/Sub (ephemeral)
   â€¢ Missed messages: Redis List or Database (persistent)
   â€¢ Never rely on WebSocket server memory!

4. GRACEFUL SHUTDOWN
   â€¢ Drain connections before shutdown
   â€¢ Send "server going down" message
   â€¢ Use rolling restarts with load balancer
```

**Sample Answer:**
> "When the server restarts, all WebSocket connections are terminated. Our clients detect this via the `onclose` event and implement exponential backoff reconnection to avoid thundering herd. We store session state in Redis, not in server memory, so no state is lost. On reconnect, the client sends its last-received message ID, and the server replays any missed messages from our Redis-backed message buffer. For deployments, we use rolling restartsâ€”the load balancer drains connections from one server before shutting it down."

---

### 2. "How do you handle out-of-order messages in a distributed system?"

**Why This Is Tricky:**
- Network delays cause reordering
- Multiple producers with different latencies
- Consumer parallelism adds more reordering
- Idempotency doesn't solve ordering

**Key Points to Cover:**
```
1. DETERMINE IF ORDER MATTERS
   â€¢ User profile updates: Order matters
   â€¢ Page view events: Order may not matter
   â€¢ Financial transactions: Order is CRITICAL

2. SOLUTIONS BY SCENARIO

   A. KAFKA PARTITION ORDERING
      â€¢ Same key â†’ same partition â†’ ordered
      â€¢ Trade-off: Limits parallelism

   B. SEQUENCE NUMBERS
      â€¢ Producer assigns monotonic sequence
      â€¢ Consumer buffers until gaps filled
      â€¢ Trade-off: Latency for ordering

   C. VECTOR CLOCKS / LAMPORT TIMESTAMPS
      â€¢ Track causality, not wall-clock time
      â€¢ Detect concurrent writes
      â€¢ Trade-off: Complexity

   D. LAST-WRITER-WINS (LWW)
      â€¢ Highest timestamp wins
      â€¢ Acceptable for some use cases
      â€¢ Trade-off: May lose updates

3. PRACTICAL IMPLEMENTATION
   â€¢ Buffer window (wait N ms for stragglers)
   â€¢ Sequence gaps trigger re-request or timeout
   â€¢ Dead letter queue for unrecoverable cases
```

**Sample Answer:**
> "First, I'd assess if ordering actually matters for the use case. For our order processing, we use Kafka with order_id as the partition keyâ€”all events for one order go to one partition and are processed in order. For cases where we can't control partitioning, we embed a sequence number in messages. The consumer maintains a buffer and only processes when sequences are contiguous. If a gap persists beyond a timeout, we either request retransmission or send to a dead letter queue for manual inspection."

---

### 3. "Your database primary fails. Walk me through what happens and what could go wrong."

**Why This Is Tricky:**
- Failure detection is imperfect
- Failover has multiple steps
- Data loss is possible
- Split-brain is possible

**Key Points to Cover:**
```
1. FAILURE DETECTION
   â€¢ Health checks from replicas/orchestrator
   â€¢ Timeout period before declaring dead
   â€¢ False positive: Network issue, not actual failure
   
2. LEADER ELECTION
   â€¢ Who decides? (Sentinel, Orchestrator, Consensus)
   â€¢ Which replica becomes primary?
   â€¢ Most up-to-date replica should win
   
3. FAILOVER EXECUTION
   â€¢ Promote replica to primary
   â€¢ Update DNS/routing
   â€¢ Redirect client connections
   
4. WHAT COULD GO WRONG

   A. SPLIT BRAIN
      â€¢ Old primary recovers, thinks it's still leader
      â€¢ Two primaries accepting writes
      â€¢ Data divergence disaster
      
      SOLUTION: Fencing tokens, STONITH (Shoot The Other Node In The Head)
   
   B. DATA LOSS (Async Replication)
      â€¢ Primary had commits not yet replicated
      â€¢ New primary is behind
      â€¢ Those transactions are GONE
      
      SOLUTION: Sync replication, or accept loss
   
   C. CLIENT STALE ROUTING
      â€¢ Clients cached old primary address
      â€¢ Continue sending to dead/old primary
      
      SOLUTION: ZooKeeper + Load Balancer (they do DIFFERENT things!)
      
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  ZOOKEEPER / etcd / Consul = SERVICE DISCOVERY                 â”‚
      â”‚  "Which instances exist and are healthy?"                      â”‚
      â”‚                                                                 â”‚
      â”‚  â€¢ Services REGISTER themselves on startup                     â”‚
      â”‚    â†’ "I am order-service-3 at IP 10.0.0.5, I'm alive!"        â”‚
      â”‚  â€¢ Health checks detect dead instances                         â”‚
      â”‚  â€¢ WATCH mechanism for instant notifications                   â”‚
      â”‚                                                                 â”‚
      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
      â”‚  LOAD BALANCER = TRAFFIC DISTRIBUTION                          â”‚
      â”‚  "Which of the 5 healthy instances should handle this request?"â”‚
      â”‚                                                                 â”‚
      â”‚  â€¢ Distributes requests across instances                       â”‚
      â”‚  â€¢ Algorithms: Round-robin, Least connections, Weighted        â”‚
      â”‚  â€¢ L4 (TCP) or L7 (HTTP) load balancing                       â”‚
      â”‚                                                                 â”‚
      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
      â”‚  HOW THEY WORK TOGETHER:                                       â”‚
      â”‚                                                                 â”‚
      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
      â”‚  â”‚ Service  â”‚â”€â”€â”€â”€â–ºâ”‚ZooKeeper â”‚â—„â”€â”€â”€â”€â”‚   Load Balancer      â”‚   â”‚
      â”‚  â”‚Instances â”‚     â”‚(registry)â”‚     â”‚                      â”‚   â”‚
      â”‚  â”‚ register â”‚     â”‚          â”‚     â”‚ "Give me list of     â”‚   â”‚
      â”‚  â”‚themselvesâ”‚     â”‚          â”‚     â”‚  healthy instances"  â”‚   â”‚
      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
      â”‚                                              â”‚                 â”‚
      â”‚                                              â–¼                 â”‚
      â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
      â”‚                   â”‚  Client request â†’ LB â†’ one of 5       â”‚   â”‚
      â”‚                   â”‚  healthy instances (round-robin, etc.) â”‚   â”‚
      â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
      â”‚                                                                 â”‚
      â”‚  EXAMPLE FLOW:                                                 â”‚
      â”‚  1. 5 instances of order-service register in ZooKeeper        â”‚
      â”‚  2. Instance-3 crashes, ZK detects via health check           â”‚
      â”‚  3. ZK removes instance-3 from registry                       â”‚
      â”‚  4. LB refreshes list from ZK, now routes to 4 instances      â”‚
      â”‚  5. Clients never notice (LB handles it!)                     â”‚
      â”‚                                                                 â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      
      TL;DR: ZooKeeper = "who is available?"
             Load Balancer = "distribute traffic to available ones"
   
   D. REPLICATION LAG VISIBILITY
      â€¢ New primary was behind
      â€¢ Clients see "old" data after failover
      
      SOLUTION: Check replica lag before promotion
```

**Sample Answer:**
> "When the primary fails, our orchestrator detects it via missed heartbeatsâ€”typically a 10-second timeout. It then initiates election: the replica with the lowest replication lag is promoted. We use synchronous replication to one replica, so we lose at most in-flight transactions. The new primary's address is updated in our service discovery, and clients reconnect. 

> The main risks are: (1) Split-brain if the old primary comes backâ€”we use fencing tokens to prevent it from accepting writes. (2) Stale client connectionsâ€”our clients have a 30-second connection timeout and retry with service discovery on failure. (3) Brief unavailability during promotionâ€”typically 15-30 seconds."

---

### 4. "You're seeing database connection pool exhaustion. How do you diagnose and fix it?"

**Why This Is Tricky:**
- Many possible root causes
- Symptoms can mislead
- Fix depends on root cause
- Can cascade into system-wide outage

**Key Points to Cover:**
```
1. SYMPTOMS
   â€¢ "Connection pool exhausted" errors
   â€¢ Request timeouts
   â€¢ Increasing latency
   â€¢ Thread/goroutine growth

2. DIAGNOSIS STEPS
   
   A. CHECK ACTIVE CONNECTIONS
      â€¢ SELECT * FROM pg_stat_activity;
      â€¢ Are they idle or active?
      
   B. IDENTIFY LONG QUERIES
      â€¢ Slow queries holding connections
      â€¢ SELECT query, state, wait_event FROM pg_stat_activity
        WHERE state != 'idle' ORDER BY query_start;
      
   C. CHECK FOR LEAKS
      â€¢ Connections not returned to pool
      â€¢ Code paths missing conn.close()
      â€¢ Exceptions bypassing cleanup
      
   D. MONITOR POOL METRICS
      â€¢ Pool size vs active connections
      â€¢ Wait time for connection
      â€¢ Connection creation rate

3. COMMON CAUSES & FIXES

   CAUSE: Slow queries holding connections
   FIX: Optimize queries, add indexes, set query timeout

   CAUSE: Connection leak in application
   FIX: Use try-finally or context managers, connection timeout
   
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  WHAT IS A CONNECTION LEAK?                                    â”‚
   â”‚                                                                 â”‚
   â”‚  = A connection is borrowed from pool but NEVER returned       â”‚
   â”‚                                                                 â”‚
   â”‚  HOW IT HAPPENS:                                                â”‚
   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
   â”‚  â”‚  conn = pool.getConnection();   // Borrow connection      â”‚ â”‚
   â”‚  â”‚  result = conn.query("SELECT..."); // Do work             â”‚ â”‚
   â”‚  â”‚  // EXCEPTION THROWN HERE! ğŸ’¥                             â”‚ â”‚
   â”‚  â”‚  conn.close();  // â† This line NEVER runs!                â”‚ â”‚
   â”‚  â”‚                                                           â”‚ â”‚
   â”‚  â”‚  // Connection is now "leaked" - pool thinks it's in use  â”‚ â”‚
   â”‚  â”‚  // but no one will ever return it                        â”‚ â”‚
   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
   â”‚                                                                 â”‚
   â”‚  CONSEQUENCE:                                                   â”‚
   â”‚  â€¢ Pool has 100 connections max                                â”‚
   â”‚  â€¢ Leak 1 connection per request                               â”‚
   â”‚  â€¢ After 100 requests â†’ pool exhausted!                        â”‚
   â”‚  â€¢ New requests wait forever for a connection â†’ app hangs     â”‚
   â”‚                                                                 â”‚
   â”‚  FIX: Always use try-finally or context managers               â”‚
   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
   â”‚  â”‚  # Python example                                         â”‚ â”‚
   â”‚  â”‚  with pool.getConnection() as conn:  # Auto-closes!      â”‚ â”‚
   â”‚  â”‚      result = conn.query("SELECT...")                     â”‚ â”‚
   â”‚  â”‚  # Connection returned even if exception occurs           â”‚ â”‚
   â”‚  â”‚                                                           â”‚ â”‚
   â”‚  â”‚  // Java example                                          â”‚ â”‚
   â”‚  â”‚  try (Connection conn = pool.getConnection()) {           â”‚ â”‚
   â”‚  â”‚      // Auto-closes in finally                            â”‚ â”‚
   â”‚  â”‚  }                                                        â”‚ â”‚
   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
   â”‚                                                                 â”‚
   â”‚  NOT ABOUT THREADS: It's about connection OBJECTS.             â”‚
   â”‚  (Though each connection may also consume a thread in some DBs)â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   CAUSE: Pool too small for load
   FIX: Increase pool size (but not too muchâ€”DB has limits!)

   CAUSE: N+1 query pattern
   FIX: Batch queries, use JOINs, DataLoader pattern

   CAUSE: Transaction held open during external call
   FIX: Keep transactions short, do I/O outside transaction

4. CONNECTION POOLING BEST PRACTICES
   â€¢ Use a dedicated connection pooler (PgBouncer)
   â€¢ Set connection timeout (e.g., 30 seconds)
   â€¢ Set statement timeout (e.g., 5 seconds)
   â€¢ Pool size = (core_count * 2) + effective_spindle_count
```

**Sample Answer:**
> "I'd start by checking `pg_stat_activity` to see what connections are doing. If most are `idle in transaction`, we have a code path holding transactions open too longâ€”likely doing HTTP calls inside a transaction block. If they're `active`, I'd check for slow queries and add appropriate indexes. 

> For the fix: (1) Set a statement_timeout to kill long queries. (2) Audit code for connection leaksâ€”ensure we're using context managers. (3) If it's traffic growth, use PgBouncer for transaction-mode poolingâ€”it can multiplex 1000s of app connections onto 100 DB connections. Long term, we might need read replicas to offload query traffic."

---

### 5. "How do you handle schema migrations in a zero-downtime deployment?"

**Why This Is Tricky:**
- Old and new code run simultaneously during deploy
- Schema changes can break old code
- Some changes require table locks
- Rollback may be needed

**Key Points to Cover:**
```
1. THE PROBLEM
   â€¢ Rolling deploy: Old and new app versions coexist
   â€¢ Database change must work for BOTH versions
   â€¢ Adding NOT NULL column breaks old code
   â€¢ Renaming column breaks old code

2. SAFE MIGRATION PATTERNS

   A. ADD COLUMN
      Safe: New code uses it, old code ignores it
      Risk: If NOT NULL, old inserts fail
      Pattern: Add nullable, backfill, then add constraint
   
   B. REMOVE COLUMN
      Safe: After all code stops using it
      Pattern: 
        1. Deploy code that doesn't use column
        2. Wait for rollout
        3. Drop column
   
   C. RENAME COLUMN
      Never safe to do directly
      Pattern:
        1. Add new column
        2. Deploy code that writes to both
        3. Backfill old column â†’ new column
        4. Deploy code that reads from new
        5. Deploy code that only writes new
        6. Drop old column
   
   D. CHANGE COLUMN TYPE
      Pattern: Similar to rename, create new column

3. ONLINE DDL
   â€¢ PostgreSQL: Most DDL takes AccessExclusive lock
   â€¢ Use pg_repack for online table rewrites
   â€¢ MySQL: Online DDL with ALGORITHM=INPLACE
   
4. ROLLBACK CONSIDERATIONS
   â€¢ Can old code run with new schema?
   â€¢ Keep migrations reversible
   â€¢ Feature flags to control rollout
```

**Sample Answer:**
> "We follow expand-contract pattern. For adding a column: (1) Add it as nullable with a default, no lock contention. (2) Deploy new code that writes to it. (3) Backfill historical data in batches. (4) Once verified, add NOT NULL constraint if neededâ€”but this requires a table scan on Postgres.

> For removing a column: (1) Deploy code that doesn't read it. (2) Deploy code that doesn't write it. (3) Only then drop the column. 

> For large table changes, we use gh-ost or pg-online-schema-change to avoid locking. We also keep migrations backward-compatible so we can rollback the application without schema rollback."

---

### 6. "Your cache is returning stale data after a database update. How do you debug and fix it?"

**Why This Is Tricky:**
- Race conditions between cache and DB
- Multiple sources of truth
- Distributed systems timing issues
- Fix might introduce new problems

**Key Points to Cover:**
```
1. COMMON CAUSES

   A. CACHE INVALIDATION RACE CONDITION
      Time:    T1          T2          T3          T4
      Thread1: Read DB     Write Cache
      Thread2:     Update DB    Invalidate Cache
      
      Result: Cache has old value from T1!
   
   B. CACHE POPULATED BEFORE TRANSACTION COMMIT
      Cache updated, transaction rolls back
      Cache has data that doesn't exist
   
   C. REPLICATION LAG
      Write to primary â†’ Read from replica â†’ Cache stale data
   
   D. MULTIPLE CACHE LAYERS
      CDN cached, application cache invalidated
      User sees CDN stale data

2. DEBUGGING STEPS
   â€¢ Add cache-debug header showing cache source + timestamp
   â€¢ Log cache operations with correlation ID
   â€¢ Compare cache value timestamp vs DB updated_at
   â€¢ Check for replication lag on read replicas

3. SOLUTIONS

   A. WRITE-THROUGH CACHE
      â€¢ Update DB and cache atomically
      â€¢ Cache always fresh after writes
   
   B. CACHE-ASIDE WITH VERSION
      â€¢ Store version/updated_at in cache
      â€¢ On read: compare with DB version
   
   C. CDC-BASED INVALIDATION
      â€¢ Debezium watches DB changes
      â€¢ Invalidates cache asynchronously
      â€¢ Guaranteed to catch all changes
   
   D. SHORT TTL AS SAFETY NET
      â€¢ Eventual consistency via expiration
      â€¢ Even if invalidation misses, TTL fixes it
   
   E. DELETE INSTEAD OF UPDATE
      â€¢ On DB write: delete cache key
      â€¢ Next read: cache miss â†’ fresh from DB
      â€¢ Avoids race of stale write

4. THE INFAMOUS DOUBLE-DELETE
   Pattern for cache-aside:
   1. Delete cache key
   2. Update database
   3. Delete cache key again (after short delay)
   
   Why: Handles race where read repopulates before write completes
```

**Sample Answer:**
> "I'd first verify it's actually staleâ€”check if the cache key has a timestamp and compare with the database record's updated_at. If the cache is newer than the DB update, we have a race condition.

> Root cause is usually: (1) Read-modify-write raceâ€”another thread read stale data and cached it after our invalidation. (2) We're reading from a replica that's behind.

> Fix: We switched to CDC-based invalidation using Debezium. The CDC event triggers cache deletion. Since CDC reads from the WAL, it sees all changes in commit order, eliminating races. We also use 'delete on write' instead of 'update on write' to avoid the race of caching stale data."

---

## Additional Senior Questions

### 7. "How do you ensure exactly-once processing in a distributed system?"

```
ANSWER FRAMEWORK:

1. IMPOSSIBILITY RESULT
   â€¢ True exactly-once is impossible in async distributed systems
   â€¢ We aim for "effectively exactly-once" via idempotency

2. STRATEGIES

   A. IDEMPOTENT OPERATIONS
      â€¢ Design operations that can be safely repeated
      â€¢ Increment counter â†’ Set counter (idempotent)
      
   B. IDEMPOTENCY KEYS
      â€¢ Client generates unique request ID
      â€¢ Server deduplicates by ID
      â€¢ Store processed IDs in DB (with TTL cleanup)
      
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  WHAT IS AN IDEMPOTENCY KEY?                                   â”‚
      â”‚                                                                 â”‚
      â”‚  = A unique ID sent by client to prevent duplicate processing  â”‚
      â”‚                                                                 â”‚
      â”‚  EXAMPLE: Payment API                                          â”‚
      â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
      â”‚                                                                 â”‚
      â”‚  Request:                                                       â”‚
      â”‚  POST /api/payments                                             â”‚
      â”‚  Headers:                                                       â”‚
      â”‚    Idempotency-Key: pay_a1b2c3d4e5f6    â† Client generates     â”‚
      â”‚  Body:                                                          â”‚
      â”‚    { "amount": 100, "to": "merchant_123" }                     â”‚
      â”‚                                                                 â”‚
      â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
      â”‚                                                                 â”‚
      â”‚  SCENARIO: Network timeout, client retries                     â”‚
      â”‚                                                                 â”‚
      â”‚  1. User clicks "Pay $100"                                     â”‚
      â”‚  2. Request sent with key: pay_a1b2c3d4e5f6                   â”‚
      â”‚  3. Server processes payment âœ…                                â”‚
      â”‚  4. Network timeout - client never gets response! ğŸ˜±           â”‚
      â”‚  5. Client retries with SAME key: pay_a1b2c3d4e5f6            â”‚
      â”‚  6. Server checks: "Already processed this key"               â”‚
      â”‚  7. Server returns CACHED response (no re-processing!)        â”‚
      â”‚  8. User charged only ONCE âœ…                                  â”‚
      â”‚                                                                 â”‚
      â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
      â”‚                                                                 â”‚
      â”‚  SERVER LOGIC:                                                  â”‚
      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
      â”‚  â”‚  def process_payment(idempotency_key, amount):            â”‚ â”‚
      â”‚  â”‚      # Check if already processed                         â”‚ â”‚
      â”‚  â”‚      existing = db.get(f"idem:{idempotency_key}")         â”‚ â”‚
      â”‚  â”‚      if existing:                                         â”‚ â”‚
      â”‚  â”‚          return existing  # Return cached result          â”‚ â”‚
      â”‚  â”‚                                                           â”‚ â”‚
      â”‚  â”‚      # Process new request                                â”‚ â”‚
      â”‚  â”‚      result = charge_user(amount)                         â”‚ â”‚
      â”‚  â”‚                                                           â”‚ â”‚
      â”‚  â”‚      # Store result with TTL (e.g., 24 hours)             â”‚ â”‚
      â”‚  â”‚      db.set(f"idem:{idempotency_key}", result, ttl=86400) â”‚ â”‚
      â”‚  â”‚      return result                                        â”‚ â”‚
      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
      â”‚                                                                 â”‚
      â”‚  KEY FORMAT EXAMPLES:                                          â”‚
      â”‚  â€¢ UUID: 550e8400-e29b-41d4-a716-446655440000                  â”‚
      â”‚  â€¢ Stripe style: req_abc123xyz                                 â”‚
      â”‚  â€¢ Custom: {user_id}_{action}_{timestamp}_{random}             â”‚
      â”‚                                                                 â”‚
      â”‚  USED BY: Stripe, PayPal, AWS, most payment/financial APIs    â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   
   C. TRANSACTIONAL OUTBOX
      â€¢ Write to DB + outbox in same transaction
      â€¢ Outbox reader marks as processed
      â€¢ Consumers check "already processed" table
   
   D. KAFKA TRANSACTIONS
      â€¢ Producer: enable.idempotence=true
      â€¢ Consumer: read_committed isolation
      â€¢ Exactly-once semantics within Kafka
```

### 8. "What happens when your Redis cache runs out of memory?"

```
EVICTION POLICIES (maxmemory-policy):

â€¢ noeviction: Return errors for writes (dangerous!)
â€¢ allkeys-lru: Evict least recently used keys
â€¢ volatile-lru: Evict LRU among keys with TTL
â€¢ allkeys-lfu: Evict least frequently used
â€¢ volatile-ttl: Evict keys with shortest TTL

WHAT TO DISCUSS:
â€¢ Monitor memory usage before it's critical
â€¢ Set maxmemory to 80% of available RAM
â€¢ Use allkeys-lru for cache use case
â€¢ Have alerting for eviction rate spikes
â€¢ Consider Redis Cluster for horizontal scaling
```

### 9. "A single slow query is taking down your database. What do you do?"

```
IMMEDIATE: 
â€¢ Identify: pg_stat_activity / SHOW PROCESSLIST
â€¢ Kill: pg_terminate_backend(pid) / KILL <id>
â€¢ Throttle: Connection limits per user/app

SHORT-TERM:
â€¢ Add statement_timeout (e.g., 30s)
â€¢ Query analysis: EXPLAIN ANALYZE
â€¢ Add missing indexes
â€¢ Rewrite query (avoid sequential scan)

LONG-TERM:
â€¢ Slow query logging (log_min_duration_statement)
â€¢ Query review in CI/CD
â€¢ Read replicas for analytics queries
â€¢ Separate OLTP and OLAP databases
```

### 10. "How do you handle distributed transactions across microservices?"

```
OPTIONS:

1. TWO-PHASE COMMIT (2PC)
   â€¢ Coordinator manages prepare/commit
   â€¢ Blocking; coordinator failure is problematic
   â€¢ Use only within single database cluster

2. SAGA PATTERN
   â€¢ Chain of local transactions
   â€¢ Compensating transactions for rollback
   â€¢ Eventually consistent
   
   CHOREOGRAPHY: Services react to events
   ORCHESTRATION: Central saga coordinator

3. OUTBOX + CDC
   â€¢ Each service writes to local DB + outbox
   â€¢ CDC publishes events
   â€¢ Other services eventually update
   
4. TCC (Try-Confirm/Cancel)
   â€¢ Try: Reserve resources
   â€¢ Confirm: Commit reservation
   â€¢ Cancel: Release reservation
   â€¢ Requires business logic changes
```

---

## Interview Tips

### When You Don't Know
```
DON'T: "I don't know"

DO: 
"I haven't encountered this specific scenario, but based on 
distributed systems principles, I'd approach it by:
1. [Apply relevant principle]
2. [Reason through trade-offs]
3. [Propose a solution]
4. [Acknowledge limitations]"
```

### Demonstrate Seniority
```
1. ASK CLARIFYING QUESTIONS
   â€¢ "What's the consistency requirement?"
   â€¢ "What's the acceptable downtime?"
   â€¢ "What's the scale we're designing for?"

2. DISCUSS TRADE-OFFS
   â€¢ Every solution has downsides
   â€¢ Senior engineers articulate them

3. REFERENCE REAL EXPERIENCE
   â€¢ "At my previous company, we faced..."
   â€¢ "We learned the hard way that..."

4. CONSIDER OPERATIONAL ASPECTS
   â€¢ Monitoring and alerting
   â€¢ Rollback procedures
   â€¢ Team capabilities
```

---

## Next Steps

Review the **[Quick Reference Card](QUICK_REFERENCE_CARD.md)** for a 1-page summary before your interview.

