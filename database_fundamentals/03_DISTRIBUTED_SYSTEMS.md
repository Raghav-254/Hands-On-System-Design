# Level 3: Distributed Systems (The Scalability Layer)

> When a single database server isn't enough, you enter the world of distributed systemsâ€”where network partitions, consistency trade-offs, and careful architectural decisions become critical.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ğŸ—ºï¸  HOW THIS LEVEL CONNECTS                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  LEVEL 3: DISTRIBUTED SYSTEMS â—„â”€â”€ YOU ARE HERE                             â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚  Scope: MULTI-NODE, scaling beyond one machine                              â”‚
â”‚  Focus: Replication, sharding, consistency trade-offs, failure handling    â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚                         BUILDS ON LEVEL 1 & 2                           â”‚â”‚
â”‚  â”‚                                                                         â”‚â”‚
â”‚  â”‚  Level 1 (Storage)         Level 3 (This)                               â”‚â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                               â”‚â”‚
â”‚  â”‚  WAL                  â”€â”€â”€â–º Replication USES WAL streaming!              â”‚â”‚
â”‚  â”‚  Pages/Buffer Pool    â”€â”€â”€â–º Each node has its own buffer pool            â”‚â”‚
â”‚  â”‚  B-Tree/LSM-Tree      â”€â”€â”€â–º Each shard uses same storage engine          â”‚â”‚
â”‚  â”‚                                                                         â”‚â”‚
â”‚  â”‚  Level 2 (Logic)           Level 3 (This)                               â”‚â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                               â”‚â”‚
â”‚  â”‚  MVCC (local)         â”€â”€â”€â–º Distributed MVCC (global timestamps)         â”‚â”‚
â”‚  â”‚  Isolation levels     â”€â”€â”€â–º Distributed transactions are HARDER          â”‚â”‚
â”‚  â”‚  Indexes              â”€â”€â”€â–º Global vs Local secondary indexes            â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚                         EXTENDS TO LEVEL 4                              â”‚â”‚
â”‚  â”‚                                                                         â”‚â”‚
â”‚  â”‚  Level 3 (This)            Level 4 (Real-time)                          â”‚â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                          â”‚â”‚
â”‚  â”‚  Replication          â”€â”€â”€â–º CDC taps into replication stream             â”‚â”‚
â”‚  â”‚  Sharding             â”€â”€â”€â–º Fan-out must consider shard locations        â”‚â”‚
â”‚  â”‚  Consistency          â”€â”€â”€â–º Real-time updates need causal ordering       â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                              â”‚
â”‚  WHEN DO YOU NEED THIS?                                                     â”‚
â”‚  â€¢ Single node can't handle the load (scale)                               â”‚
â”‚  â€¢ Need high availability (replicas for failover)                          â”‚
â”‚  â€¢ Users are globally distributed (geo-replication)                        â”‚
â”‚  â€¢ Data too large for one machine (sharding)                               â”‚
â”‚                                                                              â”‚
â”‚  APPLIES TO:                                                                â”‚
â”‚  âœ… SQL: PostgreSQL replication, Vitess, CockroachDB, Spanner              â”‚
â”‚  âœ… NoSQL: Cassandra, DynamoDB, MongoDB (designed for distributed)         â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Table of Contents
1. [Replication Strategies](#1-replication-strategies)
2. [Sharding (Horizontal Partitioning)](#2-sharding-horizontal-partitioning)
3. [CAP Theorem and PACELC](#3-cap-theorem-and-pacelc)
4. [Consensus and Coordination](#4-consensus-and-coordination)
5. [Conflict Resolution & Anti-Entropy](#5-conflict-resolution--anti-entropy)
6. [Interview Checklist](#6-interview-checklist)

---

## 1. Replication Strategies

### Why Replicate?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    REPLICATION GOALS                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  1. HIGH AVAILABILITY                                               â”‚
â”‚     â””â”€â”€ If primary fails, replica takes over                        â”‚
â”‚                                                                      â”‚
â”‚  2. READ SCALABILITY                                                â”‚
â”‚     â””â”€â”€ Distribute read load across multiple servers                â”‚
â”‚                                                                      â”‚
â”‚  3. GEOGRAPHIC DISTRIBUTION                                         â”‚
â”‚     â””â”€â”€ Reduce latency by placing replicas near users               â”‚
â”‚                                                                      â”‚
â”‚  4. DISASTER RECOVERY                                               â”‚
â”‚     â””â”€â”€ Replicas in different data centers survive outages          â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Synchronous vs Asynchronous Replication

#### Synchronous Replication

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SYNCHRONOUS REPLICATION                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  Client         Primary           Replica 1         Replica 2        â”‚
â”‚    â”‚               â”‚                  â”‚                â”‚             â”‚
â”‚    â”‚â”€â”€â”€â”€ WRITE â”€â”€â”€â–ºâ”‚                  â”‚                â”‚             â”‚
â”‚    â”‚               â”‚                  â”‚                â”‚             â”‚
â”‚    â”‚               â”‚â”€â”€ Replicate â”€â”€â”€â”€â–ºâ”‚                â”‚             â”‚
â”‚    â”‚               â”‚                  â”‚                â”‚             â”‚
â”‚    â”‚               â”‚â”€â”€ Replicate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚             â”‚
â”‚    â”‚               â”‚                  â”‚                â”‚             â”‚
â”‚    â”‚               â”‚â—„â”€â”€â”€â”€ ACK â”€â”€â”€â”€â”€â”€â”€â”€â”‚                â”‚             â”‚
â”‚    â”‚               â”‚                  â”‚                â”‚             â”‚
â”‚    â”‚               â”‚â—„â”€â”€â”€â”€ ACK â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚             â”‚
â”‚    â”‚               â”‚                  â”‚                â”‚             â”‚
â”‚    â”‚â—„â”€â”€â”€â”€ OK â”€â”€â”€â”€â”€â”€â”‚  (only after    â”‚                â”‚             â”‚
â”‚    â”‚               â”‚   ALL acks!)    â”‚                â”‚             â”‚
â”‚                                                                      â”‚
â”‚  PROPERTIES:                                                         â”‚
â”‚  âœ… Strong consistency (all replicas have same data)                 â”‚
â”‚  âœ… No data loss on failover                                         â”‚
â”‚  âŒ Higher write latency (wait for slowest replica)                  â”‚
â”‚  âŒ Reduced availability (if replica down, writes blocked)           â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Asynchronous Replication

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ASYNCHRONOUS REPLICATION                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  Client         Primary           Replica 1         Replica 2        â”‚
â”‚    â”‚               â”‚                  â”‚                â”‚             â”‚
â”‚    â”‚â”€â”€â”€â”€ WRITE â”€â”€â”€â–ºâ”‚                  â”‚                â”‚             â”‚
â”‚    â”‚               â”‚                  â”‚                â”‚             â”‚
â”‚    â”‚â—„â”€â”€â”€â”€ OK â”€â”€â”€â”€â”€â”€â”‚  (immediate!)    â”‚                â”‚             â”‚
â”‚    â”‚               â”‚                  â”‚                â”‚             â”‚
â”‚    â”‚               â”‚â”€â”€ Replicate â”€â”€â”€â”€â–ºâ”‚                â”‚             â”‚
â”‚    â”‚               â”‚        (background, best-effort)  â”‚             â”‚
â”‚    â”‚               â”‚â”€â”€ Replicate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚             â”‚
â”‚    â”‚               â”‚                  â”‚                â”‚             â”‚
â”‚                                                                      â”‚
â”‚  PROPERTIES:                                                         â”‚
â”‚  âœ… Low write latency (don't wait for replicas)                      â”‚
â”‚  âœ… High availability (replica failure doesn't block writes)         â”‚
â”‚  âŒ Potential data loss on failover (uncommitted writes on primary)  â”‚
â”‚  âŒ Replication lag (replicas may be seconds/minutes behind)         â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Semi-Synchronous Replication

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  SEMI-SYNCHRONOUS REPLICATION                        â”‚
â”‚                    (Quorum-based approach)                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  CONFIGURATION: 3 replicas, require ACK from 2 (majority)            â”‚
â”‚                                                                      â”‚
â”‚  Client         Primary           Replica 1         Replica 2        â”‚
â”‚    â”‚               â”‚                  â”‚                â”‚             â”‚
â”‚    â”‚â”€â”€â”€â”€ WRITE â”€â”€â”€â–ºâ”‚                  â”‚                â”‚             â”‚
â”‚    â”‚               â”‚                  â”‚                â”‚             â”‚
â”‚    â”‚               â”‚â”€â”€ Replicate â”€â”€â”€â”€â–ºâ”‚                â”‚             â”‚
â”‚    â”‚               â”‚                  â”‚                â”‚             â”‚
â”‚    â”‚               â”‚â”€â”€ Replicate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚             â”‚
â”‚    â”‚               â”‚                  â”‚                â”‚             â”‚
â”‚    â”‚               â”‚â—„â”€â”€â”€â”€ ACK â”€â”€â”€â”€â”€â”€â”€â”€â”‚                â”‚             â”‚
â”‚    â”‚               â”‚                  â”‚  (Replica 2    â”‚             â”‚
â”‚    â”‚â—„â”€â”€â”€â”€ OK â”€â”€â”€â”€â”€â”€â”‚  (1 ACK +        â”‚   may be slow) â”‚             â”‚
â”‚    â”‚               â”‚   primary = 2!)  â”‚                â”‚             â”‚
â”‚    â”‚               â”‚                  â”‚                â”‚             â”‚
â”‚    â”‚               â”‚â—„â”€â”€â”€â”€ ACK â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚             â”‚
â”‚    â”‚               â”‚       (arrives later, logged)     â”‚             â”‚
â”‚                                                                      â”‚
â”‚  PROPERTIES:                                                         â”‚
â”‚  âœ… Balance of consistency and latency                               â”‚
â”‚  âœ… Tolerates minority failures                                      â”‚
â”‚  âš ï¸  Quorum must be configured correctly                             â”‚
â”‚                                                                      â”‚
â”‚  QUORUM FORMULA:                                                     â”‚
â”‚  Write Quorum (W) + Read Quorum (R) > N (total replicas)             â”‚
â”‚  For strict consistency: W + R > N                                   â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Replication Topologies

```
SINGLE-LEADER (Primary-Replica)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PRIMARY  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ REPLICA  â”‚
â”‚ (writes) â”‚         â”‚ (reads)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ REPLICA  â”‚
                     â”‚ (reads)  â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

âœ… Simple consistency model
âœ… No write conflicts
âŒ Single point of failure for writes
âŒ All writes through one node (bottleneck)


MULTI-LEADER (Active-Active)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LEADER 1 â”‚â—„â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ LEADER 2 â”‚
â”‚  (DC-A)  â”‚         â”‚  (DC-B)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

âœ… Low latency writes in each DC
âœ… Writes continue if one DC fails
âŒ CONFLICT RESOLUTION REQUIRED
âŒ Complex (last-write-wins, merge, custom logic)


LEADERLESS (Dynamo-style)
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Node A  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â–²  â–²
       /    \
      /      \
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Node B â”‚â”€â”€â”‚ Node C â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜

âœ… No single point of failure
âœ… Any node can accept writes
âŒ Requires quorum reads/writes
âŒ Conflict resolution via vector clocks
```

### Handling Replication Lag

```
SCENARIO: User updates profile, then immediately views it

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                      â”‚
â”‚  User â”€â”€â”€â”€ UPDATE â”€â”€â”€â–º Primary â”€â”€â”€â”€ replicate â”€â”€â”€â–º Replica          â”‚
â”‚    â”‚                                                 (delayed)       â”‚
â”‚    â”‚                                                    â”‚            â”‚
â”‚    â””â”€â”€â”€â”€ READ (load balanced to replica) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                                      â”‚
â”‚  RESULT: User sees OLD data! ğŸ˜±                                      â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

SOLUTIONS:

1. READ YOUR WRITES (Read-after-write consistency)
   â€¢ After a write, read from primary for X seconds
   â€¢ Or track "last write timestamp" and route accordingly

2. MONOTONIC READS
   â€¢ User always reads from same replica
   â€¢ Prevents "going back in time"

3. CAUSAL CONSISTENCY  
   â€¢ Track happens-before relationships
   â€¢ Ensure causally-related writes seen in order

4. STICKY SESSIONS
   â€¢ Route user to same replica consistently
   â€¢ Simple but reduces load balancing flexibility
```

---

## 2. Sharding (Horizontal Partitioning)

### Why Shard?

```
SINGLE DATABASE LIMITS:
â€¢ Storage: Disk is finite (~10-100 TB practical limit)
â€¢ Write throughput: Single node can only handle so many IOPS
â€¢ RAM: Buffer pool limited to one machine's memory
â€¢ CPU: Query processing bound by one machine

SHARDING SOLUTION:
â€¢ Distribute data across multiple independent databases
â€¢ Each shard handles a subset of the data
â€¢ Scale horizontally by adding more shards
```

### Sharding Strategies

#### Range-Based Sharding

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RANGE-BASED SHARDING                              â”‚
â”‚              (Partition by value ranges)                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  Shard Key: user_id                                                  â”‚
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚   SHARD 1    â”‚ â”‚   SHARD 2    â”‚ â”‚   SHARD 3    â”‚                â”‚
â”‚  â”‚  user_id     â”‚ â”‚  user_id     â”‚ â”‚  user_id     â”‚                â”‚
â”‚  â”‚  1 - 1M      â”‚ â”‚  1M - 2M     â”‚ â”‚  2M - 3M     â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                                                                      â”‚
â”‚  âœ… Efficient range queries (scan single shard)                      â”‚
â”‚  âœ… Easy to understand and debug                                     â”‚
â”‚  âŒ HOTSPOT RISK: New users all go to latest shard                   â”‚
â”‚  âŒ Uneven distribution over time                                    â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Hash-Based Sharding

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    HASH-BASED SHARDING                               â”‚
â”‚              (Partition by hash of key)                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  shard_id = hash(user_id) % num_shards                               â”‚
â”‚                                                                      â”‚
â”‚  Example: user_id = 12345                                            â”‚
â”‚           hash(12345) = 8472391                                      â”‚
â”‚           8472391 % 3 = 1  â†’ Goes to Shard 1                         â”‚
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚   SHARD 0    â”‚ â”‚   SHARD 1    â”‚ â”‚   SHARD 2    â”‚                â”‚
â”‚  â”‚  user_ids:   â”‚ â”‚  user_ids:   â”‚ â”‚  user_ids:   â”‚                â”‚
â”‚  â”‚  3,6,9,12... â”‚ â”‚  1,4,7,10... â”‚ â”‚  2,5,8,11... â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                                                                      â”‚
â”‚  âœ… Even distribution (no hotspots)                                  â”‚
â”‚  âœ… Works with any key type                                          â”‚
â”‚  âŒ Range queries require scatter-gather                             â”‚
â”‚  âŒ Adding shards requires rehashing (unless consistent hashing)     â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Consistent Hashing

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CONSISTENT HASHING                                â”‚
â”‚           (Minimize reshuffling when adding/removing nodes)          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚                    Hash Ring (0 to 2^32-1)                           â”‚
â”‚                                                                      â”‚
â”‚                         0/MAX                                        â”‚
â”‚                          â”‚                                           â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”                                    â”‚
â”‚                   /             \                                    â”‚
â”‚                  /               \                                   â”‚
â”‚           Shard A                 Shard B                            â”‚
â”‚            (2^30)                  (2^31)                            â”‚
â”‚                  \               /                                   â”‚
â”‚                   \             /                                    â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                                    â”‚
â”‚                          â”‚                                           â”‚
â”‚                       Shard C                                        â”‚
â”‚                      (3*2^30)                                        â”‚
â”‚                                                                      â”‚
â”‚  Key placement: hash(key), find next shard clockwise                â”‚
â”‚                                                                      â”‚
â”‚  ADDING A NEW SHARD:                                                â”‚
â”‚  â€¢ Only keys between new shard and predecessor move                 â”‚
â”‚  â€¢ ~1/N of data migrates (not all data like simple hash mod)        â”‚
â”‚                                                                      â”‚
â”‚  VIRTUAL NODES:                                                      â”‚
â”‚  â€¢ Each physical shard has multiple positions on ring               â”‚
â”‚  â€¢ Improves load balancing                                          â”‚
â”‚  â€¢ Handles heterogeneous hardware                                   â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Choosing a Shard Key

```
GOOD SHARD KEY PROPERTIES:

1. HIGH CARDINALITY
   âœ… user_id (millions of values)
   âŒ country (only ~200 values)

2. EVEN DISTRIBUTION  
   âœ… user_id (users distributed evenly)
   âŒ celebrity_id (some have millions of followers)

3. QUERY ISOLATION
   âœ… user_id for user data (queries usually filter by user)
   âŒ status (queries rarely filter by status alone)

4. WRITE DISTRIBUTION
   âœ… user_id (writes spread across users)
   âŒ created_date (all new writes to "today" shard)
```

### The Hotspot Problem

```
SCENARIO: Social media with celebrity users

Shard Key: user_id
Celebrity Taylor Swift: user_id = 12345

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                      â”‚
â”‚  PROBLEM:                                                            â”‚
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚  â”‚  Shard 0   â”‚ â”‚  Shard 1   â”‚ â”‚  Shard 2   â”‚                       â”‚
â”‚  â”‚   10 QPS   â”‚ â”‚  10 QPS    â”‚ â”‚ 10,000 QPS â”‚ â† Taylor's shard!    â”‚
â”‚  â”‚            â”‚ â”‚            â”‚ â”‚  ğŸ”¥ğŸ”¥ğŸ”¥    â”‚                       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                                                                      â”‚
â”‚  SOLUTIONS:                                                          â”‚
â”‚                                                                      â”‚
â”‚  1. SHARD BY CONTENT, NOT USER                                       â”‚
â”‚     â€¢ Shard posts by post_id                                         â”‚
â”‚     â€¢ Taylor's posts spread across shards                            â”‚
â”‚     â€¢ But: Fetching "all Taylor's posts" is scatter-gather           â”‚
â”‚                                                                      â”‚
â”‚  2. COMPOUND SHARD KEY                                               â”‚
â”‚     â€¢ Shard by (user_id, timestamp)                                  â”‚
â”‚     â€¢ Taylor's posts spread by time                                  â”‚
â”‚     â€¢ Range queries by time still work                               â”‚
â”‚                                                                      â”‚
â”‚  3. APPLICATION-LEVEL CACHING                                        â”‚
â”‚     â€¢ Cache hot users in Redis                                       â”‚
â”‚     â€¢ Only cold reads hit the database                               â”‚
â”‚                                                                      â”‚
â”‚  4. READ REPLICAS FOR HOT SHARDS                                     â”‚
â”‚     â€¢ More replicas for shard 2                                      â”‚
â”‚     â€¢ Doesn't help write hotspots                                    â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Cross-Shard Operations

```
PROBLEM: JOINs and transactions across shards are expensive

QUERY: SELECT * FROM orders o 
       JOIN users u ON o.user_id = u.id 
       WHERE o.status = 'pending'

If orders and users are sharded by user_id:
âœ… Join within same shard (co-located data)

If orders sharded by order_id, users by user_id:
âŒ Every order needs a cross-shard lookup for user data!

SOLUTIONS:

1. CO-LOCATE RELATED DATA
   â€¢ Shard orders AND users by user_id
   â€¢ Trade-off: Some queries become scatter-gather

2. DENORMALIZATION
   â€¢ Store user name in orders table
   â€¢ Trade-off: Data duplication, update complexity

3. APPLICATION-LEVEL JOINS
   â€¢ Fetch orders, then batch fetch users
   â€¢ Trade-off: More round trips, application complexity

4. AVOID CROSS-SHARD TRANSACTIONS
   â€¢ Saga pattern for distributed transactions
   â€¢ Eventually consistent where possible
```

### SQL Sharding: Add-On vs Native

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SQL SHARDING OPTIONS                                      â”‚
â”‚              (Referenced from Level 2: Database Logic)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  DESIGN PHILOSOPHY:                                                          â”‚
â”‚  â€¢ Traditional SQL (PostgreSQL, MySQL): Single-node by default              â”‚
â”‚  â€¢ NoSQL (Cassandra, DynamoDB): Distributed by default                      â”‚
â”‚  â€¢ BUT: SQL CAN be sharded! It's just not the default.                      â”‚
â”‚                                                                              â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚  OPTION 1: APPLICATION-LEVEL SHARDING                                       â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚                                                                              â”‚
â”‚  Your application code decides which database to query:                     â”‚
â”‚                                                                              â”‚
â”‚    def get_user(user_id):                                                   â”‚
â”‚        shard_id = user_id % NUM_SHARDS                                      â”‚
â”‚        db = connections[f"shard_{shard_id}"]                                â”‚
â”‚        return db.query("SELECT * FROM users WHERE id = ?", user_id)         â”‚
â”‚                                                                              â”‚
â”‚  âœ… Pros: Full control, no middleware                                       â”‚
â”‚  âŒ Cons: Cross-shard queries are YOUR problem, resharding is painful       â”‚
â”‚                                                                              â”‚
â”‚  Used by: Instagram, Pinterest (early days)                                 â”‚
â”‚                                                                              â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚  OPTION 2: MIDDLEWARE / PROXY SHARDING                                      â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚                                                                              â”‚
â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                       â”‚
â”‚       â”‚    Sharding Middleware      â”‚                                       â”‚
â”‚       â”‚  (Vitess, Citus, ProxySQL)  â”‚                                       â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                       â”‚
â”‚             /        |        \                                              â”‚
â”‚            â†“         â†“         â†“                                             â”‚
â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                                      â”‚
â”‚       â”‚ MySQL  â”‚ â”‚ MySQL  â”‚ â”‚ MySQL  â”‚                                      â”‚
â”‚       â”‚ Shard1 â”‚ â”‚ Shard2 â”‚ â”‚ Shard3 â”‚                                      â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                      â”‚
â”‚                                                                              â”‚
â”‚  Tools:                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Vitess        â”‚ MySQL sharding (powers YouTube, Slack, GitHub)        â”‚  â”‚
â”‚  â”‚ Citus         â”‚ PostgreSQL extension for horizontal scaling           â”‚  â”‚
â”‚  â”‚ ProxySQL      â”‚ MySQL query routing and load balancing                â”‚  â”‚
â”‚  â”‚ PgBouncer     â”‚ PostgreSQL connection pooling (not sharding)          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                              â”‚
â”‚  âœ… Pros: App doesn't know about shards, some cross-shard support          â”‚
â”‚  âŒ Cons: Added complexity, potential single point of failure              â”‚
â”‚                                                                              â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚  OPTION 3: NEWSQL (Native Distributed SQL)                                  â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚                                                                              â”‚
â”‚  SQL databases DESIGNED for distribution from the start:                    â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ CockroachDB   â”‚ PostgreSQL-compatible, automatic sharding, ACID       â”‚  â”‚
â”‚  â”‚ Google Spannerâ”‚ Globally distributed, TrueTime, used by Google        â”‚  â”‚
â”‚  â”‚ YugabyteDB    â”‚ PostgreSQL & Cassandra compatible                     â”‚  â”‚
â”‚  â”‚ TiDB          â”‚ MySQL-compatible, from PingCAP                        â”‚  â”‚
â”‚  â”‚ PlanetScale   â”‚ Serverless MySQL (built on Vitess)                    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                              â”‚
â”‚  âœ… Pros: Full SQL, automatic sharding, distributed ACID transactions      â”‚
â”‚  âŒ Cons: Higher latency than single-node, operational complexity          â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### SQL vs NoSQL Sharding Comparison

| Aspect | Traditional SQL + Sharding | NoSQL (Cassandra) | NewSQL (CockroachDB) |
|--------|---------------------------|-------------------|---------------------|
| **Sharding** | Add-on (manual/middleware) | Native (built-in) | Native (built-in) |
| **Cross-shard JOINs** | Hard (scatter-gather) | Not supported | Supported |
| **Cross-shard TXNs** | Complex (2PC needed) | Limited | Full ACID |
| **Query flexibility** | Full SQL | Limited (CQL) | Full SQL |
| **Operational complexity** | High (you manage) | Medium | Medium |
| **When to use** | Existing SQL apps | Write-heavy, simple queries | Need SQL + scale |

---

## 3. CAP Theorem and PACELC

### CAP Theorem

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       CAP THEOREM                                    â”‚
â”‚    "In a distributed system, during a network partition,            â”‚
â”‚     you can only guarantee either Consistency or Availability"       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚                        â”‚ CONSISTENCY   â”‚                            â”‚
â”‚                        â”‚ (All nodes    â”‚                            â”‚
â”‚                        â”‚  see same     â”‚                            â”‚
â”‚                        â”‚  data)        â”‚                            â”‚
â”‚                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚                             /\                                       â”‚
â”‚                            /  \                                      â”‚
â”‚                           /    \                                     â”‚
â”‚                          /      \                                    â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   /        \   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚     â”‚ AVAILABILITY  â”‚â—„â”€â”˜          â””â”€â–ºâ”‚ PARTITION     â”‚              â”‚
â”‚     â”‚ (Every requestâ”‚                â”‚ TOLERANCE     â”‚              â”‚
â”‚     â”‚  gets a       â”‚                â”‚ (System works â”‚              â”‚
â”‚     â”‚  response)    â”‚                â”‚  despite      â”‚              â”‚
â”‚     â”‚               â”‚                â”‚  network      â”‚              â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚  splits)      â”‚              â”‚
â”‚                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                                                      â”‚
â”‚  YOU MUST HAVE P:                                                   â”‚
â”‚  Network partitions WILL happen. The real choice is C vs A.         â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### During a Partition: CP vs AP

```
SCENARIO: Network partition between DC-East and DC-West

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         PARTITION         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   DC-East   â”‚    â•â•â•â•â•â•â•â•³â•â•â•â•â•â•â•â•â•â•â•    â”‚   DC-West   â”‚
â”‚   Primary   â”‚                           â”‚   Replica   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


CP CHOICE (Consistency over Availability):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â€¢ DC-West becomes read-only or unavailable                         â”‚
â”‚  â€¢ Writes only accepted by DC-East (primary)                        â”‚
â”‚  â€¢ Users in DC-West see errors or stale data                        â”‚
â”‚  â€¢ When partition heals, data is consistent                         â”‚
â”‚                                                                      â”‚
â”‚  EXAMPLES: Traditional RDBMS, ZooKeeper, etcd                       â”‚
â”‚  USE WHEN: Financial transactions, inventory, anything where        â”‚
â”‚            wrong data is worse than no data                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


AP CHOICE (Availability over Consistency):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â€¢ Both DCs continue accepting reads AND writes                     â”‚
â”‚  â€¢ Users in both locations can work normally                        â”‚
â”‚  â€¢ Data diverges during partition                                   â”‚
â”‚  â€¢ When partition heals, CONFLICT RESOLUTION needed                 â”‚
â”‚                                                                      â”‚
â”‚  EXAMPLES: Cassandra, DynamoDB, CouchDB                             â”‚
â”‚  USE WHEN: Shopping carts, social media, anything where             â”‚
â”‚            availability is more important than perfect consistency  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### PACELC: The Complete Picture

```
CAP only talks about partition scenarios. 
PACELC asks: What about normal operation?

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         PACELC                                       â”‚
â”‚                                                                      â”‚
â”‚     IF                           ELSE                                â”‚
â”‚   Partition                  (normal operation)                      â”‚
â”‚      â”‚                             â”‚                                 â”‚
â”‚      â–¼                             â–¼                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”                      â”Œâ”€â”€â”€â”€â”€â”                              â”‚
â”‚   â”‚ A   â”‚  or                  â”‚ L   â”‚  or                          â”‚
â”‚   â”‚     â”‚                      â”‚     â”‚                              â”‚
â”‚   â”‚ C   â”‚                      â”‚ C   â”‚                              â”‚
â”‚   â””â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”˜                              â”‚
â”‚                                                                      â”‚
â”‚   P: Partition                  E: Else (normal)                    â”‚
â”‚   A: Availability               L: Latency                          â”‚
â”‚   C: Consistency                C: Consistency                      â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PACELC CLASSIFICATIONS:

PA/EL: High availability, low latency, eventual consistency
       Examples: DynamoDB, Cassandra
       "Be fast and available, consistency can wait"

PC/EC: Strong consistency always, sacrifice latency
       Examples: RDBMS with sync replication
       "Consistency is king, we'll wait for it"

PA/EC: Available during partition, but consistent normally
       Examples: MongoDB (default config)
       "Be consistent when we can, available when we must"

PC/EL: Consistent during partition, but fast normally
       Examples: PAXOS-based systems with local reads
       "Fast reads, but partition = unavailability"
```

### Database CAP/PACELC Classifications

| Database | CAP | PACELC | Trade-off Notes |
|----------|-----|--------|-----------------|
| **PostgreSQL** (single) | CA* | PC/EC | *No partition tolerance in single node |
| **PostgreSQL** (sync rep) | CP | PC/EC | Replica failure blocks writes |
| **MySQL** (async rep) | AP | PA/EL | Can lose committed writes |
| **Cassandra** | AP | PA/EL | Tunable consistency per query |
| **MongoDB** | CP | PC/EC | Default; can tune for AP |
| **DynamoDB** | AP | PA/EL | Eventual consistency default |
| **CockroachDB** | CP | PC/EC | Serializable everywhere |
| **Redis** (cluster) | AP | PA/EL | Async replication |

---

## 4. Consensus and Coordination

### Why Consensus Matters

```
SCENARIO: Primary database fails, need to elect new primary

Without consensus:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Replica A: "I should be primary!"                                  â”‚
â”‚  Replica B: "No, I should be primary!"                              â”‚
â”‚  Replica C: "I have the most recent data!"                          â”‚
â”‚                                                                      â”‚
â”‚  RESULT: Split brain ğŸ§ ğŸ’¥ğŸ§                                          â”‚
â”‚  Multiple nodes think they're primary                               â”‚
â”‚  Clients write to different "primaries"                             â”‚
â”‚  Data divergence and corruption!                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

With consensus (Raft/Paxos):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Nodes vote, majority agrees on ONE leader                          â”‚
â”‚  All writes go through single agreed leader                         â”‚
â”‚  No split brain possible                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Raft Consensus (Simplified)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RAFT CONSENSUS                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  THREE ROLES:                                                        â”‚
â”‚  â€¢ Leader: Handles all writes, sends heartbeats                      â”‚
â”‚  â€¢ Follower: Replicates from leader, votes in elections              â”‚
â”‚  â€¢ Candidate: Requesting votes to become leader                      â”‚
â”‚                                                                      â”‚
â”‚  LEADER ELECTION:                                                    â”‚
â”‚  1. Leader stops sending heartbeats (crashed or partitioned)         â”‚
â”‚  2. Followers timeout, become candidates                             â”‚
â”‚  3. Candidates request votes from all nodes                          â”‚
â”‚  4. Candidate with majority votes becomes new leader                 â”‚
â”‚  5. New leader starts sending heartbeats                             â”‚
â”‚                                                                      â”‚
â”‚  LOG REPLICATION:                                                    â”‚
â”‚  1. Client sends write to leader                                     â”‚
â”‚  2. Leader appends to its log, sends to followers                    â”‚
â”‚  3. Followers append to their logs, send ACK                         â”‚
â”‚  4. Once majority ACKs, leader commits                               â”‚
â”‚  5. Leader notifies followers to commit                              â”‚
â”‚  6. Client gets success response                                     â”‚
â”‚                                                                      â”‚
â”‚  SAFETY GUARANTEES:                                                  â”‚
â”‚  â€¢ At most one leader per term                                       â”‚
â”‚  â€¢ Leader's log is always "most complete"                            â”‚
â”‚  â€¢ Committed entries are never lost                                  â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Consensus Use Cases

```
1. LEADER ELECTION
   â€¢ Database primary failover
   â€¢ Kafka partition leader
   â€¢ Distributed lock services

2. CONFIGURATION MANAGEMENT  
   â€¢ Cluster membership (who's in/out)
   â€¢ Schema changes
   â€¢ Feature flags

3. DISTRIBUTED LOCKS
   â€¢ Exactly-once processing
   â€¢ Resource allocation
   â€¢ Fencing tokens

4. ATOMIC BROADCAST
   â€¢ Total ordering of messages
   â€¢ Replicated state machines
   â€¢ Distributed transactions (2PC/3PC)
```

---

## 5. Conflict Resolution & Anti-Entropy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ğŸ“ CONNECTION TO LEVEL 2 (MVCC)                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  In Level 2, we covered MVCC for SINGLE-NODE concurrency:                   â”‚
â”‚  â€¢ Transaction IDs (xmin/xmax) to track row versions                        â”‚
â”‚  â€¢ Visibility rules for snapshot isolation                                  â”‚
â”‚  â€¢ Readers never block writers on ONE server                                â”‚
â”‚                                                                              â”‚
â”‚  NOW WE EXTEND TO DISTRIBUTED:                                               â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                                              â”‚
â”‚  Single-Node MVCC              Distributed Challenge                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                         â”‚
â”‚  Local transaction ID    â†’     Need GLOBAL timestamp ordering               â”‚
â”‚  One copy of data        â†’     Multiple copies, may diverge                 â”‚
â”‚  Visibility = "committed?"â†’    Visibility = "committed on which nodes?"     â”‚
â”‚  No conflicts (one writer)â†’    Conflicts when multiple nodes write          â”‚
â”‚                                                                              â”‚
â”‚  DISTRIBUTED MVCC SOLUTIONS:                                                â”‚
â”‚  â€¢ Hybrid Logical Clocks (HLC) - CockroachDB, YugabyteDB                   â”‚
â”‚  â€¢ TrueTime (GPS + atomic clocks) - Google Spanner                         â”‚
â”‚  â€¢ Vector Clocks - Riak, Dynamo                                             â”‚
â”‚  â€¢ Conflict Resolution - LWW, CRDTs, application logic                     â”‚
â”‚                                                                              â”‚
â”‚  ğŸ‘‰ This section covers what happens when versions CONFLICT across nodes.   â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> When multiple nodes can accept writes, conflicts are inevitable. How do we detect and resolve them?

---

### The Conflict Problem

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    WRITE CONFLICT SCENARIO                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  Multi-leader or leaderless system (e.g., Cassandra, DynamoDB)              â”‚
â”‚                                                                              â”‚
â”‚  TIME    NODE A (US)              NODE B (EU)                                â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚  T1      User sets name="Alice"                                              â”‚
â”‚  T2                               User sets name="Alicia"                    â”‚
â”‚  T3      (network partition - nodes can't sync)                              â”‚
â”‚  T4      (partition heals - sync happens)                                    â”‚
â”‚                                                                              â”‚
â”‚  QUESTION: What is the user's name now? "Alice" or "Alicia"?                â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Conflict Resolution Strategies

#### 1. Last-Write-Wins (LWW)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LAST-WRITE-WINS (LWW)                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  RULE: The write with the highest timestamp wins.                           â”‚
â”‚                                                                              â”‚
â”‚  Node A: name="Alice",  timestamp=1000                                       â”‚
â”‚  Node B: name="Alicia", timestamp=1002  â† HIGHER, THIS WINS!                â”‚
â”‚                                                                              â”‚
â”‚  RESULT: name="Alicia"                                                       â”‚
â”‚                                                                              â”‚
â”‚  âœ… PROS:                                                                    â”‚
â”‚  â€¢ Simple to implement                                                       â”‚
â”‚  â€¢ No conflict resolution logic needed                                       â”‚
â”‚  â€¢ Deterministic (all nodes reach same result)                               â”‚
â”‚                                                                              â”‚
â”‚  âŒ CONS:                                                                    â”‚
â”‚  â€¢ DATA LOSS! The "Alice" write is silently discarded                       â”‚
â”‚  â€¢ Clock synchronization issues (what if Node A's clock is ahead?)          â”‚
â”‚  â€¢ Concurrent writes = random winner (depends on clock)                     â”‚
â”‚                                                                              â”‚
â”‚  USED BY: Cassandra (default), DynamoDB                                     â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 2. Vector Clocks (Detect Conflicts, Don't Auto-Resolve)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    VECTOR CLOCKS                                             â”‚
â”‚         "Track causality, detect concurrent writes"                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  CONCEPT: Each node maintains a counter. Vector = [NodeA:count, NodeB:count]â”‚
â”‚                                                                              â”‚
â”‚  EXAMPLE:                                                                    â”‚
â”‚                                                                              â”‚
â”‚  STEP 1: Initial write on Node A                                            â”‚
â”‚          name="Alice", vector=[A:1, B:0]                                     â”‚
â”‚                                                                              â”‚
â”‚  STEP 2: Sync to Node B                                                      â”‚
â”‚          Node B receives: name="Alice", vector=[A:1, B:0]                    â”‚
â”‚                                                                              â”‚
â”‚  STEP 3: Node B updates (knows about A:1)                                   â”‚
â”‚          name="Alice B.", vector=[A:1, B:1]                                  â”‚
â”‚                                                                              â”‚
â”‚  STEP 4: Meanwhile, Node A also updates (doesn't know B:1)                  â”‚
â”‚          name="Alice A.", vector=[A:2, B:0]                                  â”‚
â”‚                                                                              â”‚
â”‚  STEP 5: Sync - CONFLICT DETECTED!                                          â”‚
â”‚          Version 1: [A:1, B:1] - has B:1 but only A:1                       â”‚
â”‚          Version 2: [A:2, B:0] - has A:2 but no B                           â”‚
â”‚          Neither "happens before" the other â†’ CONCURRENT!                   â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ COMPARISON RULES:                                                       â”‚â”‚
â”‚  â”‚                                                                         â”‚â”‚
â”‚  â”‚ [A:1, B:1] vs [A:2, B:0]                                                â”‚â”‚
â”‚  â”‚                                                                         â”‚â”‚
â”‚  â”‚ Is [A:1,B:1] â‰¤ [A:2,B:0]? NO (B:1 > B:0)                                â”‚â”‚
â”‚  â”‚ Is [A:2,B:0] â‰¤ [A:1,B:1]? NO (A:2 > A:1)                                â”‚â”‚
â”‚  â”‚                                                                         â”‚â”‚
â”‚  â”‚ Neither dominates â†’ CONCURRENT WRITES â†’ CONFLICT!                       â”‚â”‚
â”‚  â”‚                                                                         â”‚â”‚
â”‚  â”‚ If [A:1,B:0] vs [A:2,B:1]:                                              â”‚â”‚
â”‚  â”‚ Is [A:1,B:0] â‰¤ [A:2,B:1]? YES (1â‰¤2, 0â‰¤1) â†’ [A:2,B:1] is NEWER          â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                              â”‚
â”‚  ON CONFLICT: Return BOTH versions to client, let them resolve!            â”‚
â”‚               (Amazon's shopping cart: merge items from both)               â”‚
â”‚                                                                              â”‚
â”‚  USED BY: Riak, Amazon Dynamo (original paper)                              â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 3. CRDTs (Conflict-Free Replicated Data Types)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CRDTs                                                     â”‚
â”‚         "Design data structures that auto-merge without conflicts"          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  IDEA: Use data structures where ALL merge orders give SAME result          â”‚
â”‚                                                                              â”‚
â”‚  EXAMPLE: G-Counter (Grow-only Counter)                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚                                                                              â”‚
â”‚  Each node keeps its own counter:                                           â”‚
â”‚                                                                              â”‚
â”‚  Node A: {A: 5, B: 0, C: 0}  â†’ A incremented 5 times                        â”‚
â”‚  Node B: {A: 0, B: 3, C: 0}  â†’ B incremented 3 times                        â”‚
â”‚  Node C: {A: 0, B: 0, C: 2}  â†’ C incremented 2 times                        â”‚
â”‚                                                                              â”‚
â”‚  MERGE: Take MAX of each component                                          â”‚
â”‚  Result: {A: 5, B: 3, C: 2}  â†’ Total = 5 + 3 + 2 = 10                       â”‚
â”‚                                                                              â”‚
â”‚  No matter what order nodes sync, result is always 10!                      â”‚
â”‚                                                                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚  EXAMPLE: LWW-Register (Last-Writer-Wins Register)                          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚                                                                              â”‚
â”‚  Each value has timestamp:                                                  â”‚
â”‚  Node A: {value: "Alice", ts: 1000}                                         â”‚
â”‚  Node B: {value: "Bob", ts: 1005}                                           â”‚
â”‚                                                                              â”‚
â”‚  MERGE: Keep value with highest timestamp                                   â”‚
â”‚  Result: {value: "Bob", ts: 1005}                                           â”‚
â”‚                                                                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚  EXAMPLE: OR-Set (Observed-Remove Set)                                      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚                                                                              â”‚
â”‚  Add wins over concurrent remove (for shopping carts):                      â”‚
â”‚  Node A: ADD "item1"                                                        â”‚
â”‚  Node B: REMOVE "item1" (concurrent)                                        â”‚
â”‚                                                                              â”‚
â”‚  MERGE: Item stays! (Add wins, better UX for e-commerce)                    â”‚
â”‚                                                                              â”‚
â”‚  USED BY: Redis CRDT, Riak, Automerge, Yjs (collaborative editing)         â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Merkle Trees (Anti-Entropy / Consistency Verification)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MERKLE TREES                                              â”‚
â”‚         "Efficiently detect which data is out of sync"                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  PROBLEM: Node A has 1 billion keys, Node B has 1 billion keys.             â”‚
â”‚           How do we find which keys are different WITHOUT comparing all?    â”‚
â”‚                                                                              â”‚
â”‚  SOLUTION: Hash tree (Merkle tree)                                          â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚                         ROOT HASH                                        â”‚â”‚
â”‚  â”‚                        H(H1 + H2)                                        â”‚â”‚
â”‚  â”‚                      = "abc123..."                                       â”‚â”‚
â”‚  â”‚                       /         \                                        â”‚â”‚
â”‚  â”‚                      /           \                                       â”‚â”‚
â”‚  â”‚               â”Œâ”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”                                â”‚â”‚
â”‚  â”‚               â”‚ H1  â”‚             â”‚ H2  â”‚                                â”‚â”‚
â”‚  â”‚               â”‚H(A+B)             â”‚H(C+D)                                â”‚â”‚
â”‚  â”‚               â””â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”˜                                â”‚â”‚
â”‚  â”‚               /     \             /     \                                â”‚â”‚
â”‚  â”‚              /       \           /       \                               â”‚â”‚
â”‚  â”‚          â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”                              â”‚â”‚
â”‚  â”‚          â”‚ H(A)â”‚ â”‚ H(B)â”‚   â”‚ H(C)â”‚ â”‚ H(D)â”‚                              â”‚â”‚
â”‚  â”‚          â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜                              â”‚â”‚
â”‚  â”‚             â”‚       â”‚         â”‚       â”‚                                  â”‚â”‚
â”‚  â”‚          â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”                              â”‚â”‚
â”‚  â”‚          â”‚Key Aâ”‚ â”‚Key Bâ”‚   â”‚Key Câ”‚ â”‚Key Dâ”‚                              â”‚â”‚
â”‚  â”‚          â”‚=100 â”‚ â”‚=200 â”‚   â”‚=300 â”‚ â”‚=400 â”‚                              â”‚â”‚
â”‚  â”‚          â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜                              â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                              â”‚
â”‚  SYNC PROCESS:                                                               â”‚
â”‚                                                                              â”‚
â”‚  1. Node A sends root hash to Node B                                        â”‚
â”‚  2. Node B compares: Same? â†’ In sync! Different? â†’ Drill down               â”‚
â”‚  3. Compare H1 vs H1': Same? â†’ Left subtree in sync                         â”‚
â”‚                        Different? â†’ Check H(A), H(B)                         â”‚
â”‚  4. Find exactly which keys differ: O(log N) comparisons!                   â”‚
â”‚                                                                              â”‚
â”‚  EXAMPLE:                                                                    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚  Node A: 1 billion keys                                                      â”‚
â”‚  Node B: 1 billion keys, but Key C is different                             â”‚
â”‚                                                                              â”‚
â”‚  Without Merkle: Compare 1 billion keys = 1 billion operations              â”‚
â”‚  With Merkle:    Compare ~30 hashes (logâ‚‚ 1 billion â‰ˆ 30)                   â”‚
â”‚                                                                              â”‚
â”‚  USED BY: Cassandra (anti-entropy repair), Bitcoin, Git, IPFS               â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Read Repair & Anti-Entropy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    KEEPING REPLICAS IN SYNC                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  1. READ REPAIR (On-demand, during reads)                                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚                                                                              â”‚
â”‚  Client reads key "user:100" from 3 replicas (quorum read):                â”‚
â”‚                                                                              â”‚
â”‚  Replica A: {name: "Alice", version: 5}                                     â”‚
â”‚  Replica B: {name: "Alice", version: 5}                                     â”‚
â”‚  Replica C: {name: "Al", version: 3}     â† STALE!                           â”‚
â”‚                                                                              â”‚
â”‚  Coordinator notices version mismatch:                                       â”‚
â”‚  â†’ Return version 5 to client                                               â”‚
â”‚  â†’ Asynchronously UPDATE Replica C with version 5                           â”‚
â”‚                                                                              â”‚
â”‚  2. ANTI-ENTROPY (Background, periodic)                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚                                                                              â”‚
â”‚  Background process compares Merkle trees between replicas:                 â”‚
â”‚                                                                              â”‚
â”‚  Every 10 minutes:                                                          â”‚
â”‚  1. Node A and Node B exchange Merkle root hashes                           â”‚
â”‚  2. If different, drill down to find divergent keys                         â”‚
â”‚  3. Exchange and reconcile those specific keys                              â”‚
â”‚                                                                              â”‚
â”‚  WHY BOTH?                                                                   â”‚
â”‚  â€¢ Read repair: Fixes hot (frequently read) data quickly                    â”‚
â”‚  â€¢ Anti-entropy: Fixes cold (rarely read) data eventually                   â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Conflict Resolution Summary

| Strategy | Data Loss? | Complexity | Best For |
|----------|------------|------------|----------|
| **Last-Write-Wins** | Yes (silent) | Low | Simple cases, logs |
| **Vector Clocks** | No (client resolves) | Medium | Shopping carts, docs |
| **CRDTs** | No (auto-merge) | High | Counters, sets, collab editing |
| **Application Logic** | No (custom) | High | Domain-specific rules |

---

## 6. Interview Checklist

### Questions You Should Be Able to Answer

#### Replication
- [ ] "What's the difference between sync and async replication?"
- [ ] "How do you handle replication lag for reads?"
- [ ] "What is the CAP theorem? Give an example of a CP and AP system."
- [ ] "How does quorum-based replication work?"

#### Sharding
- [ ] "How would you choose a shard key for a social media app?"
- [ ] "What is a hotspot and how do you prevent it?"
- [ ] "How do you handle cross-shard joins?"
- [ ] "What is consistent hashing and when would you use it?"

#### CAP/PACELC
- [ ] "During a partition, should this system prioritize consistency or availability?"
- [ ] "What trade-offs does eventual consistency create?"
- [ ] "How does PACELC extend CAP?"

### Decision Framework

```
CHOOSING REPLICATION STRATEGY:

Is data loss acceptable?
â”œâ”€â”€ NO â†’ Synchronous replication (or quorum with W = all)
â””â”€â”€ YES â†’ How much lag is acceptable?
          â”œâ”€â”€ Seconds â†’ Async with monitoring
          â””â”€â”€ Minutes â†’ Async, eventual consistency

CHOOSING SHARD KEY:

What's your primary access pattern?
â”œâ”€â”€ User-centric â†’ Shard by user_id
â”œâ”€â”€ Time-series â†’ Shard by time bucket + hash
â”œâ”€â”€ Geographic â†’ Shard by region
â””â”€â”€ Random â†’ Hash-based sharding

Need range queries?
â”œâ”€â”€ YES â†’ Consider range-based sharding (accept hotspot risk)
â””â”€â”€ NO â†’ Hash-based is safer
```

### Common Pitfalls

| Mistake | Why It's Wrong | Correct Understanding |
|---------|----------------|----------------------|
| "CAP means pick 2 of 3" | P is mandatory | Choose C or A during partitions |
| "Sharding is always better" | Adds massive complexity | Only shard when you must |
| "Use celebrity_id as shard key" | Creates hotspots | Shard by content_id or compound key |
| "Sync replication = no data loss" | Can still lose in-flight txns | Need 2PC for guaranteed durability |

---

## Next Steps

Continue to **[Level 4: Real-Time Updates](04_REALTIME_UPDATES.md)** to learn about CDC, WebSockets, and fan-out patterns.

