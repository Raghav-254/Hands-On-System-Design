# Level 3: Distributed Systems (The Scalability Layer)

> When a single database server isn't enough, you enter the world of distributed systemsâ€”where network partitions, consistency trade-offs, and careful architectural decisions become critical.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ğŸ—ºï¸  HOW THIS LEVEL CONNECTS                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  LEVEL 3: DISTRIBUTED SYSTEMS â—„â”€â”€ YOU ARE HERE                             â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚  Scope: MULTI-NODE, scaling beyond one machine                              â”‚
â”‚  Focus: Replication, sharding, consistency trade-offs, failure handling    â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚                         BUILDS ON LEVEL 1 & 2                           â”‚â”‚
â”‚  â”‚                                                                         â”‚â”‚
â”‚  â”‚  Level 1 (Storage)         Level 3 (This)                               â”‚â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                               â”‚â”‚
â”‚  â”‚  WAL                  â”€â”€â”€â–º Replication USES WAL streaming!              â”‚â”‚
â”‚  â”‚  Pages/Buffer Pool    â”€â”€â”€â–º Each node has its own buffer pool            â”‚â”‚
â”‚  â”‚  B-Tree/LSM-Tree      â”€â”€â”€â–º Each shard uses same storage engine          â”‚â”‚
â”‚  â”‚                                                                         â”‚â”‚
â”‚  â”‚  Level 2 (Logic)           Level 3 (This)                               â”‚â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                               â”‚â”‚
â”‚  â”‚  MVCC (local)         â”€â”€â”€â–º Distributed MVCC (global timestamps)         â”‚â”‚
â”‚  â”‚  Isolation levels     â”€â”€â”€â–º Distributed transactions (2PC, Saga)         â”‚â”‚
â”‚  â”‚  Write conflicts      â”€â”€â”€â–º LWW, Vector Clocks, CRDTs                    â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚                         EXTENDS TO LEVEL 4                              â”‚â”‚
â”‚  â”‚                                                                         â”‚â”‚
â”‚  â”‚  Level 3 (This)            Level 4 (Real-time)                          â”‚â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                          â”‚â”‚
â”‚  â”‚  Replication          â”€â”€â”€â–º CDC taps into replication stream             â”‚â”‚
â”‚  â”‚  Sharding             â”€â”€â”€â–º Fan-out must consider shard locations        â”‚â”‚
â”‚  â”‚  Consistency          â”€â”€â”€â–º Real-time updates need causal ordering       â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                              â”‚
â”‚  WHEN DO YOU NEED THIS?                                                     â”‚
â”‚  â€¢ Single node can't handle the load (scale)                               â”‚
â”‚  â€¢ Need high availability (replicas for failover)                          â”‚
â”‚  â€¢ Users are globally distributed (geo-replication)                        â”‚
â”‚  â€¢ Data too large for one machine (sharding)                               â”‚
â”‚                                                                              â”‚
â”‚  APPLIES TO:                                                                â”‚
â”‚  âœ… SQL: PostgreSQL replication, Vitess, CockroachDB, Spanner              â”‚
â”‚  âœ… NoSQL: Cassandra, DynamoDB, MongoDB (designed for distributed)         â”‚
â”‚                                                                              â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚                                                                              â”‚
â”‚  âš ï¸  SQL vs NoSQL: WHICH TOPICS APPLY WHERE?                                â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                              â”‚
â”‚  Not all distributed concepts apply equally to SQL and NoSQL!              â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Topic                 â”‚ SQL        â”‚ NewSQL     â”‚ NoSQL (Leaderless)   â”‚â”‚
â”‚  â”‚                       â”‚(PostgreSQL)â”‚(Cockroach) â”‚(Cassandra, Dynamo)   â”‚â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚
â”‚  â”‚ Replication           â”‚ âœ… Yes     â”‚ âœ… Yes     â”‚ âœ… Yes               â”‚â”‚
â”‚  â”‚ Sharding              â”‚ âš ï¸ Add-on  â”‚ âœ… Native  â”‚ âœ… Native            â”‚â”‚
â”‚  â”‚ CAP/PACELC            â”‚ âœ… Applies â”‚ âœ… Applies â”‚ âœ… Applies           â”‚â”‚
â”‚  â”‚ Consensus (Raft)      â”‚ âœ… For HA  â”‚ âœ… Core    â”‚ âŒ No leader         â”‚â”‚
â”‚  â”‚ Conflict Resolution   â”‚ âŒ N/A     â”‚ âœ… Internalâ”‚ âœ… Primary use       â”‚â”‚
â”‚  â”‚ Hinted Handoff        â”‚ âŒ N/A     â”‚ âŒ Uses Raftâ”‚ âœ… Primary use       â”‚â”‚
â”‚  â”‚ Read Repair           â”‚ âŒ N/A     â”‚ âš ï¸ Some    â”‚ âœ… Primary use       â”‚â”‚
â”‚  â”‚ Anti-Entropy          â”‚ âŒ N/A     â”‚ âš ï¸ Some    â”‚ âœ… Primary use       â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                              â”‚
â”‚  WHY THE DIFFERENCE?                                                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                              â”‚
â”‚  â€¢ SQL (single-leader): ONE node accepts writes â†’ no write conflicts!     â”‚
â”‚    Uses consensus only for FAILOVER (elect new leader when primary dies). â”‚
â”‚                                                                              â”‚
â”‚  â€¢ NewSQL: Uses consensus for EVERY WRITE to ensure strong consistency.   â”‚
â”‚    Handles conflicts internally, you don't see them.                      â”‚
â”‚                                                                              â”‚
â”‚  â€¢ NoSQL (leaderless): ANY node can accept writes â†’ conflicts happen!     â”‚
â”‚    Needs LWW/Vector Clocks/CRDTs + repair mechanisms for consistency.     â”‚
â”‚                                                                              â”‚
â”‚  ğŸ‘‰ Sections 1-4 apply broadly. Section 5 is primarily for leaderless.    â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Table of Contents

1. [Replication Strategies](#1-replication-strategies)
   - [Why Replicate?](#why-replicate)
   - [Synchronous vs Asynchronous Replication](#synchronous-vs-asynchronous-replication)
   - [Replication Topologies](#replication-topologies)
   - [Handling Replication Lag](#handling-replication-lag)

2. [Sharding (Horizontal Partitioning)](#2-sharding-horizontal-partitioning)
   - [Sharding Strategies](#sharding-strategies) (Range, Hash, Consistent Hashing)
   - [Choosing a Shard Key](#choosing-a-shard-key)
   - [The Hotspot Problem](#the-hotspot-problem)
   - [Cross-Shard Operations](#cross-shard-operations)
   - [SQL Sharding: Add-On vs Native](#sql-sharding-add-on-vs-native)

3. [CAP Theorem and PACELC](#3-cap-theorem-and-pacelc)
   - [CAP Theorem](#cap-theorem)
   - [CP vs AP During Partition](#during-a-partition-cp-vs-ap)
   - [PACELC: The Complete Picture](#pacelc-the-complete-picture)
   - [Database Classifications](#database-cappacelc-classifications)

4. [Consensus and Coordination](#4-consensus-and-coordination)
   - [Why Consensus Matters](#why-consensus-matters)
   - [Raft Consensus](#raft-consensus-simplified)
   - [Consensus Use Cases](#consensus-use-cases)
   - [Distributed Locks vs Database Locks](#distributed-locks-vs-database-locks)
   - [Fencing Tokens](#fencing-tokens-preventing-zombie-leaders)
   - [Distributed Transactions: 2PC vs Saga](#distributed-transactions-2pc-vs-saga)

5. [Conflict Resolution & Anti-Entropy](#5-conflict-resolution--anti-entropy)
   - **Part 1: Write Conflicts**
     - [The Conflict Problem](#the-conflict-problem)
     - [Last-Write-Wins (LWW)](#1-last-write-wins-lww)
     - [Vector Clocks](#2-vector-clocks-detect-conflicts-dont-auto-resolve)
     - [CRDTs](#3-crdts-conflict-free-replicated-data-types)
   - **Part 2: Keeping Replicas in Sync**
     - [Layer 1: Hinted Handoff](#layer-1-hinted-handoff-preventing-divergence)
     - [Layer 2: Read Repair](#layer-2-read-repair-fixing-divergence-on-demand)
     - [Layer 3: Anti-Entropy with Merkle Trees](#layer-3-anti-entropy-with-merkle-trees-background-repair)
   - [Section 5 Summary](#section-5-summary)

6. [Interview Checklist](#6-interview-checklist)

---

## 1. Replication Strategies

### Why Replicate?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    REPLICATION GOALS                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  1. HIGH AVAILABILITY                                               â”‚
â”‚     â””â”€â”€ If primary fails, replica takes over                        â”‚
â”‚                                                                      â”‚
â”‚  2. READ SCALABILITY                                                â”‚
â”‚     â””â”€â”€ Distribute read load across multiple servers                â”‚
â”‚                                                                      â”‚
â”‚  3. GEOGRAPHIC DISTRIBUTION                                         â”‚
â”‚     â””â”€â”€ Reduce latency by placing replicas near users               â”‚
â”‚                                                                      â”‚
â”‚  4. DISASTER RECOVERY                                               â”‚
â”‚     â””â”€â”€ Replicas in different data centers survive outages          â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Synchronous vs Asynchronous Replication

#### Synchronous Replication

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SYNCHRONOUS REPLICATION                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  Client         Primary           Replica 1         Replica 2        â”‚
â”‚    â”‚               â”‚                  â”‚                â”‚             â”‚
â”‚    â”‚â”€â”€â”€â”€ WRITE â”€â”€â”€â–ºâ”‚                  â”‚                â”‚             â”‚
â”‚    â”‚               â”‚                  â”‚                â”‚             â”‚
â”‚    â”‚               â”‚â”€â”€ Replicate â”€â”€â”€â”€â–ºâ”‚                â”‚             â”‚
â”‚    â”‚               â”‚                  â”‚                â”‚             â”‚
â”‚    â”‚               â”‚â”€â”€ Replicate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚             â”‚
â”‚    â”‚               â”‚                  â”‚                â”‚             â”‚
â”‚    â”‚               â”‚â—„â”€â”€â”€â”€ ACK â”€â”€â”€â”€â”€â”€â”€â”€â”‚                â”‚             â”‚
â”‚    â”‚               â”‚                  â”‚                â”‚             â”‚
â”‚    â”‚               â”‚â—„â”€â”€â”€â”€ ACK â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚             â”‚
â”‚    â”‚               â”‚                  â”‚                â”‚             â”‚
â”‚    â”‚â—„â”€â”€â”€â”€ OK â”€â”€â”€â”€â”€â”€â”‚  (only after    â”‚                â”‚             â”‚
â”‚    â”‚               â”‚   ALL acks!)    â”‚                â”‚             â”‚
â”‚                                                                      â”‚
â”‚  PROPERTIES:                                                         â”‚
â”‚  âœ… Strong consistency (all replicas have same data)                 â”‚
â”‚  âœ… No data loss on failover                                         â”‚
â”‚  âŒ Higher write latency (wait for slowest replica)                  â”‚
â”‚  âŒ Reduced availability (if replica down, writes blocked)           â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Asynchronous Replication

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ASYNCHRONOUS REPLICATION                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  Client         Primary           Replica 1         Replica 2        â”‚
â”‚    â”‚               â”‚                  â”‚                â”‚             â”‚
â”‚    â”‚â”€â”€â”€â”€ WRITE â”€â”€â”€â–ºâ”‚                  â”‚                â”‚             â”‚
â”‚    â”‚               â”‚                  â”‚                â”‚             â”‚
â”‚    â”‚â—„â”€â”€â”€â”€ OK â”€â”€â”€â”€â”€â”€â”‚  (immediate!)    â”‚                â”‚             â”‚
â”‚    â”‚               â”‚                  â”‚                â”‚             â”‚
â”‚    â”‚               â”‚â”€â”€ Replicate â”€â”€â”€â”€â–ºâ”‚                â”‚             â”‚
â”‚    â”‚               â”‚        (background, best-effort)  â”‚             â”‚
â”‚    â”‚               â”‚â”€â”€ Replicate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚             â”‚
â”‚    â”‚               â”‚                  â”‚                â”‚             â”‚
â”‚                                                                      â”‚
â”‚  PROPERTIES:                                                         â”‚
â”‚  âœ… Low write latency (don't wait for replicas)                      â”‚
â”‚  âœ… High availability (replica failure doesn't block writes)         â”‚
â”‚  âŒ Potential data loss on failover (uncommitted writes on primary)  â”‚
â”‚  âŒ Replication lag (replicas may be seconds/minutes behind)         â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Semi-Synchronous Replication

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  SEMI-SYNCHRONOUS REPLICATION                        â”‚
â”‚                    (Quorum-based approach)                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  CONFIGURATION: 3 replicas, require ACK from 2 (majority)            â”‚
â”‚                                                                      â”‚
â”‚  Client         Primary           Replica 1         Replica 2        â”‚
â”‚    â”‚               â”‚                  â”‚                â”‚             â”‚
â”‚    â”‚â”€â”€â”€â”€ WRITE â”€â”€â”€â–ºâ”‚                  â”‚                â”‚             â”‚
â”‚    â”‚               â”‚                  â”‚                â”‚             â”‚
â”‚    â”‚               â”‚â”€â”€ Replicate â”€â”€â”€â”€â–ºâ”‚                â”‚             â”‚
â”‚    â”‚               â”‚                  â”‚                â”‚             â”‚
â”‚    â”‚               â”‚â”€â”€ Replicate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚             â”‚
â”‚    â”‚               â”‚                  â”‚                â”‚             â”‚
â”‚    â”‚               â”‚â—„â”€â”€â”€â”€ ACK â”€â”€â”€â”€â”€â”€â”€â”€â”‚                â”‚             â”‚
â”‚    â”‚               â”‚                  â”‚  (Replica 2    â”‚             â”‚
â”‚    â”‚â—„â”€â”€â”€â”€ OK â”€â”€â”€â”€â”€â”€â”‚  (1 ACK +        â”‚   may be slow) â”‚             â”‚
â”‚    â”‚               â”‚   primary = 2!)  â”‚                â”‚             â”‚
â”‚    â”‚               â”‚                  â”‚                â”‚             â”‚
â”‚    â”‚               â”‚â—„â”€â”€â”€â”€ ACK â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚             â”‚
â”‚    â”‚               â”‚       (arrives later, logged)     â”‚             â”‚
â”‚                                                                      â”‚
â”‚  PROPERTIES:                                                         â”‚
â”‚  âœ… Balance of consistency and latency                               â”‚
â”‚  âœ… Tolerates minority failures                                      â”‚
â”‚  âš ï¸  Quorum must be configured correctly                             â”‚
â”‚                                                                      â”‚
â”‚  QUORUM FORMULA:                                                     â”‚
â”‚  Write Quorum (W) + Read Quorum (R) > N (total replicas)             â”‚
â”‚  For strict consistency: W + R > N                                   â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Replication Topologies

```
SINGLE-LEADER (Primary-Replica)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PRIMARY  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ REPLICA  â”‚
â”‚ (writes) â”‚         â”‚ (reads)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ REPLICA  â”‚
                     â”‚ (reads)  â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

âœ… Simple consistency model
âœ… No write conflicts
âŒ Single point of failure for writes
âŒ All writes through one node (bottleneck)


MULTI-LEADER (Active-Active)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LEADER 1 â”‚â—„â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ LEADER 2 â”‚
â”‚  (DC-A)  â”‚         â”‚  (DC-B)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

âœ… Low latency writes in each DC
âœ… Writes continue if one DC fails
âŒ CONFLICT RESOLUTION REQUIRED
âŒ Complex (last-write-wins, merge, custom logic)


LEADERLESS (Dynamo-style)
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Node A  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â–²  â–²
       /    \
      /      \
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Node B â”‚â”€â”€â”‚ Node C â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜

âœ… No single point of failure
âœ… Any node can accept writes
âŒ Requires quorum reads/writes
âŒ Conflict resolution via vector clocks
```

### Handling Replication Lag

```
SCENARIO: User updates profile, then immediately views it

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                      â”‚
â”‚  User â”€â”€â”€â”€ UPDATE â”€â”€â”€â–º Primary â”€â”€â”€â”€ replicate â”€â”€â”€â–º Replica          â”‚
â”‚    â”‚                                                 (delayed)       â”‚
â”‚    â”‚                                                    â”‚            â”‚
â”‚    â””â”€â”€â”€â”€ READ (load balanced to replica) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                                      â”‚
â”‚  RESULT: User sees OLD data! ğŸ˜±                                      â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

SOLUTIONS:

1. READ YOUR WRITES (Read-after-write consistency)
   â€¢ After a write, read from primary for X seconds
   â€¢ Or track "last write timestamp" and route accordingly

2. MONOTONIC READS
   â€¢ User always reads from same replica
   â€¢ Prevents "going back in time"

3. CAUSAL CONSISTENCY  
   â€¢ Track happens-before relationships
   â€¢ Ensure causally-related writes seen in order

4. STICKY SESSIONS
   â€¢ Route user to same replica consistently
   â€¢ Simple but reduces load balancing flexibility
```

---

## 2. Sharding (Horizontal Partitioning)

### Why Shard?

```
SINGLE DATABASE LIMITS:
â€¢ Storage: Disk is finite (~10-100 TB practical limit)
â€¢ Write throughput: Single node can only handle so many IOPS
â€¢ RAM: Buffer pool limited to one machine's memory
â€¢ CPU: Query processing bound by one machine

SHARDING SOLUTION:
â€¢ Distribute data across multiple independent databases
â€¢ Each shard handles a subset of the data
â€¢ Scale horizontally by adding more shards
```

### Sharding Strategies

#### Range-Based Sharding

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RANGE-BASED SHARDING                              â”‚
â”‚              (Partition by value ranges)                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  Shard Key: user_id                                                  â”‚
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚   SHARD 1    â”‚ â”‚   SHARD 2    â”‚ â”‚   SHARD 3    â”‚                â”‚
â”‚  â”‚  user_id     â”‚ â”‚  user_id     â”‚ â”‚  user_id     â”‚                â”‚
â”‚  â”‚  1 - 1M      â”‚ â”‚  1M - 2M     â”‚ â”‚  2M - 3M     â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                                                                      â”‚
â”‚  âœ… Efficient range queries (scan single shard)                      â”‚
â”‚  âœ… Easy to understand and debug                                     â”‚
â”‚  âŒ HOTSPOT RISK: New users all go to latest shard                   â”‚
â”‚  âŒ Uneven distribution over time                                    â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Hash-Based Sharding

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    HASH-BASED SHARDING                               â”‚
â”‚              (Partition by hash of key)                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  shard_id = hash(user_id) % num_shards                               â”‚
â”‚                                                                      â”‚
â”‚  Example: user_id = 12345                                            â”‚
â”‚           hash(12345) = 8472391                                      â”‚
â”‚           8472391 % 3 = 1  â†’ Goes to Shard 1                         â”‚
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚   SHARD 0    â”‚ â”‚   SHARD 1    â”‚ â”‚   SHARD 2    â”‚                â”‚
â”‚  â”‚  user_ids:   â”‚ â”‚  user_ids:   â”‚ â”‚  user_ids:   â”‚                â”‚
â”‚  â”‚  3,6,9,12... â”‚ â”‚  1,4,7,10... â”‚ â”‚  2,5,8,11... â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                                                                      â”‚
â”‚  âœ… Even distribution of KEYS across shards                          â”‚
â”‚  âœ… Works with any key type                                          â”‚
â”‚  âŒ Range queries require scatter-gather                             â”‚
â”‚  âŒ Adding shards requires rehashing (unless consistent hashing)     â”‚
â”‚  âš ï¸  Does NOT solve "hot key" problem (celebrity still on 1 shard!) â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Consistent Hashing

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CONSISTENT HASHING                                â”‚
â”‚           (Minimize reshuffling when adding/removing nodes)          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚                    Hash Ring (0 to 2^32-1)                           â”‚
â”‚                                                                      â”‚
â”‚                         0/MAX                                        â”‚
â”‚                          â”‚                                           â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”                                    â”‚
â”‚                   /             \                                    â”‚
â”‚                  /               \                                   â”‚
â”‚           Shard A                 Shard B                            â”‚
â”‚            (2^30)                  (2^31)                            â”‚
â”‚                  \               /                                   â”‚
â”‚                   \             /                                    â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                                    â”‚
â”‚                          â”‚                                           â”‚
â”‚                       Shard C                                        â”‚
â”‚                      (3*2^30)                                        â”‚
â”‚                                                                      â”‚
â”‚  Key placement: hash(key), find next shard clockwise                â”‚
â”‚                                                                      â”‚
â”‚  ADDING A NEW SHARD:                                                â”‚
â”‚  â€¢ Only keys between new shard and predecessor move                 â”‚
â”‚  â€¢ ~1/N of data migrates (not all data like simple hash mod)        â”‚
â”‚                                                                      â”‚
â”‚  VIRTUAL NODES:                                                      â”‚
â”‚  â€¢ Each physical shard has multiple positions on ring               â”‚
â”‚  â€¢ Improves load balancing                                          â”‚
â”‚  â€¢ Handles heterogeneous hardware                                   â”‚
â”‚                                                                      â”‚
â”‚  âš ï¸  IMPORTANT: Consistent hashing solves RESHARDING, not HOT KEYS! â”‚
â”‚  â€¢ Celebrity's data still lands on ONE shard                        â”‚
â”‚  â€¢ See "The Hotspot Problem" section below for solutions            â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Choosing a Shard Key

```
GOOD SHARD KEY PROPERTIES:

1. HIGH CARDINALITY
   âœ… user_id (millions of values)
   âŒ country (only ~200 values)

2. EVEN DISTRIBUTION  
   âœ… user_id (users distributed evenly)
   âŒ celebrity_id (some have millions of followers)

3. QUERY ISOLATION
   âœ… user_id for user data (queries usually filter by user)
   âŒ status (queries rarely filter by status alone)

4. WRITE DISTRIBUTION
   âœ… user_id (writes spread across users)
   âŒ created_date (all new writes to "today" shard)
```

### The Hotspot Problem

```
SCENARIO: Social media with celebrity users

Shard Key: user_id
Celebrity Taylor Swift: user_id = 12345

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                      â”‚
â”‚  PROBLEM:                                                            â”‚
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚  â”‚  Shard 0   â”‚ â”‚  Shard 1   â”‚ â”‚  Shard 2   â”‚                       â”‚
â”‚  â”‚   10 QPS   â”‚ â”‚  10 QPS    â”‚ â”‚ 10,000 QPS â”‚ â† Taylor's shard!    â”‚
â”‚  â”‚            â”‚ â”‚            â”‚ â”‚  ğŸ”¥ğŸ”¥ğŸ”¥    â”‚                       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                                                                      â”‚
â”‚  SOLUTIONS:                                                          â”‚
â”‚                                                                      â”‚
â”‚  1. SHARD BY CONTENT, NOT USER                                       â”‚
â”‚     â€¢ Shard posts by post_id                                         â”‚
â”‚     â€¢ Taylor's posts spread across shards                            â”‚
â”‚     â€¢ But: Fetching "all Taylor's posts" is scatter-gather           â”‚
â”‚                                                                      â”‚
â”‚  2. COMPOUND SHARD KEY                                               â”‚
â”‚     â€¢ Shard by (user_id, timestamp)                                  â”‚
â”‚     â€¢ Taylor's posts spread by time                                  â”‚
â”‚     â€¢ Range queries by time still work                               â”‚
â”‚                                                                      â”‚
â”‚  3. APPLICATION-LEVEL CACHING                                        â”‚
â”‚     â€¢ Cache hot users in Redis                                       â”‚
â”‚     â€¢ Only cold reads hit the database                               â”‚
â”‚                                                                      â”‚
â”‚  4. READ REPLICAS FOR HOT SHARDS                                     â”‚
â”‚     â€¢ More replicas for shard 2                                      â”‚
â”‚     â€¢ Doesn't help write hotspots                                    â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Cross-Shard Operations

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    THE PROBLEM: JOINs ACROSS SHARDS                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  You have TWO TABLES that need to be joined:                                â”‚
â”‚                                                                              â”‚
â”‚  TABLE: users                    TABLE: orders                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ id â”‚ name   â”‚ email  â”‚        â”‚ order_id â”‚ user_id â”‚ total â”‚ ...â”‚       â”‚
â”‚  â”‚ 1  â”‚ Alice  â”‚ a@...  â”‚        â”‚ 101      â”‚ 1       â”‚ $50   â”‚    â”‚       â”‚
â”‚  â”‚ 2  â”‚ Bob    â”‚ b@...  â”‚        â”‚ 102      â”‚ 2       â”‚ $30   â”‚    â”‚       â”‚
â”‚  â”‚ 3  â”‚ Carol  â”‚ c@...  â”‚        â”‚ 103      â”‚ 1       â”‚ $75   â”‚    â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚ 104      â”‚ 3       â”‚ $20   â”‚    â”‚       â”‚
â”‚                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                                              â”‚
â”‚  QUERY: "Get all pending orders with user details"                          â”‚
â”‚  SELECT * FROM orders o JOIN users u ON o.user_id = u.id                   â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCENARIO A: BOTH tables sharded by user_id (CO-LOCATED) âœ…                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  Shard 0 (user_id % 2 = 0)         Shard 1 (user_id % 2 = 1)               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚ users:                    â”‚     â”‚ users:                    â”‚           â”‚
â”‚  â”‚   id=2 (Bob)              â”‚     â”‚   id=1 (Alice)            â”‚           â”‚
â”‚  â”‚                           â”‚     â”‚   id=3 (Carol)            â”‚           â”‚
â”‚  â”‚ orders:                   â”‚     â”‚ orders:                   â”‚           â”‚
â”‚  â”‚   order_id=102, user_id=2 â”‚     â”‚   order_id=101, user_id=1 â”‚           â”‚
â”‚  â”‚                           â”‚     â”‚   order_id=103, user_id=1 â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚   order_id=104, user_id=3 â”‚           â”‚
â”‚                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                                                              â”‚
â”‚  âœ… JOIN happens WITHIN each shard (no network hop!)                        â”‚
â”‚  âœ… Each shard has the user AND their orders together                       â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCENARIO B: Tables sharded by DIFFERENT keys âŒ                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  users sharded by user_id:         orders sharded by order_id:             â”‚
â”‚                                                                              â”‚
â”‚  Shard 0         Shard 1           Shard 0         Shard 1                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚ user 2  â”‚     â”‚ user 1  â”‚       â”‚ order   â”‚     â”‚ order   â”‚             â”‚
â”‚  â”‚ (Bob)   â”‚     â”‚ (Alice) â”‚       â”‚ 102     â”‚     â”‚ 101     â”‚             â”‚
â”‚  â”‚         â”‚     â”‚ user 3  â”‚       â”‚ 104     â”‚     â”‚ 103     â”‚             â”‚
â”‚  â”‚         â”‚     â”‚ (Carol) â”‚       â”‚         â”‚     â”‚         â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                                                              â”‚
â”‚  To JOIN order 101 with user 1:                                            â”‚
â”‚                                                                              â”‚
â”‚    Orders Shard 1          Users Shard 1                                   â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚    â”‚ order 101 â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚ user 1    â”‚   â† NETWORK HOP!                 â”‚
â”‚    â”‚ user_id=1 â”‚  lookup   â”‚ (Alice)   â”‚                                   â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚                                                                              â”‚
â”‚  âŒ EVERY order needs a cross-shard lookup to get user info!               â”‚
â”‚  âŒ 1000 orders = 1000 network round trips (or batch, still slow)          â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

SOLUTIONS:

1. CO-LOCATE RELATED DATA
   â€¢ Shard BOTH tables by user_id (Scenario A above)
   â€¢ Trade-off: "Find all orders" becomes scatter-gather

2. DENORMALIZATION
   â€¢ Store user_name directly in orders table
   â€¢ Trade-off: Data duplication, update complexity

3. APPLICATION-LEVEL JOINS
   â€¢ Fetch orders first, then batch fetch users by user_ids
   â€¢ Trade-off: More round trips, application complexity

4. AVOID CROSS-SHARD TRANSACTIONS
   â€¢ Use Saga pattern for distributed transactions
   â€¢ Accept eventual consistency where possible
```

### SQL Sharding: Add-On vs Native

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SQL SHARDING OPTIONS                                      â”‚
â”‚              (Referenced from Level 2: Database Logic)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  DESIGN PHILOSOPHY:                                                          â”‚
â”‚  â€¢ Traditional SQL (PostgreSQL, MySQL): Single-node by default              â”‚
â”‚  â€¢ NoSQL (Cassandra, DynamoDB): Distributed by default                      â”‚
â”‚  â€¢ BUT: SQL CAN be sharded! It's just not the default.                      â”‚
â”‚                                                                              â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚  OPTION 1: APPLICATION-LEVEL SHARDING                                       â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚                                                                              â”‚
â”‚  Your application code decides which database to query:                     â”‚
â”‚                                                                              â”‚
â”‚    def get_user(user_id):                                                   â”‚
â”‚        shard_id = user_id % NUM_SHARDS                                      â”‚
â”‚        db = connections[f"shard_{shard_id}"]                                â”‚
â”‚        return db.query("SELECT * FROM users WHERE id = ?", user_id)         â”‚
â”‚                                                                              â”‚
â”‚  âœ… Pros: Full control, no middleware                                       â”‚
â”‚  âŒ Cons: Cross-shard queries are YOUR problem, resharding is painful       â”‚
â”‚                                                                              â”‚
â”‚  Used by: Instagram, Pinterest (early days)                                 â”‚
â”‚                                                                              â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚  OPTION 2: MIDDLEWARE / PROXY SHARDING                                      â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚                                                                              â”‚
â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                       â”‚
â”‚       â”‚    Sharding Middleware      â”‚                                       â”‚
â”‚       â”‚  (Vitess, Citus, ProxySQL)  â”‚                                       â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                       â”‚
â”‚             /        |        \                                              â”‚
â”‚            â†“         â†“         â†“                                             â”‚
â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                                      â”‚
â”‚       â”‚ MySQL  â”‚ â”‚ MySQL  â”‚ â”‚ MySQL  â”‚                                      â”‚
â”‚       â”‚ Shard1 â”‚ â”‚ Shard2 â”‚ â”‚ Shard3 â”‚                                      â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                      â”‚
â”‚                                                                              â”‚
â”‚  Tools:                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Vitess        â”‚ MySQL sharding (powers YouTube, Slack, GitHub)        â”‚  â”‚
â”‚  â”‚ Citus         â”‚ PostgreSQL extension for horizontal scaling           â”‚  â”‚
â”‚  â”‚ ProxySQL      â”‚ MySQL query routing and load balancing                â”‚  â”‚
â”‚  â”‚ PgBouncer     â”‚ PostgreSQL connection pooling (not sharding)          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                              â”‚
â”‚  âœ… Pros: App doesn't know about shards, some cross-shard support          â”‚
â”‚  âŒ Cons: Added complexity, potential single point of failure              â”‚
â”‚                                                                              â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚  OPTION 3: NEWSQL (Native Distributed SQL)                                  â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚                                                                              â”‚
â”‚  SQL databases DESIGNED for distribution from the start:                    â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ CockroachDB   â”‚ PostgreSQL-compatible, automatic sharding, ACID       â”‚  â”‚
â”‚  â”‚ Google Spannerâ”‚ Globally distributed, TrueTime, used by Google        â”‚  â”‚
â”‚  â”‚ YugabyteDB    â”‚ PostgreSQL & Cassandra compatible                     â”‚  â”‚
â”‚  â”‚ TiDB          â”‚ MySQL-compatible, from PingCAP                        â”‚  â”‚
â”‚  â”‚ PlanetScale   â”‚ Serverless MySQL (built on Vitess)                    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                              â”‚
â”‚  âœ… Pros: Full SQL, automatic sharding, distributed ACID transactions      â”‚
â”‚  âŒ Cons: Higher latency than single-node, operational complexity          â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### SQL vs NoSQL Sharding Comparison

| Aspect | Traditional SQL + Sharding | NoSQL (Cassandra) | NewSQL (CockroachDB) |
|--------|---------------------------|-------------------|---------------------|
| **Sharding** | Add-on (manual/middleware) | Native (built-in) | Native (built-in) |
| **Cross-shard JOINs** | Hard (scatter-gather) | Not supported | Supported |
| **Cross-shard TXNs** | Complex (2PC needed) | Limited | Full ACID |
| **Query flexibility** | Full SQL | Limited (CQL) | Full SQL |
| **Operational complexity** | High (you manage) | Medium | Medium |
| **When to use** | Existing SQL apps | Write-heavy, simple queries | Need SQL + scale |

---

## 3. CAP Theorem and PACELC

### CAP Theorem

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       CAP THEOREM                                    â”‚
â”‚    "In a distributed system, during a network partition,            â”‚
â”‚     you can only guarantee either Consistency or Availability"       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚                        â”‚ CONSISTENCY   â”‚                            â”‚
â”‚                        â”‚ (All nodes    â”‚                            â”‚
â”‚                        â”‚  see same     â”‚                            â”‚
â”‚                        â”‚  data)        â”‚                            â”‚
â”‚                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚                             /\                                       â”‚
â”‚                            /  \                                      â”‚
â”‚                           /    \                                     â”‚
â”‚                          /      \                                    â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   /        \   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚     â”‚ AVAILABILITY  â”‚â—„â”€â”˜          â””â”€â–ºâ”‚ PARTITION     â”‚              â”‚
â”‚     â”‚ (Every requestâ”‚                â”‚ TOLERANCE     â”‚              â”‚
â”‚     â”‚  gets a       â”‚                â”‚ (System works â”‚              â”‚
â”‚     â”‚  response)    â”‚                â”‚  despite      â”‚              â”‚
â”‚     â”‚               â”‚                â”‚  network      â”‚              â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚  splits)      â”‚              â”‚
â”‚                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                                                      â”‚
â”‚  YOU MUST HAVE P:                                                   â”‚
â”‚  Network partitions WILL happen. The real choice is C vs A.         â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### During a Partition: CP vs AP

```
SCENARIO: Network partition between DC-East and DC-West

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         PARTITION         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   DC-East   â”‚    â•â•â•â•â•â•â•â•³â•â•â•â•â•â•â•â•â•â•â•    â”‚   DC-West   â”‚
â”‚   Primary   â”‚                           â”‚   Replica   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


CP CHOICE (Consistency over Availability):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â€¢ DC-West becomes read-only or unavailable                         â”‚
â”‚  â€¢ Writes only accepted by DC-East (primary)                        â”‚
â”‚  â€¢ Users in DC-West see errors or stale data                        â”‚
â”‚  â€¢ When partition heals, data is consistent                         â”‚
â”‚                                                                      â”‚
â”‚  EXAMPLES: Traditional RDBMS, ZooKeeper, etcd                       â”‚
â”‚  USE WHEN: Financial transactions, inventory, anything where        â”‚
â”‚            wrong data is worse than no data                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


AP CHOICE (Availability over Consistency):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â€¢ Both DCs continue accepting reads AND writes                     â”‚
â”‚  â€¢ Users in both locations can work normally                        â”‚
â”‚  â€¢ Data diverges during partition                                   â”‚
â”‚  â€¢ When partition heals, CONFLICT RESOLUTION needed                 â”‚
â”‚                                                                      â”‚
â”‚  EXAMPLES: Cassandra, DynamoDB, CouchDB                             â”‚
â”‚  USE WHEN: Shopping carts, social media, anything where             â”‚
â”‚            availability is more important than perfect consistency  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### PACELC: The Complete Picture

```
CAP only talks about partition scenarios. 
PACELC asks: What about normal operation?

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         PACELC                                       â”‚
â”‚                                                                      â”‚
â”‚     IF                           ELSE                                â”‚
â”‚   Partition                  (normal operation)                      â”‚
â”‚      â”‚                             â”‚                                 â”‚
â”‚      â–¼                             â–¼                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”                      â”Œâ”€â”€â”€â”€â”€â”                              â”‚
â”‚   â”‚ A   â”‚  or                  â”‚ L   â”‚  or                          â”‚
â”‚   â”‚     â”‚                      â”‚     â”‚                              â”‚
â”‚   â”‚ C   â”‚                      â”‚ C   â”‚                              â”‚
â”‚   â””â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”˜                              â”‚
â”‚                                                                      â”‚
â”‚   P: Partition                  E: Else (normal)                    â”‚
â”‚   A: Availability               L: Latency                          â”‚
â”‚   C: Consistency                C: Consistency                      â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PACELC CLASSIFICATIONS:

PA/EL: High availability, low latency, eventual consistency
       Examples: DynamoDB, Cassandra
       "Be fast and available, consistency can wait"

PC/EC: Strong consistency always, sacrifice latency
       Examples: RDBMS with sync replication
       "Consistency is king, we'll wait for it"

PA/EC: Available during partition, but consistent normally
       Examples: MongoDB (default config)
       "Be consistent when we can, available when we must"

PC/EL: Consistent during partition, but fast normally
       Examples: PAXOS-based systems with local reads
       "Fast reads, but partition = unavailability"
```

### Database CAP/PACELC Classifications

| Database | CAP | PACELC | Trade-off Notes |
|----------|-----|--------|-----------------|
| **PostgreSQL** (single) | CA* | PC/EC | *No partition tolerance in single node |
| **PostgreSQL** (sync rep) | CP | PC/EC | Replica failure blocks writes |
| **MySQL** (async rep) | AP | PA/EL | Can lose committed writes |
| **Cassandra** | AP | PA/EL | Tunable consistency per query |
| **MongoDB** | CP | PC/EC | Default; can tune for AP |
| **DynamoDB** | AP | PA/EL | Eventual consistency default |
| **CockroachDB** | CP | PC/EC | Serializable everywhere |
| **Redis** (cluster) | AP | PA/EL | Async replication |

---

## 4. Consensus and Coordination

### Why Consensus Matters

```
SCENARIO: Primary database fails, need to elect new primary

Without consensus:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Replica A: "I should be primary!"                                  â”‚
â”‚  Replica B: "No, I should be primary!"                              â”‚
â”‚  Replica C: "I have the most recent data!"                          â”‚
â”‚                                                                      â”‚
â”‚  RESULT: Split brain ğŸ§ ğŸ’¥ğŸ§                                          â”‚
â”‚  Multiple nodes think they're primary                               â”‚
â”‚  Clients write to different "primaries"                             â”‚
â”‚  Data divergence and corruption!                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

With consensus (Raft/Paxos):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Nodes vote, majority agrees on ONE leader                          â”‚
â”‚  All writes go through single agreed leader                         â”‚
â”‚  No split brain possible                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Raft Consensus (Simplified)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RAFT CONSENSUS                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  THREE ROLES:                                                        â”‚
â”‚  â€¢ Leader: Handles all writes, sends heartbeats                      â”‚
â”‚  â€¢ Follower: Replicates from leader, votes in elections              â”‚
â”‚  â€¢ Candidate: Requesting votes to become leader                      â”‚
â”‚                                                                      â”‚
â”‚  LEADER ELECTION:                                                    â”‚
â”‚  1. Leader stops sending heartbeats (crashed or partitioned)         â”‚
â”‚  2. Followers timeout, become candidates                             â”‚
â”‚  3. Candidates request votes from all nodes                          â”‚
â”‚  4. Candidate with majority votes becomes new leader                 â”‚
â”‚  5. New leader starts sending heartbeats                             â”‚
â”‚                                                                      â”‚
â”‚  LOG REPLICATION (uses QUORUM internally!):                          â”‚
â”‚  1. Client sends write to leader                                     â”‚
â”‚  2. Leader appends to its log, sends to followers                    â”‚
â”‚  3. Followers append to their logs, send ACK                         â”‚
â”‚  4. Once majority ACKs, leader commits  â† This is W = majority!     â”‚
â”‚  5. Leader notifies followers to commit                              â”‚
â”‚  6. Client gets success response                                     â”‚
â”‚                                                                      â”‚
â”‚  NOTE: Raft uses quorum (majority = N/2 + 1) for commits.           â”‚
â”‚  Difference from Dynamo-style quorum: Raft has a LEADER,            â”‚
â”‚  Dynamo is leaderless (any node can accept writes).                 â”‚
â”‚                                                                      â”‚
â”‚  SAFETY GUARANTEES:                                                  â”‚
â”‚  â€¢ At most one leader per term                                       â”‚
â”‚  â€¢ Leader's log is always "most complete"                            â”‚
â”‚  â€¢ Committed entries are never lost                                  â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Consensus Use Cases

```
1. LEADER ELECTION
   â€¢ Database primary failover
   â€¢ Kafka partition leader
   â€¢ Distributed lock services

2. CONFIGURATION MANAGEMENT  
   â€¢ Cluster membership (who's in/out)
   â€¢ Schema changes
   â€¢ Feature flags

3. DISTRIBUTED LOCKS
   â€¢ Exactly-once processing
   â€¢ Resource allocation
   â€¢ Fencing tokens (see below)

4. ATOMIC BROADCAST
   â€¢ Total ordering of messages
   â€¢ Replicated state machines
   â€¢ Distributed transactions (2PC/3PC)
```

### Distributed Locks vs Database Locks

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        DATABASE LOCKS (Level 2)  vs  DISTRIBUTED LOCKS (Level 3)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  DATABASE LOCKS (What we covered in Level 2):                               â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                              â”‚
â”‚  â€¢ Managed BY the database (PostgreSQL, MySQL)                              â”‚
â”‚  â€¢ Scope: ONE database, transactions within that database                   â”‚
â”‚  â€¢ Use case: Prevent lost updates, ensure isolation                        â”‚
â”‚  â€¢ Examples: SELECT...FOR UPDATE, S-Lock, X-Lock, 2PL                      â”‚
â”‚  â€¢ Automatic: Database handles lock acquisition/release                    â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Service A                                                          â”‚   â”‚
â”‚  â”‚     â”‚                                                               â”‚   â”‚
â”‚  â”‚     â””â”€â”€â”€â”€â–º PostgreSQL â”€â”€â”€ SELECT...FOR UPDATE â”€â”€â”€â–º row locked      â”‚   â”‚
â”‚  â”‚             (manages locks internally)                              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                              â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚                                                                              â”‚
â”‚  DISTRIBUTED LOCKS (This level):                                            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                          â”‚
â”‚  â€¢ Managed by EXTERNAL lock service (Redis, ZooKeeper, etcd)               â”‚
â”‚  â€¢ Scope: MULTIPLE services/servers that DON'T share a database            â”‚
â”‚  â€¢ Use case: Coordinate actions when there's NO shared database            â”‚
â”‚  â€¢ Examples: Redis SETNX, Redlock, ZooKeeper locks                         â”‚
â”‚  â€¢ Manual: YOU must handle expiry, renewal, fencing tokens                 â”‚
â”‚                                                                              â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚                                                                              â”‚
â”‚  CONCRETE EXAMPLE: Cron Job on Multiple Servers                            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                              â”‚
â”‚  SCENARIO: "Send daily email digest at 9 AM"                               â”‚
â”‚  You have 5 app servers, each with a cron job scheduled for 9 AM.         â”‚
â”‚                                                                              â”‚
â”‚  âš ï¸  WHY LOAD BALANCER CAN'T HELP:                                          â”‚
â”‚  â€¢ Cron jobs are INTERNAL scheduled tasks, not external requests           â”‚
â”‚  â€¢ Each server's cron daemon triggers independently at 9 AM                â”‚
â”‚  â€¢ There's no incoming HTTP request for load balancer to route!            â”‚
â”‚                                                                              â”‚
â”‚  WITHOUT coordination: All 5 servers wake up at 9 AM â†’ 5 emails sent!     â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                                                                       â”‚ â”‚
â”‚  â”‚  At 9:00 AM, ALL servers wake up independently:                       â”‚ â”‚
â”‚  â”‚                                                                       â”‚ â”‚
â”‚  â”‚  Server 1 â”€â”€â”€â”                                                        â”‚ â”‚
â”‚  â”‚  Server 2 â”€â”€â”€â”¤     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚ â”‚
â”‚  â”‚  Server 3 â”€â”€â”€â”¼â”€â”€â”€â”€â–ºâ”‚  Redis           â”‚   Only ONE gets the lock!    â”‚ â”‚
â”‚  â”‚  Server 4 â”€â”€â”€â”¤     â”‚  "daily_email"   â”‚   That one sends the email.  â”‚ â”‚
â”‚  â”‚  Server 5 â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   Others see lock, skip.     â”‚ â”‚
â”‚  â”‚                                                                       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                              â”‚
â”‚  CODE:                                                                      â”‚
â”‚  # Each server runs this at 9 AM via cron                                  â”‚
â”‚  if redis.setnx("daily_email_lock", server_id, expiry=60):                 â”‚
â”‚      send_daily_email()                                                    â”‚
â”‚      redis.delete("daily_email_lock")                                      â”‚
â”‚  else:                                                                      â”‚
â”‚      # Another server already has the lock, skip                           â”‚
â”‚                                                                              â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚                                                                              â”‚
â”‚  SIMPLE LOCKS vs CONSENSUS-BASED LOCKS:                                    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                              â”‚
â”‚  SIMPLE LOCK (Redis SETNX):                                                â”‚
â”‚  â€¢ Single Redis server manages the lock                                    â”‚
â”‚  â€¢ Fast and simple                                                         â”‚
â”‚  â€¢ âŒ Single point of failure (Redis dies = lock lost)                     â”‚
â”‚  â€¢ âŒ NOT using consensus                                                  â”‚
â”‚  â€¢ Good for: Non-critical tasks (email digest, cache refresh)             â”‚
â”‚                                                                              â”‚
â”‚  CONSENSUS-BASED LOCK (ZooKeeper, etcd):                                   â”‚
â”‚  â€¢ Lock state replicated across multiple nodes using Raft/Paxos           â”‚
â”‚  â€¢ âœ… Survives node failures (majority must agree)                         â”‚
â”‚  â€¢ âœ… Uses consensus internally                                            â”‚
â”‚  â€¢ Slower, more complex                                                    â”‚
â”‚  â€¢ Good for: Critical tasks (payment processing, leader election)         â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â”‚  Redis Lock:         ZooKeeper Lock:                               â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚   â”‚
â”‚  â”‚  â”‚ Redis  â”‚           â”‚ ZK Nodeâ”‚â—„â”€â”€â”                               â”‚   â”‚
â”‚  â”‚  â”‚(single)â”‚           â”‚   A    â”‚   â”‚ Raft/Paxos                   â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ replication                  â”‚   â”‚
â”‚  â”‚      â”‚                    â–²        â”‚                               â”‚   â”‚
â”‚  â”‚      â”‚                    â”‚        â”‚                               â”‚   â”‚
â”‚  â”‚  If Redis dies,       â”Œâ”€â”€â”´â”€â”€â”€â”€â” â”Œâ”€â”´â”€â”€â”€â”€â”€â”€â”                        â”‚   â”‚
â”‚  â”‚  lock is lost!        â”‚ZK Nodeâ”‚ â”‚ZK Node â”‚                        â”‚   â”‚
â”‚  â”‚                       â”‚   B   â”‚ â”‚   C    â”‚                        â”‚   â”‚
â”‚  â”‚                       â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚   â”‚
â”‚  â”‚                       If A dies, B or C takes over!              â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                              â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚                                                                              â”‚
â”‚  WHY CAN'T WE USE DATABASE LOCK HERE?                                       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                              â”‚
â”‚  â€¢ There's no "row" to lock - we're coordinating a TASK, not data         â”‚
â”‚  â€¢ The job might call external APIs, not just database                    â”‚
â”‚  â€¢ We need a lock BEFORE we decide what to do                             â”‚
â”‚  â€¢ Database lock = "lock this data" | Distributed lock = "lock this task" â”‚
â”‚                                                                              â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚                                                                              â”‚
â”‚  WHEN TO USE WHICH:                                                         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                              â”‚
â”‚  DATABASE LOCKS:                                                            â”‚
â”‚  â€¢ "Lock this ROW while I update it"                                      â”‚
â”‚  â€¢ Inventory decrement, bank transfer, booking seat                       â”‚
â”‚  â€¢ You're protecting DATA in a database                                   â”‚
â”‚                                                                              â”‚
â”‚  DISTRIBUTED LOCKS:                                                         â”‚
â”‚  â€¢ "Only ONE server should run this TASK"                                 â”‚
â”‚  â€¢ Cron jobs, scheduled tasks, batch processing                           â”‚
â”‚  â€¢ Leader election (only one server is "active")                          â”‚
â”‚  â€¢ You're coordinating WORK across servers                                â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Fencing Tokens (Preventing Zombie Leaders)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    THE PROBLEM: ZOMBIE LEADER                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  SCENARIO: Leader election for a database cluster                          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                              â”‚
â”‚  You have 3 servers: A, B, C. Only ONE can be leader (accept writes).      â”‚
â”‚  ZooKeeper manages leader election.                                        â”‚
â”‚                                                                              â”‚
â”‚  T1: Server A is elected leader                                            â”‚
â”‚  T2: Server A accepts writes, sends to storage                             â”‚
â”‚  T3: Server A hits GC pause / network partition (stuck!)                   â”‚
â”‚  T4: ZooKeeper: "A is unresponsive, elect new leader"                      â”‚
â”‚  T5: Server B becomes new leader                                           â”‚
â”‚  T6: Server B accepts writes, sends to storage                             â”‚
â”‚  T7: Server A wakes up, STILL THINKS IT'S THE LEADER! ğŸ˜±                   â”‚
â”‚  T8: Server A accepts a write â†’ CONFLICTS with B's writes!                 â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â”‚  Timeline:                                                          â”‚   â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â”‚  Server A: [LEADER]â”€â”€writesâ”€â”€â–º[GC PAUSE ğŸ’¤]â”€â”€wakes upâ”€â”€â–º[writes!]  â”‚   â”‚
â”‚  â”‚                                      â”‚                    â†“         â”‚   â”‚
â”‚  â”‚  ZooKeeper:               [A dead?]â”€â”€â”´â”€â”€[elect B]        CONFLICT!  â”‚   â”‚
â”‚  â”‚                                             â”‚             â†‘         â”‚   â”‚
â”‚  â”‚  Server B:                           [LEADER]â”€â”€â”€writesâ”€â”€â”€â”˜         â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â”‚  RESULT: Two servers both think they're leader!                    â”‚   â”‚
â”‚  â”‚          Data corruption, split-brain!                             â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    THE SOLUTION: FENCING TOKENS                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  IDEA: Every leader election gets a monotonically increasing token (epoch).â”‚
â”‚        Storage rejects writes from old leaders with outdated tokens.       â”‚
â”‚                                                                              â”‚
â”‚  HOW IT WORKS:                                                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                              â”‚
â”‚  T1: Server A elected â†’ gets epoch #33                                     â”‚
â”‚  T2: Server A writes with epoch #33 â†’ Storage accepts, stores epoch=33    â”‚
â”‚  T5: Server B elected â†’ gets epoch #34                                     â”‚
â”‚  T6: Server B writes with epoch #34 â†’ Storage updates epoch=34            â”‚
â”‚  T7: Server A wakes up, writes with epoch #33                              â”‚
â”‚  T8: Storage: "33 < 34? REJECTED! You're not the leader anymore!"         â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â”‚  Server A        ZooKeeper           Storage           Server B    â”‚   â”‚
â”‚  â”‚     â”‚                â”‚                   â”‚                  â”‚       â”‚   â”‚
â”‚  â”‚     â”‚â—„â”€leader #33â”€â”€â”€â”€â”‚                   â”‚                  â”‚       â”‚   â”‚
â”‚  â”‚     â”‚â”€â”€â”€write+#33â”€â”€â”€â–ºâ”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ epoch=33         â”‚       â”‚   â”‚
â”‚  â”‚     â”‚                â”‚                   â”‚                  â”‚       â”‚   â”‚
â”‚  â”‚     â”‚  ğŸ’¤ PAUSE      â”‚                   â”‚                  â”‚       â”‚   â”‚
â”‚  â”‚     â”‚                â”‚                   â”‚                  â”‚       â”‚   â”‚
â”‚  â”‚     â”‚                â”‚â”€â”€leader #34â”€â”€â”€â”€â”€â”€â–ºâ”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚       â”‚   â”‚
â”‚  â”‚     â”‚                â”‚                   â”‚â—„â”€â”€â”€write+#34â”€â”€â”€â”€â”€â”‚       â”‚   â”‚
â”‚  â”‚     â”‚                â”‚                   â”‚ epoch=34         â”‚       â”‚   â”‚
â”‚  â”‚     â”‚                â”‚                   â”‚                  â”‚       â”‚   â”‚
â”‚  â”‚     â”‚â”€â”€â”€write+#33â”€â”€â”€â–ºâ”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚                  â”‚       â”‚   â”‚
â”‚  â”‚     â”‚                â”‚                   â”‚ 33 < 34          â”‚       â”‚   â”‚
â”‚  â”‚     â”‚â—„â”€â”€â”€REJECTED!â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ NOT LEADER!      â”‚       â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                              â”‚
â”‚  KEY POINTS:                                                                â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                              â”‚
â”‚  1. The fencing token is a monotonically increasing number (epoch/term)    â”‚
â”‚  2. STORAGE must check the token and reject old ones                       â”‚
â”‚  3. ZooKeeper's zxid, Raft's term, Kafka's epoch are all fencing tokens   â”‚
â”‚  4. Without fencing tokens, you get split-brain (two leaders)              â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Distributed Transactions: 2PC vs Saga

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DISTRIBUTED TRANSACTIONS                                  â”‚
â”‚         "How to update multiple services/databases atomically?"             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  PROBLEM: Order service needs to:                                           â”‚
â”‚  1. Deduct inventory (Inventory DB)                                         â”‚
â”‚  2. Charge payment (Payment Service)                                        â”‚
â”‚  3. Create order (Order DB)                                                 â”‚
â”‚                                                                              â”‚
â”‚  If step 2 fails, step 1 must be rolled back! How?                         â”‚
â”‚                                                                              â”‚
â”‚  TWO APPROACHES:                                                            â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚                                                                              â”‚
â”‚  1. TWO-PHASE COMMIT (2PC) - Strong consistency, blocking                  â”‚
â”‚  2. SAGA PATTERN - Eventual consistency, non-blocking                      â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TWO-PHASE COMMIT (2PC)                                    â”‚
â”‚         "All-or-nothing across multiple nodes"                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  PHASE 1: PREPARE (Voting)                                                  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                              â”‚
â”‚  Coordinator â”€â”€â–º "Can you commit?" â”€â”€â–º Participant A                       â”‚
â”‚              â”€â”€â–º "Can you commit?" â”€â”€â–º Participant B                       â”‚
â”‚                                                                              â”‚
â”‚  Participants:                                                              â”‚
â”‚  â€¢ Lock resources                                                           â”‚
â”‚  â€¢ Write to local WAL                                                       â”‚
â”‚  â€¢ Reply YES (prepared) or NO (abort)                                      â”‚
â”‚                                                                              â”‚
â”‚  PHASE 2: COMMIT (Decision)                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                              â”‚
â”‚  If ALL said YES:                                                           â”‚
â”‚     Coordinator â”€â”€â–º "COMMIT" â”€â”€â–º All participants commit                   â”‚
â”‚                                                                              â”‚
â”‚  If ANY said NO:                                                            â”‚
â”‚     Coordinator â”€â”€â–º "ABORT" â”€â”€â–º All participants rollback                  â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Coordinator        Inventory DB       Payment         Order DB    â”‚   â”‚
â”‚  â”‚      â”‚                  â”‚                â”‚                â”‚         â”‚   â”‚
â”‚  â”‚      â”‚â”€â”€ PREPARE â”€â”€â”€â”€â”€â”€â–ºâ”‚                â”‚                â”‚         â”‚   â”‚
â”‚  â”‚      â”‚â”€â”€ PREPARE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚                â”‚         â”‚   â”‚
â”‚  â”‚      â”‚â”€â”€ PREPARE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚         â”‚   â”‚
â”‚  â”‚      â”‚                  â”‚                â”‚                â”‚         â”‚   â”‚
â”‚  â”‚      â”‚â—„â”€â”€ YES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                â”‚                â”‚         â”‚   â”‚
â”‚  â”‚      â”‚â—„â”€â”€ YES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                â”‚         â”‚   â”‚
â”‚  â”‚      â”‚â—„â”€â”€ YES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚         â”‚   â”‚
â”‚  â”‚      â”‚                  â”‚                â”‚                â”‚         â”‚   â”‚
â”‚  â”‚      â”‚â”€â”€ COMMIT â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚                â”‚                â”‚         â”‚   â”‚
â”‚  â”‚      â”‚â”€â”€ COMMIT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚                â”‚         â”‚   â”‚
â”‚  â”‚      â”‚â”€â”€ COMMIT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                              â”‚
â”‚  âœ… PROS: Strong consistency (ACID across nodes)                           â”‚
â”‚  âŒ CONS:                                                                   â”‚
â”‚     â€¢ BLOCKING: Locks held during entire protocol                          â”‚
â”‚     â€¢ Coordinator is SPOF (if crashes during phase 2, participants stuck)  â”‚
â”‚     â€¢ High latency (2 network round-trips minimum)                         â”‚
â”‚     â€¢ Doesn't scale well (all participants must be available)              â”‚
â”‚                                                                              â”‚
â”‚  USED BY: Traditional databases, XA transactions, some NewSQL              â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SAGA PATTERN                                              â”‚
â”‚         "Sequence of local transactions with compensating actions"          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  IDEA: Break distributed transaction into local transactions.              â”‚
â”‚  If one fails, execute COMPENSATING TRANSACTIONS to undo previous steps.   â”‚
â”‚                                                                              â”‚
â”‚  EXAMPLE: Order Saga                                                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â”‚  HAPPY PATH:                                                        â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â”‚  T1: Reserve Inventory â”€â”€â–º T2: Charge Payment â”€â”€â–º T3: Create Order â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â”‚  FAILURE (Payment fails):                                          â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â”‚  T1: Reserve Inventory â”€â”€â–º T2: Charge Payment âœ—                    â”‚   â”‚
â”‚  â”‚           â”‚                       â”‚                                 â”‚   â”‚
â”‚  â”‚           â”‚â—„â”€â”€â”€â”€ C1: Release â—„â”€â”€â”€â”€â”˜                                â”‚   â”‚
â”‚  â”‚                  Inventory                                          â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â”‚  Each step has a COMPENSATING action:                              â”‚   â”‚
â”‚  â”‚  T1: Reserve Inventory  â†’  C1: Release Inventory                   â”‚   â”‚
â”‚  â”‚  T2: Charge Payment     â†’  C2: Refund Payment                      â”‚   â”‚
â”‚  â”‚  T3: Create Order       â†’  C3: Cancel Order                        â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                              â”‚
â”‚  TWO COORDINATION STYLES:                                                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                              â”‚
â”‚  CHOREOGRAPHY: Services emit events, others react                          â”‚
â”‚  â€¢ Order â†’ "InventoryReserved" â†’ Payment â†’ "PaymentCharged" â†’ ...         â”‚
â”‚  â€¢ No central coordinator, but complex to track                            â”‚
â”‚                                                                              â”‚
â”‚  ORCHESTRATION: Central saga orchestrator directs each step                â”‚
â”‚  â€¢ Orchestrator calls each service in sequence                             â”‚
â”‚  â€¢ Easier to understand, single point to monitor                           â”‚
â”‚                                                                              â”‚
â”‚  âœ… PROS:                                                                   â”‚
â”‚     â€¢ Non-blocking (no long-held locks)                                    â”‚
â”‚     â€¢ Better availability (services can be temporarily down)               â”‚
â”‚     â€¢ Scales better than 2PC                                               â”‚
â”‚                                                                              â”‚
â”‚  âŒ CONS:                                                                   â”‚
â”‚     â€¢ Eventual consistency (not ACID)                                      â”‚
â”‚     â€¢ Complex to implement (compensating logic)                            â”‚
â”‚     â€¢ Harder to debug                                                      â”‚
â”‚                                                                              â”‚
â”‚  USED BY: Microservices, e-commerce, booking systems                       â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    2PC vs SAGA: WHEN TO USE                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Aspect          â”‚ 2PC                 â”‚ Saga                           â”‚â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚
â”‚  â”‚ Consistency     â”‚ Strong (ACID)       â”‚ Eventual                       â”‚â”‚
â”‚  â”‚ Availability    â”‚ Lower (blocking)    â”‚ Higher (non-blocking)          â”‚â”‚
â”‚  â”‚ Latency         â”‚ Higher (2 RTTs)     â”‚ Lower (async possible)         â”‚â”‚
â”‚  â”‚ Complexity      â”‚ Protocol is complex â”‚ Compensation logic is complex  â”‚â”‚
â”‚  â”‚ Scale           â”‚ Limited             â”‚ Better                         â”‚â”‚
â”‚  â”‚ Isolation       â”‚ Yes (locks)         â”‚ No (dirty reads possible)      â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                              â”‚
â”‚  USE 2PC WHEN:                                                              â”‚
â”‚  â€¢ Strong consistency is mandatory (financial transactions)                â”‚
â”‚  â€¢ Few participants, low latency requirements                              â”‚
â”‚  â€¢ Within a single database cluster (not across microservices)            â”‚
â”‚                                                                              â”‚
â”‚  USE SAGA WHEN:                                                             â”‚
â”‚  â€¢ High availability is priority                                           â”‚
â”‚  â€¢ Microservices architecture                                              â”‚
â”‚  â€¢ Can tolerate eventual consistency                                       â”‚
â”‚  â€¢ Long-running transactions (minutes/hours)                               â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5. Conflict Resolution & Anti-Entropy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ğŸ“ CONNECTION TO LEVEL 2 (MVCC)                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  In Level 2, we covered MVCC for SINGLE-NODE concurrency:                   â”‚
â”‚  â€¢ Transaction IDs (xmin/xmax) to track row versions                        â”‚
â”‚  â€¢ Visibility rules for snapshot isolation                                  â”‚
â”‚  â€¢ Readers never block writers on ONE server                                â”‚
â”‚                                                                              â”‚
â”‚  NOW WE EXTEND TO DISTRIBUTED:                                               â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                                              â”‚
â”‚  Single-Node MVCC              Distributed Challenge                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                         â”‚
â”‚  Local transaction ID    â†’     Need GLOBAL timestamp ordering               â”‚
â”‚  One copy of data        â†’     Multiple copies, may diverge                 â”‚
â”‚  Visibility = "committed?"â†’    Visibility = "committed on which nodes?"     â”‚
â”‚  No conflicts (one writer)â†’    Conflicts when multiple nodes write          â”‚
â”‚                                                                              â”‚
â”‚  DISTRIBUTED MVCC SOLUTIONS:                                                â”‚
â”‚  â€¢ Hybrid Logical Clocks (HLC) - CockroachDB, YugabyteDB                   â”‚
â”‚  â€¢ TrueTime (GPS + atomic clocks) - Google Spanner                         â”‚
â”‚  â€¢ Vector Clocks - Riak, Dynamo                                             â”‚
â”‚  â€¢ Conflict Resolution - LWW, CRDTs, application logic                     â”‚
â”‚                                                                              â”‚
â”‚  ğŸ‘‰ This section covers what happens when versions CONFLICT across nodes.   â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### The Two Big Problems in Distributed Data

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TWO CHALLENGES, DIFFERENT SOLUTIONS                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  When you have MULTIPLE NODES that can accept writes, you face:             â”‚
â”‚                                                                              â”‚
â”‚  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  â”‚
â”‚  â•‘  PROBLEM 1: WRITE CONFLICTS                                           â•‘  â”‚
â”‚  â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•‘  â”‚
â”‚  â•‘  "Two users update the SAME key on DIFFERENT nodes at the same time" â•‘  â”‚
â”‚  â•‘                                                                       â•‘  â”‚
â”‚  â•‘  Node A: name = "Alice"    Node B: name = "Alicia"                   â•‘  â”‚
â”‚  â•‘                    â†˜             â†™                                    â•‘  â”‚
â”‚  â•‘                      WHICH WINS?                                      â•‘  â”‚
â”‚  â•‘                                                                       â•‘  â”‚
â”‚  â•‘  SOLUTIONS: LWW, Vector Clocks, CRDTs (see below)                    â•‘  â”‚
â”‚  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  â”‚
â”‚                                                                              â”‚
â”‚  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  â”‚
â”‚  â•‘  PROBLEM 2: REPLICA DIVERGENCE                                        â•‘  â”‚
â”‚  â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•‘  â”‚
â”‚  â•‘  "Replicas get out of sync due to failures, delays, or partitions"   â•‘  â”‚
â”‚  â•‘                                                                       â•‘  â”‚
â”‚  â•‘  Node A: version 5 âœ“       Node B: version 5 âœ“       Node C: version 3 â†STALE!â•‘
â”‚  â•‘                                                                       â•‘  â”‚
â”‚  â•‘  SOLUTIONS: Three layers of repair (see below)                       â•‘  â”‚
â”‚  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  â”‚
â”‚                                                                              â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚                                                                              â”‚
â”‚  THIS SECTION'S STORYLINE:                                                  â”‚
â”‚                                                                              â”‚
â”‚  1. WRITE CONFLICTS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚     â”‚                                                                       â”‚
â”‚     â”œâ”€â”€ Last-Write-Wins (simple, but loses data)                           â”‚
â”‚     â”œâ”€â”€ Vector Clocks (detect conflicts, let client resolve)              â”‚
â”‚     â””â”€â”€ CRDTs (auto-merge, no conflicts by design)                        â”‚
â”‚                                                                              â”‚
â”‚  2. REPLICA DIVERGENCE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚     â”‚                                                                       â”‚
â”‚     â”œâ”€â”€ Hinted Handoff (PREVENT divergence during temp failures)          â”‚
â”‚     â”‚   â””â”€â”€ "Node is down? Store hint, deliver when it's back"            â”‚
â”‚     â”‚                                                                       â”‚
â”‚     â”œâ”€â”€ Read Repair (FIX divergence on-demand during reads)               â”‚
â”‚     â”‚   â””â”€â”€ "Reading and found stale replica? Update it!"                 â”‚
â”‚     â”‚                                                                       â”‚
â”‚     â””â”€â”€ Anti-Entropy + Merkle Trees (FIX divergence in background)        â”‚
â”‚         â””â”€â”€ "Periodically compare and sync all replicas"                  â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Part 1: Write Conflicts

> When multiple nodes accept writes, which value wins?

### The Conflict Problem

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    WRITE CONFLICT SCENARIO                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  Multi-leader or leaderless system (e.g., Cassandra, DynamoDB)              â”‚
â”‚                                                                              â”‚
â”‚  TIME    NODE A (US)              NODE B (EU)                                â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚  T1      User sets name="Alice"   (accepts locally, ACKs client)            â”‚
â”‚  T2                               User sets name="Alicia" (accepts locally) â”‚
â”‚  T3      â†â”€â”€â”€â”€ async background replication â”€â”€â”€â”€â†’                           â”‚
â”‚          (Leaders continuously stream changes to each other)                â”‚
â”‚                                                                              â”‚
â”‚  QUESTION: What is the user's name now? "Alice" or "Alicia"?                â”‚
â”‚                                                                              â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚                                                                              â”‚
â”‚  WHEN DOES SYNC HAPPEN? (Multi-Leader)                                      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                              â”‚
â”‚  YES, both leaders sync! But ASYNCHRONOUSLY:                                â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â”‚  Leader A (US)                    Leader B (EU)                    â”‚   â”‚
â”‚  â”‚     â”‚                                â”‚                              â”‚   â”‚
â”‚  â”‚     â”‚â—„â”€â”€â”€ Background replication â”€â”€â”€â–ºâ”‚                              â”‚   â”‚
â”‚  â”‚     â”‚     (bidirectional, async)     â”‚                              â”‚   â”‚
â”‚  â”‚     â”‚                                â”‚                              â”‚   â”‚
â”‚  â”‚  HOW:                                                               â”‚   â”‚
â”‚  â”‚  â€¢ Each leader has a "change stream" or "replication log"          â”‚   â”‚
â”‚  â”‚  â€¢ Continuously sends new writes to other leaders                  â”‚   â”‚
â”‚  â”‚  â€¢ When conflicting writes arrive â†’ CONFLICT RESOLUTION kicks in   â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â”‚  TIMING:                                                            â”‚   â”‚
â”‚  â”‚  â€¢ Usually milliseconds to seconds (depends on network)            â”‚   â”‚
â”‚  â”‚  â€¢ During partition: changes queue up, sync when healed            â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                              â”‚
â”‚  âš ï¸  KEY INSIGHT: Conflicts are DETECTED during sync, not during write!    â”‚
â”‚  Each leader happily accepts its write. Only when they exchange data       â”‚
â”‚  do they realize "oops, we both wrote to the same key!"                    â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Conflict Resolution Strategies

#### 1. Last-Write-Wins (LWW)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LAST-WRITE-WINS (LWW)                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  RULE: The write with the highest timestamp wins.                           â”‚
â”‚                                                                              â”‚
â”‚  Node A: name="Alice",  timestamp=1000                                       â”‚
â”‚  Node B: name="Alicia", timestamp=1002  â† HIGHER, THIS WINS!                â”‚
â”‚                                                                              â”‚
â”‚  RESULT: name="Alicia"                                                       â”‚
â”‚                                                                              â”‚
â”‚  âœ… PROS:                                                                    â”‚
â”‚  â€¢ Simple to implement                                                       â”‚
â”‚  â€¢ No conflict resolution logic needed                                       â”‚
â”‚  â€¢ Deterministic (all nodes reach same result)                               â”‚
â”‚                                                                              â”‚
â”‚  âŒ CONS:                                                                    â”‚
â”‚  â€¢ DATA LOSS! The "Alice" write is silently discarded                       â”‚
â”‚  â€¢ Clock synchronization issues (what if Node A's clock is ahead?)          â”‚
â”‚  â€¢ Concurrent writes = random winner (depends on clock)                     â”‚
â”‚                                                                              â”‚
â”‚  USED BY: Cassandra (default), DynamoDB                                     â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 2. Vector Clocks (Detect Conflicts, Don't Auto-Resolve)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    VECTOR CLOCKS                                             â”‚
â”‚         "Track causality, detect concurrent writes"                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  CONCEPT: Each node maintains a counter. Vector = [NodeA:count, NodeB:count]â”‚
â”‚                                                                              â”‚
â”‚  EXAMPLE:                                                                    â”‚
â”‚                                                                              â”‚
â”‚  STEP 1: Initial write on Node A                                            â”‚
â”‚          name="Alice", vector=[A:1, B:0]                                     â”‚
â”‚                                                                              â”‚
â”‚  STEP 2: Sync to Node B                                                      â”‚
â”‚          Node B receives: name="Alice", vector=[A:1, B:0]                    â”‚
â”‚                                                                              â”‚
â”‚  STEP 3: Node B updates (knows about A:1)                                   â”‚
â”‚          name="Alice B.", vector=[A:1, B:1]                                  â”‚
â”‚                                                                              â”‚
â”‚  STEP 4: Meanwhile, Node A also updates (doesn't know B:1)                  â”‚
â”‚          name="Alice A.", vector=[A:2, B:0]                                  â”‚
â”‚                                                                              â”‚
â”‚  STEP 5: Sync - CONFLICT DETECTED!                                          â”‚
â”‚          Version 1: [A:1, B:1] - has B:1 but only A:1                       â”‚
â”‚          Version 2: [A:2, B:0] - has A:2 but no B                           â”‚
â”‚          Neither "happens before" the other â†’ CONCURRENT!                   â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ COMPARISON RULES:                                                       â”‚â”‚
â”‚  â”‚                                                                         â”‚â”‚
â”‚  â”‚ [A:1, B:1] vs [A:2, B:0]                                                â”‚â”‚
â”‚  â”‚                                                                         â”‚â”‚
â”‚  â”‚ Is [A:1,B:1] â‰¤ [A:2,B:0]? NO (B:1 > B:0)                                â”‚â”‚
â”‚  â”‚ Is [A:2,B:0] â‰¤ [A:1,B:1]? NO (A:2 > A:1)                                â”‚â”‚
â”‚  â”‚                                                                         â”‚â”‚
â”‚  â”‚ Neither dominates â†’ CONCURRENT WRITES â†’ CONFLICT!                       â”‚â”‚
â”‚  â”‚                                                                         â”‚â”‚
â”‚  â”‚ If [A:1,B:0] vs [A:2,B:1]:                                              â”‚â”‚
â”‚  â”‚ Is [A:1,B:0] â‰¤ [A:2,B:1]? YES (1â‰¤2, 0â‰¤1) â†’ [A:2,B:1] is NEWER          â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                              â”‚
â”‚  ON CONFLICT: Return BOTH versions to client, let them resolve!            â”‚
â”‚               (Amazon's shopping cart: merge items from both)               â”‚
â”‚                                                                              â”‚
â”‚  USED BY: Riak, Amazon Dynamo (original paper)                              â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 3. CRDTs (Conflict-Free Replicated Data Types)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CRDTs                                                     â”‚
â”‚         "Design data structures that auto-merge without conflicts"          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  IDEA: Use data structures where ALL merge orders give SAME result          â”‚
â”‚                                                                              â”‚
â”‚  EXAMPLE: G-Counter (Grow-only Counter)                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚                                                                              â”‚
â”‚  Each node keeps its own counter:                                           â”‚
â”‚                                                                              â”‚
â”‚  Node A: {A: 5, B: 0, C: 0}  â†’ A incremented 5 times                        â”‚
â”‚  Node B: {A: 0, B: 3, C: 0}  â†’ B incremented 3 times                        â”‚
â”‚  Node C: {A: 0, B: 0, C: 2}  â†’ C incremented 2 times                        â”‚
â”‚                                                                              â”‚
â”‚  MERGE: Take MAX of each component                                          â”‚
â”‚  Result: {A: 5, B: 3, C: 2}  â†’ Total = 5 + 3 + 2 = 10                       â”‚
â”‚                                                                              â”‚
â”‚  No matter what order nodes sync, result is always 10!                      â”‚
â”‚                                                                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚  EXAMPLE: LWW-Register (Last-Writer-Wins Register)                          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚                                                                              â”‚
â”‚  Each value has timestamp:                                                  â”‚
â”‚  Node A: {value: "Alice", ts: 1000}                                         â”‚
â”‚  Node B: {value: "Bob", ts: 1005}                                           â”‚
â”‚                                                                              â”‚
â”‚  MERGE: Keep value with highest timestamp                                   â”‚
â”‚  Result: {value: "Bob", ts: 1005}                                           â”‚
â”‚                                                                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚  EXAMPLE: OR-Set (Observed-Remove Set)                                      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚                                                                              â”‚
â”‚  Add wins over concurrent remove (for shopping carts):                      â”‚
â”‚  Node A: ADD "item1"                                                        â”‚
â”‚  Node B: REMOVE "item1" (concurrent)                                        â”‚
â”‚                                                                              â”‚
â”‚  MERGE: Item stays! (Add wins, better UX for e-commerce)                    â”‚
â”‚                                                                              â”‚
â”‚  USED BY: Redis CRDT, Riak, Automerge, Yjs (collaborative editing)         â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Part 2: Keeping Replicas in Sync (Handling Divergence)

> Conflicts are about "which value wins?" But what about replicas that simply missed an update due to failures or network issues? We need mechanisms to detect and repair divergence.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    THREE LAYERS OF REPLICA REPAIR                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  Think of these as defense layers - each handles different scenarios:      â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚                                                                         â”‚â”‚
â”‚  â”‚  LAYER 1: HINTED HANDOFF                                               â”‚â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚â”‚
â”‚  â”‚  WHEN: Node is temporarily DOWN during a write                         â”‚â”‚
â”‚  â”‚  HOW:  Store "hint" on another node, deliver when it recovers          â”‚â”‚
â”‚  â”‚  GOAL: PREVENT divergence before it happens                            â”‚â”‚
â”‚  â”‚                                                                         â”‚â”‚
â”‚  â”‚  Timeline: Write arrives â†’ Node down â†’ Hint stored â†’ Node up â†’ Synced  â”‚â”‚
â”‚  â”‚                                                                         â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                          â†“                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚                                                                         â”‚â”‚
â”‚  â”‚  LAYER 2: READ REPAIR                                                  â”‚â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚â”‚
â”‚  â”‚  WHEN: Client reads data and we notice a stale replica                 â”‚â”‚
â”‚  â”‚  HOW:  Compare versions from multiple replicas, update stale ones      â”‚â”‚
â”‚  â”‚  GOAL: FIX divergence on-demand (for HOT data that gets read often)   â”‚â”‚
â”‚  â”‚                                                                         â”‚â”‚
â”‚  â”‚  Timeline: Read request â†’ Check replicas â†’ Stale found â†’ Update async  â”‚â”‚
â”‚  â”‚                                                                         â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                          â†“                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚                                                                         â”‚â”‚
â”‚  â”‚  LAYER 3: ANTI-ENTROPY (with Merkle Trees)                             â”‚â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚â”‚
â”‚  â”‚  WHEN: Background process runs periodically (every 10 mins)            â”‚â”‚
â”‚  â”‚  HOW:  Compare Merkle tree hashes to find divergent keys efficiently   â”‚â”‚
â”‚  â”‚  GOAL: FIX divergence for COLD data that's rarely read                 â”‚â”‚
â”‚  â”‚                                                                         â”‚â”‚
â”‚  â”‚  Timeline: Every N mins â†’ Compare hashes â†’ Find diff â†’ Sync keys       â”‚â”‚
â”‚  â”‚                                                                         â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                              â”‚
â”‚  WHY ALL THREE?                                                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                              â”‚
â”‚  â€¢ Hinted Handoff: Fast, but hints can expire or be lost                   â”‚
â”‚  â€¢ Read Repair: Fast for hot data, but cold data never gets fixed          â”‚
â”‚  â€¢ Anti-Entropy: Catches everything, but adds background load              â”‚
â”‚                                                                              â”‚
â”‚  Together: Eventually consistent with good performance trade-offs!         â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Layer 1: Hinted Handoff (Preventing Divergence)

> **First line of defense**: When a node is temporarily down, don't let it miss data.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    HINTED HANDOFF                                            â”‚
â”‚         "Write now, sync later when node recovers"                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  PROBLEM:                                                                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                              â”‚
â”‚  You want to write to Node A, but Node A is temporarily down.              â”‚
â”‚                                                                              â”‚
â”‚  OPTIONS:                                                                   â”‚
â”‚  1. Fail the write â†’ Bad for availability!                                 â”‚
â”‚  2. Write only to available nodes â†’ Node A misses data forever?            â”‚
â”‚  3. Hinted Handoff â†’ Write succeeds, A gets data when it recovers âœ“        â”‚
â”‚                                                                              â”‚
â”‚  HOW IT WORKS:                                                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                              â”‚
â”‚  SCENARIO: Write to replicas A, B, C. Node A is down.                      â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                                                                       â”‚ â”‚
â”‚  â”‚  Client                                                               â”‚ â”‚
â”‚  â”‚     â”‚                                                                 â”‚ â”‚
â”‚  â”‚     â”œâ”€â”€writeâ”€â”€â–º Node A (DOWN! âŒ)                                     â”‚ â”‚
â”‚  â”‚     â”‚                                                                 â”‚ â”‚
â”‚  â”‚     â”œâ”€â”€writeâ”€â”€â–º Node B âœ“  (stores normally)                          â”‚ â”‚
â”‚  â”‚     â”‚                                                                 â”‚ â”‚
â”‚  â”‚     â””â”€â”€writeâ”€â”€â–º Node C âœ“  (stores normally)                          â”‚ â”‚
â”‚  â”‚                    â”‚                                                  â”‚ â”‚
â”‚  â”‚                    â””â”€â”€â–º ALSO stores "hint" for Node A:               â”‚ â”‚
â”‚  â”‚                         "When A is back, give it this data"          â”‚ â”‚
â”‚  â”‚                                                                       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                              â”‚
â”‚  LATER, WHEN NODE A RECOVERS:                                               â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                                                                       â”‚ â”‚
â”‚  â”‚  Node A comes back online                                             â”‚ â”‚
â”‚  â”‚     â”‚                                                                 â”‚ â”‚
â”‚  â”‚     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€ Node C sends "hinted" data â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚ â”‚
â”‚  â”‚     â”‚                                              â”‚                  â”‚ â”‚
â”‚  â”‚     â–¼                                              â”‚                  â”‚ â”‚
â”‚  â”‚  Node A now has the data!                    Hint delivered,         â”‚ â”‚
â”‚  â”‚  Replicas A, B, C are in sync.               hint deleted.           â”‚ â”‚
â”‚  â”‚                                                                       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                              â”‚
â”‚  KEY POINTS:                                                                â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                              â”‚
â”‚  1. HINT STORAGE: The hint is stored on another node (C in example)        â”‚
â”‚     - Contains: "for Node A" + the actual data                             â”‚
â”‚     - Temporary: deleted after successful handoff                          â”‚
â”‚                                                                              â”‚
â”‚  2. SLOPPY QUORUM: Can write to "substitute" node if original is down     â”‚
â”‚     - W=2 needed â†’ A down â†’ write to B, C instead                         â”‚
â”‚     - Maintains write availability during failures                         â”‚
â”‚                                                                              â”‚
â”‚  3. LIMITATIONS (Why we need more layers!):                                â”‚
â”‚     - Hints have TTL (e.g., 3 hours in Cassandra)                          â”‚
â”‚     - If A is down too long, hint expires â†’ Layer 2 or 3 must fix it      â”‚
â”‚     - Hints stored in memory/disk â†’ node crash = hints lost               â”‚
â”‚                                                                              â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚                                                                              â”‚
â”‚  âš ï¸  WHICH SYSTEMS USE THIS?                                                â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                              â”‚
â”‚  LEADERLESS (Dynamo-style): âœ… YES - Primary use case!                      â”‚
â”‚  â€¢ Cassandra, Riak, DynamoDB, Voldemort                                    â”‚
â”‚  â€¢ Any node can accept writes, so sloppy quorum makes sense               â”‚
â”‚  â€¢ If preferred replica is down, write to substitute + hint               â”‚
â”‚                                                                              â”‚
â”‚  MULTI-LEADER: âœ… Sometimes (within each leader's replica set)             â”‚
â”‚  â€¢ Each leader may have followers; hinted handoff can apply there         â”‚
â”‚                                                                              â”‚
â”‚  SINGLE-LEADER: âŒ NOT applicable                                          â”‚
â”‚  â€¢ Only ONE node (leader) accepts writes                                   â”‚
â”‚  â€¢ If leader fails â†’ FAILOVER (elect new leader via consensus)            â”‚
â”‚  â€¢ No "substitute node" concept - writes blocked until new leader         â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Single-Leader Failure:        Leaderless Failure:                 â”‚   â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                 â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â”‚  Leader down?                  Node A down?                        â”‚   â”‚
â”‚  â”‚     â†“                             â†“                                â”‚   â”‚
â”‚  â”‚  WAIT for failover!           Write to B, C instead!              â”‚   â”‚
â”‚  â”‚  (Raft elects new leader)     (Store hint for A)                  â”‚   â”‚
â”‚  â”‚     â†“                             â†“                                â”‚   â”‚
â”‚  â”‚  Writes blocked briefly       Writes continue! âœ“                  â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                              â”‚
â”‚  USED BY: Cassandra, Riak, DynamoDB, Voldemort (all leaderless)            â”‚
â”‚                                                                              â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚                                                                              â”‚
â”‚  SLOPPY QUORUM VISUALIZATION:                                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                              â”‚
â”‚  STRICT QUORUM (W=2 of A, B, C):                                           â”‚
â”‚  A down â†’ Can't achieve quorum â†’ WRITE FAILS!                              â”‚
â”‚                                                                              â”‚
â”‚  SLOPPY QUORUM (W=2, any 2 of A, B, C, D, E...):                          â”‚
â”‚  A down â†’ Write to B + D (D holds hint for A) â†’ WRITE SUCCEEDS!           â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â”‚  Ring with 5 nodes, data should go to A, B, C:                     â”‚   â”‚
â”‚  â”‚  (A is down, so we use D as substitute)                            â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â”‚              â”Œâ”€â”€â”€â”                                                  â”‚   â”‚
â”‚  â”‚         â”Œâ”€â”€â”€â”€â”‚ A â”‚â—„â”€â”€â”€ DOWN!                                       â”‚   â”‚
â”‚  â”‚         â”‚    â””â”€â”€â”€â”˜                                                  â”‚   â”‚
â”‚  â”‚       â”Œâ”€â”€â”€â”       â”Œâ”€â”€â”€â”                                            â”‚   â”‚
â”‚  â”‚       â”‚ E â”‚       â”‚ B â”‚ â—„â”€â”€â”€ Gets write (regular replica, NO hint)â”‚   â”‚
â”‚  â”‚       â””â”€â”€â”€â”˜       â””â”€â”€â”€â”˜                                            â”‚   â”‚
â”‚  â”‚         â”‚    â”Œâ”€â”€â”€â”   â”‚                                              â”‚   â”‚
â”‚  â”‚         â””â”€â”€â”€â”€â”‚ D â”‚â—„â”€â”€â”´â”€â”€ Gets write + HINT for A                   â”‚   â”‚
â”‚  â”‚              â””â”€â”€â”€â”˜       (D is SUBSTITUTE for A)                   â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â”‚  WHO STORES HINTS?                                                 â”‚   â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚   â”‚
â”‚  â”‚  â€¢ B: Regular replica â†’ stores data ONLY (no hint)                â”‚   â”‚
â”‚  â”‚  â€¢ D: Substitute for A â†’ stores data + HINT for A                 â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â”‚  WHY? D knows "I'm not supposed to have this data permanently.    â”‚   â”‚
â”‚  â”‚  When A comes back, I need to hand it off." B doesn't need a      â”‚   â”‚
â”‚  â”‚  hint because B is a legitimate replica for this key.             â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â”‚  Result: W=2 achieved (B + D), data not lost!                      â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Layer 2: Read Repair (Fixing Divergence On-Demand)

> **Second line of defense**: When a client reads, check if replicas are in sync. If not, fix them.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    READ REPAIR                                               â”‚
â”‚         "Notice stale data during reads, fix it immediately"                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  SCENARIO: Client reads key "user:100" from 3 replicas (quorum read)       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                                                                       â”‚ â”‚
â”‚  â”‚  Client                           Coordinator                         â”‚ â”‚
â”‚  â”‚     â”‚                                â”‚                                â”‚ â”‚
â”‚  â”‚     â”‚â”€â”€â”€ READ "user:100" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚                                â”‚ â”‚
â”‚  â”‚     â”‚                                â”‚                                â”‚ â”‚
â”‚  â”‚     â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚ â”‚
â”‚  â”‚     â”‚                   â”‚            â”‚            â”‚                   â”‚ â”‚
â”‚  â”‚     â”‚                   â–¼            â–¼            â–¼                   â”‚ â”‚
â”‚  â”‚     â”‚              Replica A    Replica B    Replica C               â”‚ â”‚
â”‚  â”‚     â”‚              version=5    version=5    version=3 â† STALE!      â”‚ â”‚
â”‚  â”‚     â”‚              name=Alice   name=Alice   name=Al                 â”‚ â”‚
â”‚  â”‚     â”‚                   â”‚            â”‚            â”‚                   â”‚ â”‚
â”‚  â”‚     â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚ â”‚
â”‚  â”‚     â”‚                                â”‚                                â”‚ â”‚
â”‚  â”‚     â”‚                     Coordinator notices:                        â”‚ â”‚
â”‚  â”‚     â”‚                     "C has old version!"                        â”‚ â”‚
â”‚  â”‚     â”‚                                â”‚                                â”‚ â”‚
â”‚  â”‚     â”‚â—„â”€â”€â”€â”€ Return version 5 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                                â”‚ â”‚
â”‚  â”‚     â”‚      (Alice)                   â”‚                                â”‚ â”‚
â”‚  â”‚     â”‚                                â”‚                                â”‚ â”‚
â”‚  â”‚     â”‚                  (async) â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â–º Update Replica C       â”‚ â”‚
â”‚  â”‚     â”‚                                         with version 5         â”‚ â”‚
â”‚  â”‚                                                                       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                              â”‚
â”‚  KEY POINTS:                                                                â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                              â”‚
â”‚  1. HAPPENS DURING READS: No extra background process needed               â”‚
â”‚  2. ASYNC UPDATE: Client doesn't wait for repair to complete               â”‚
â”‚  3. GREAT FOR HOT DATA: Frequently read keys get fixed quickly             â”‚
â”‚  4. LIMITATION: Cold data (rarely read) stays stale forever!               â”‚
â”‚     â†’ That's why we need Layer 3 (Anti-Entropy)                            â”‚
â”‚                                                                              â”‚
â”‚  USED BY: Cassandra, Riak, DynamoDB                                        â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Layer 3: Anti-Entropy with Merkle Trees (Background Repair)

> **Third line of defense**: Periodically compare ALL data across replicas and fix any divergence. Catches cold data that read repair misses.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    THE CHALLENGE                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  Node A has 1 billion keys, Node B has 1 billion keys.                      â”‚
â”‚  Some keys might be out of sync. How do we find them efficiently?          â”‚
â”‚                                                                              â”‚
â”‚  NAIVE APPROACH: Compare all 1 billion keys = 1 billion network calls! âŒ   â”‚
â”‚                                                                              â”‚
â”‚  SMART APPROACH: Use Merkle Trees for O(log N) comparisons! âœ“              â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MERKLE TREES                                              â”‚
â”‚         "Hash trees that efficiently detect which data is out of sync"      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  Each node builds a hash tree of its data:                                 â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚                         ROOT HASH                                        â”‚â”‚
â”‚  â”‚                        H(H1 + H2)                                        â”‚â”‚
â”‚  â”‚                      = "abc123..."                                       â”‚â”‚
â”‚  â”‚                       /         \                                        â”‚â”‚
â”‚  â”‚                      /           \                                       â”‚â”‚
â”‚  â”‚               â”Œâ”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”                                â”‚â”‚
â”‚  â”‚               â”‚ H1  â”‚             â”‚ H2  â”‚                                â”‚â”‚
â”‚  â”‚               â”‚H(A+B)             â”‚H(C+D)                                â”‚â”‚
â”‚  â”‚               â””â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”˜                                â”‚â”‚
â”‚  â”‚               /     \             /     \                                â”‚â”‚
â”‚  â”‚              /       \           /       \                               â”‚â”‚
â”‚  â”‚          â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”                              â”‚â”‚
â”‚  â”‚          â”‚ H(A)â”‚ â”‚ H(B)â”‚   â”‚ H(C)â”‚ â”‚ H(D)â”‚                              â”‚â”‚
â”‚  â”‚          â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜                              â”‚â”‚
â”‚  â”‚             â”‚       â”‚         â”‚       â”‚                                  â”‚â”‚
â”‚  â”‚          â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”                              â”‚â”‚
â”‚  â”‚          â”‚Key Aâ”‚ â”‚Key Bâ”‚   â”‚Key Câ”‚ â”‚Key Dâ”‚                              â”‚â”‚
â”‚  â”‚          â”‚=100 â”‚ â”‚=200 â”‚   â”‚=300 â”‚ â”‚=400 â”‚                              â”‚â”‚
â”‚  â”‚          â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜                              â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                              â”‚
â”‚  SYNC PROCESS:                                                               â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                              â”‚
â”‚  1. Node A sends root hash to Node B                                        â”‚
â”‚  2. Node B compares: Same? â†’ In sync! Different? â†’ Drill down               â”‚
â”‚  3. Compare H1 vs H1': Same? â†’ Left subtree in sync                         â”‚
â”‚                        Different? â†’ Check H(A), H(B)                         â”‚
â”‚  4. Find exactly which keys differ: O(log N) comparisons!                   â”‚
â”‚                                                                              â”‚
â”‚  EXAMPLE:                                                                    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚  Node A: 1 billion keys                                                      â”‚
â”‚  Node B: 1 billion keys, but Key C is different                             â”‚
â”‚                                                                              â”‚
â”‚  Without Merkle: Compare 1 billion keys = 1 billion operations              â”‚
â”‚  With Merkle:    Compare ~30 hashes (logâ‚‚ 1 billion â‰ˆ 30)                   â”‚
â”‚                                                                              â”‚
â”‚  USED BY: Cassandra (anti-entropy repair), Bitcoin, Git, IPFS               â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ANTI-ENTROPY PROCESS                                      â”‚
â”‚         "Periodic background job that uses Merkle trees to sync"            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  Every 10 minutes (configurable):                                           â”‚
â”‚                                                                              â”‚
â”‚  1. Node A and Node B exchange Merkle root hashes                           â”‚
â”‚  2. If different, drill down to find divergent keys                         â”‚
â”‚  3. Exchange and reconcile those specific keys                              â”‚
â”‚  4. Both nodes now in sync!                                                 â”‚
â”‚                                                                              â”‚
â”‚  WHY THIS IS LAYER 3 (last resort):                                         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                              â”‚
â”‚  â€¢ Runs periodically (not real-time like Layer 1 & 2)                       â”‚
â”‚  â€¢ Adds background load (computing hashes, network traffic)                 â”‚
â”‚  â€¢ But catches EVERYTHING that Layer 1 & 2 missed!                          â”‚
â”‚                                                                              â”‚
â”‚  PERFECT FOR:                                                               â”‚
â”‚  â€¢ Cold data that's rarely read (read repair never triggered)              â”‚
â”‚  â€¢ Data missed when hints expired (node down too long)                      â”‚
â”‚  â€¢ Recovering from corruption or bugs                                       â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Section 5 Summary

**Part 1: Write Conflict Resolution** (Which value wins?)

| Strategy | Data Loss? | Complexity | Best For |
|----------|------------|------------|----------|
| **Last-Write-Wins** | Yes (silent) | Low | Simple cases, logs |
| **Vector Clocks** | No (client resolves) | Medium | Shopping carts, docs |
| **CRDTs** | No (auto-merge) | High | Counters, sets, collab editing |
| **Application Logic** | No (custom) | High | Domain-specific rules |

**Part 2: Replica Sync Mechanisms** (Keeping replicas consistent)

| Layer | Mechanism | When It Runs | Best For |
|-------|-----------|--------------|----------|
| **1** | Hinted Handoff | During writes (node down) | Temp failures (minutes/hours) |
| **2** | Read Repair | During reads | Hot data (frequently accessed) |
| **3** | Anti-Entropy | Background (periodic) | Cold data, long outages |

```
DECISION FLOW: How does divergence get fixed?

Node goes down during write?
â”œâ”€â”€ YES â†’ Hinted Handoff (Layer 1): Store hint, deliver on recovery
â”‚         â†“
â”‚         Hint expired? (node down too long)
â”‚         â”œâ”€â”€ NO â†’ Node recovers, gets hinted data âœ“
â”‚         â””â”€â”€ YES â†’ Fall through to Layer 2 or 3
â”‚
â””â”€â”€ NO â†’ Data written, but some replicas might be stale
         â†“
         Is this key frequently read?
         â”œâ”€â”€ YES â†’ Read Repair (Layer 2): Fixed during next read âœ“
         â””â”€â”€ NO â†’ Anti-Entropy (Layer 3): Fixed during background sync âœ“
```

---

## 6. Interview Checklist

### Questions You Should Be Able to Answer

#### Replication
- [ ] "What's the difference between sync and async replication?"
- [ ] "How do you handle replication lag for reads?"
- [ ] "What is the CAP theorem? Give an example of a CP and AP system."
- [ ] "How does quorum-based replication work?"

#### Sharding
- [ ] "How would you choose a shard key for a social media app?"
- [ ] "What is a hotspot and how do you prevent it?"
- [ ] "How do you handle cross-shard joins?"
- [ ] "What is consistent hashing and when would you use it?"

#### CAP/PACELC
- [ ] "During a partition, should this system prioritize consistency or availability?"
- [ ] "What trade-offs does eventual consistency create?"
- [ ] "How does PACELC extend CAP?"

### Decision Framework

```
CHOOSING REPLICATION STRATEGY:

Is data loss acceptable?
â”œâ”€â”€ NO â†’ Synchronous replication (or quorum with W = all)
â””â”€â”€ YES â†’ How much lag is acceptable?
          â”œâ”€â”€ Seconds â†’ Async with monitoring
          â””â”€â”€ Minutes â†’ Async, eventual consistency

CHOOSING SHARD KEY:

What's your primary access pattern?
â”œâ”€â”€ User-centric â†’ Shard by user_id
â”œâ”€â”€ Time-series â†’ Shard by time bucket + hash
â”œâ”€â”€ Geographic â†’ Shard by region
â””â”€â”€ Random â†’ Hash-based sharding

Need range queries?
â”œâ”€â”€ YES â†’ Consider range-based sharding (accept hotspot risk)
â””â”€â”€ NO â†’ Hash-based is safer
```

### Common Pitfalls

| Mistake | Why It's Wrong | Correct Understanding |
|---------|----------------|----------------------|
| "CAP means pick 2 of 3" | P is mandatory | Choose C or A during partitions |
| "Sharding is always better" | Adds massive complexity | Only shard when you must |
| "Use celebrity_id as shard key" | Creates hotspots | Shard by content_id or compound key |
| "Sync replication = no data loss" | Can still lose in-flight txns | Need 2PC for guaranteed durability |

---

## Next Steps

Continue to **[Level 4: Real-Time Updates](04_REALTIME_UPDATES.md)** to learn about CDC, WebSockets, and fan-out patterns.

