# ğŸµ Spotify - Interview Cheatsheet

> Design a music streaming service where users can search for songs, stream audio in real-time, create and share playlists, and receive personalized recommendations â€” all at scale for hundreds of millions of users.

## Quick Reference Card

| Component | Purpose | Key Points |
|-----------|---------|------------|
| **API Gateway** | Route requests, auth, rate limit | TLS, route by path, per-user rate limit |
| **Song Metadata Service** | CRUD for songs, artists, albums | Relational DB; read-heavy, heavily cached |
| **Streaming Service** | Serve audio chunks to clients | Generates pre-signed CDN URLs; does NOT proxy audio bytes |
| **Search Service** | Full-text search across catalog | Elasticsearch; fuzzy matching, autocomplete, popularity-weighted ranking |
| **Playlist Service** | Playlist CRUD, collaborative editing | Many-to-many (playlist â†” songs); optimistic locking for concurrent edits |
| **Recommendation Service** | Personalized playlists (Discover Weekly, Daily Mix) | Offline pipeline (Spark) â†’ precomputed; served from cache |
| **Play History Service** | Record and query play events | Write-heavy â†’ Kafka â†’ Cassandra; feeds recommendations and "recently played" |
| **Object Storage (S3)** | Store audio files | Pre-encoded at multiple bitrates (128/256/320 kbps); immutable blobs |
| **CDN** | Edge-cache audio files close to users | Top 1% songs cached at edge; long-tail served from origin |
| **Metadata DB (MySQL/PG)** | Songs, artists, albums, playlists | Shard by song_id or playlist_id |
| **Cache (Redis)** | Song metadata, playlist data, search suggestions | Short TTL; invalidate on writes |
| **Kafka** | Event bus for play events, analytics | Play events â†’ recommendations, royalty calculation, analytics |

---

## The Story: Building Spotify

Users want to search for any song, tap play, and hear audio within seconds â€” whether they're on WiFi or a spotty mobile connection. Behind the scenes, 100M+ songs sit in object storage, each encoded at multiple bitrates. A CDN distributes popular tracks to edge servers worldwide. When a user taps play, the client doesn't download the whole file â€” it fetches audio in small chunks, buffering ahead to handle network jitter. Meanwhile, every play event streams through Kafka to feed recommendation pipelines, royalty calculations, and analytics. The design focuses on **how audio gets from storage to the user's ears** (CDN + chunking + adaptive bitrate), **search at scale** (Elasticsearch), **playlist management** (many-to-many data modeling, collaborative editing), and **personalization** (offline ML pipelines). Staff-level depth means we cover the streaming protocol, CDN caching strategy, data model relationships, consistency trade-offs, and failure scenarios.

---

## 1. What Are We Building? (Requirements)

### Functional Requirements

- **Search**: Find songs, artists, albums, and playlists by name. Support fuzzy matching ("beatlse" â†’ "Beatles") and autocomplete.
- **Stream music**: User taps play â†’ audio starts within 200ms. Adaptive bitrate based on network. Seamless playback (no buffering pauses).
- **Playlists**: Create, edit, delete playlists. Add/remove/reorder songs. Collaborative playlists (multiple users can edit).
- **Recommendations**: Personalized playlists (Discover Weekly, Daily Mix). Based on listening history and similar users.
- **Play history**: "Recently Played" list. Tracks every play for analytics and recommendations.
- **Artist/album catalog**: Browse artist discographies, album track lists.

### Non-Functional Requirements

- **Scale**: 500M total users, 200M DAU, 100M songs in catalog.
- **Latency**: Audio playback start < 200ms. Search results < 100ms. Metadata reads < 50ms.
- **Availability**: Streaming must be highly available (99.99%). Brief search lag is tolerable.
- **Bandwidth efficiency**: Adaptive bitrate to avoid wasting bandwidth on poor connections.
- **Durability**: Audio files must never be lost. Play history must be durable for royalty calculations.

### Scope (What We're Not Covering)

- Offline downloads / DRM â€” mention briefly; client-side encryption + license server.
- Ads (free tier) â€” separate ad-serving system.
- Social features (following, sharing) â€” standard social graph + activity feed.
- Podcast support â€” similar to music but different metadata schema.
- Payment / subscription management â€” standard billing system.

---

## 2. Back-of-the-Envelope Estimation

### Storage

```
Songs:     100M songs Ã— 3 bitrates Ã— 5 MB avg = 1.5 PB (object storage)
Metadata:  100M songs Ã— 1 KB = 100 GB (relational DB)
           10M artists Ã— 2 KB = 20 GB
           20M albums Ã— 1 KB = 20 GB
Playlists: 4B playlists Ã— 0.5 KB avg = 2 TB
           (playlist_songs rows: 4B Ã— 50 avg songs Ã— 50 bytes = 10 TB)
Play history: 4B events/day Ã— 200 bytes Ã— 365 days = ~290 TB/year (Cassandra)
```

### Bandwidth (Streaming)

```
200M DAU Ã— 30 min/day average listening = 6B minutes/day
At 128 kbps (16 KB/s): 6B Ã— 60s Ã— 16 KB = ~5.6 PB/day outbound
Average bandwidth: ~520 Gbps
Peak (2Ã— average): ~1 Tbps
â†’ This is why CDN is essential. Origin servers alone cannot serve this.
```

### QPS

```
Play requests: 200M DAU Ã— 20 songs/day = 4B/day â‰ˆ 46K req/s
Search:        200M DAU Ã— 5 searches/day = 1B/day â‰ˆ 12K req/s
Playlist ops:  200M DAU Ã— 2 ops/day = 400M/day â‰ˆ 5K req/s
Play events:   Same as play requests â‰ˆ 46K writes/s to Kafka
Metadata reads: 200M DAU Ã— 30 reads/day = 6B/day â‰ˆ 70K req/s (mostly cached)
```

### Key Takeaways

| Dimension | Value | Implication |
|-----------|-------|-------------|
| Audio storage | 1.5 PB | Object storage (S3), not DB |
| Outbound bandwidth | ~1 Tbps peak | CDN is mandatory, not optional |
| Play events | 46K writes/s | Write-heavy â†’ Kafka â†’ Cassandra |
| Metadata reads | 70K/s (mostly cached) | Redis cache in front of DB |
| Catalog size | 100M songs | Search index must handle 100M docs |

---

## 3. Core Concept: How Music Streaming Works

### Why Not Download the Whole File?

A 4-minute song at 320 kbps = ~10 MB. Downloading fully before playing means a multi-second wait. Instead, we **stream**: fetch the audio in small chunks, start playing as soon as the first chunk arrives, and keep fetching ahead (buffering) while the user listens.

### The Streaming Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    HOW AUDIO STREAMING WORKS                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  1. User taps "Play" on song X                                      â”‚
â”‚     â”‚                                                                â”‚
â”‚     â–¼                                                                â”‚
â”‚  2. Client â†’ Streaming Service: "I want to play song_id=X"          â”‚
â”‚     â”‚                                                                â”‚
â”‚     â–¼                                                                â”‚
â”‚  3. Streaming Service:                                               â”‚
â”‚     â”œâ”€â”€ Look up song metadata (bitrate files, S3 paths)             â”‚
â”‚     â”œâ”€â”€ Choose bitrate based on client's network quality             â”‚
â”‚     â”œâ”€â”€ Generate pre-signed CDN URL (time-limited, auth token)      â”‚
â”‚     â””â”€â”€ Return URL to client                                         â”‚
â”‚     â”‚                                                                â”‚
â”‚     â–¼                                                                â”‚
â”‚  4. Client fetches audio DIRECTLY from CDN (not through our servers)â”‚
â”‚     â”œâ”€â”€ HTTP Range request: bytes 0-65535 (first 64KB chunk)        â”‚
â”‚     â”œâ”€â”€ Start playing as soon as first chunk decoded                 â”‚
â”‚     â”œâ”€â”€ Prefetch next chunks in background                          â”‚
â”‚     â””â”€â”€ Buffer 10-30 seconds ahead                                   â”‚
â”‚     â”‚                                                                â”‚
â”‚     â–¼                                                                â”‚
â”‚  5. CDN serves the chunk:                                            â”‚
â”‚     â”œâ”€â”€ Cache HIT â†’ serve from edge (< 20ms)                       â”‚
â”‚     â””â”€â”€ Cache MISS â†’ fetch from S3 origin, cache, then serve       â”‚
â”‚     â”‚                                                                â”‚
â”‚     â–¼                                                                â”‚
â”‚  6. Client reports play event â†’ Kafka â†’ Play History                â”‚
â”‚                                                                      â”‚
â”‚  KEY INSIGHT: Our servers never touch audio bytes.                   â”‚
â”‚  Streaming Service only issues a signed URL. CDN does the heavy     â”‚
â”‚  lifting. This keeps our server costs low and latency minimal.      â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Adaptive Bitrate

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ADAPTIVE BITRATE SELECTION             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                     â”‚
â”‚  Network Speed        Bitrate      File Size (4min)â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€        â”€â”€â”€â”€â”€â”€â”€      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚  < 200 kbps           128 kbps     ~4 MB           â”‚
â”‚  200 kbps - 1 Mbps    256 kbps     ~8 MB           â”‚
â”‚  > 1 Mbps             320 kbps     ~10 MB          â”‚
â”‚                                                     â”‚
â”‚  Decision made by CLIENT, not server:              â”‚
â”‚  â€¢ Client monitors download speed of recent chunks â”‚
â”‚  â€¢ If speed drops â†’ switch to lower bitrate mid-   â”‚
â”‚    song (next chunk fetched at lower quality)      â”‚
â”‚  â€¢ If speed improves â†’ switch up                   â”‚
â”‚                                                     â”‚
â”‚  Each song is pre-encoded at all 3 bitrates and    â”‚
â”‚  stored as 3 separate files in S3.                 â”‚
â”‚                                                     â”‚
â”‚  S3 paths:                                          â”‚
â”‚    /songs/{song_id}/128.mp3                        â”‚
â”‚    /songs/{song_id}/256.mp3                        â”‚
â”‚    /songs/{song_id}/320.mp3                        â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Pre-signed CDN URLs

The Streaming Service does NOT proxy audio. It generates a **pre-signed URL** â€” a CDN URL with an embedded authentication token and expiry. The client uses this URL to fetch audio directly from the CDN. This keeps our servers out of the data path.

```
Pre-signed URL example:
https://cdn.spotify.internal/songs/abc123/256.mp3
  ?token=eyJhbGciOiJIUz...
  &expires=1708012800

If the URL expires or the token is invalid, CDN returns 403.
Client requests a new URL from Streaming Service.
```

Why pre-signed and not just public?
- **Access control**: Only paying users (or free-tier with ads) should stream.
- **Expiry**: URLs expire after ~1 hour. Prevents URL sharing.
- **Analytics**: Token encodes user_id for server-side logging at CDN level.

---

## 4. API Design

### Play Song

```
POST /api/v1/songs/{song_id}/play
Headers: Authorization: Bearer <token>
         X-Network-Quality: high | medium | low

Response 200:
{
  "stream_url": "https://cdn.spotify.internal/songs/abc123/256.mp3?token=...",
  "bitrate": 256,
  "duration_ms": 240000,
  "expires_at": "2025-02-15T12:30:00Z",
  "song": {
    "song_id": "abc123",
    "title": "Bohemian Rhapsody",
    "artist": "Queen",
    "album": "A Night at the Opera",
    "album_art_url": "https://cdn.spotify.internal/art/album_456.jpg"
  }
}
```

- Client sends network quality hint; server selects bitrate accordingly.
- Response includes metadata so the UI can display song info immediately.
- `stream_url` points to CDN, NOT to our servers.

### Search

```
GET /api/v1/search?q=bohemian+rhaps&type=song,artist,album&limit=10
Headers: Authorization: Bearer <token>

Response 200:
{
  "songs": [
    {"song_id": "abc123", "title": "Bohemian Rhapsody", "artist": "Queen", "album": "A Night at the Opera"}
  ],
  "artists": [
    {"artist_id": "art_001", "name": "Queen", "image_url": "..."}
  ],
  "albums": [
    {"album_id": "alb_001", "title": "A Night at the Opera", "artist": "Queen", "year": 1975}
  ]
}
```

- `type` parameter lets client request specific entity types.
- Partial/fuzzy matching: "bohemian rhaps" matches "Bohemian Rhapsody".
- Results ranked by relevance (text match score) Ã— popularity (play count).

### Playlist CRUD

```
POST /api/v1/playlists
{
  "name": "Road Trip Mix",
  "is_collaborative": false
}
â†’ 201: {"playlist_id": "pl_001", ...}

POST /api/v1/playlists/{playlist_id}/songs
Headers: X-Idempotency-Key: <uuid>
{
  "song_id": "abc123",
  "position": 0
}
â†’ 200: {"playlist_id": "pl_001", "total_songs": 15}

DELETE /api/v1/playlists/{playlist_id}/songs/{song_id}
â†’ 204

PUT /api/v1/playlists/{playlist_id}/songs/reorder
{
  "song_id": "abc123",
  "new_position": 5
}
â†’ 200
```

- Adding a song uses an idempotency key to prevent duplicate additions on retry.
- Reorder updates the `position` column in the `playlist_songs` join table.
- Collaborative playlists: any member can add/remove/reorder. Optimistic locking via `version` column prevents lost updates.

### Get Recommendations

```
GET /api/v1/recommendations/daily-mix?limit=30
Headers: Authorization: Bearer <token>

Response 200:
{
  "playlist_name": "Daily Mix 1",
  "songs": [
    {"song_id": "abc123", "title": "Bohemian Rhapsody", "artist": "Queen", ...},
    ...
  ],
  "generated_at": "2025-02-15T00:00:00Z"
}
```

- Recommendations are precomputed offline and cached. This endpoint reads from cache/DB, no real-time ML.

---

## 5. High-Level System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         SPOTIFY ARCHITECTURE                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                                â”‚
â”‚  â”‚  Client   â”‚ (Mobile / Desktop / Web)                                     â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                                                                â”‚
â”‚       â”‚                                                                      â”‚
â”‚       â”‚ HTTPS                                                                â”‚
â”‚       â–¼                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                            â”‚
â”‚  â”‚ API Gateway   â”‚ (Auth, Rate Limit, Routing)                              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                                            â”‚
â”‚         â”‚                                                                    â”‚
â”‚    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚    â–¼             â–¼              â–¼                â–¼               â–¼           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚Streaming â”‚ â”‚ Search   â”‚ â”‚ Playlist  â”‚ â”‚ Recommend â”‚ â”‚Play History â”‚     â”‚
â”‚  â”‚Service   â”‚ â”‚ Service  â”‚ â”‚ Service   â”‚ â”‚ Service   â”‚ â”‚Service      â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚       â”‚             â”‚             â”‚             â”‚              â”‚             â”‚
â”‚       â”‚             â”‚             â”‚             â”‚              â”‚             â”‚
â”‚  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  S3      â”‚ â”‚Elastic-   â”‚ â”‚Metadataâ”‚  â”‚Redis     â”‚  â”‚  Kafka    â”‚      â”‚
â”‚  â”‚ (audio)  â”‚ â”‚search     â”‚ â”‚DB (PG) â”‚  â”‚Cache     â”‚  â”‚           â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â”‚
â”‚       â”‚                                                       â”‚             â”‚
â”‚       â–¼                                                       â–¼             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚   CDN    â”‚ â—„â”€â”€ Client fetches audio directly        â”‚  Cassandra   â”‚     â”‚
â”‚  â”‚ (edge)   â”‚                                          â”‚ (play history)â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Offline Pipeline (Spark / Hadoop)                                    â”‚    â”‚
â”‚  â”‚  Reads play history from Cassandra â†’ Collaborative filtering          â”‚    â”‚
â”‚  â”‚  â†’ Writes precomputed recommendations to Redis / Metadata DB         â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Responsibilities

| Component | Responsibility | Why this technology? |
|-----------|---------------|---------------------|
| **Streaming Service** | Look up song files, generate pre-signed CDN URL | Stateless; never touches audio bytes |
| **Search Service** | Full-text search with fuzzy matching and autocomplete | Elasticsearch: inverted index, BM25 ranking, built-in fuzzy |
| **Playlist Service** | CRUD, collaborative editing, song ordering | Relational DB for strong consistency on playlist state |
| **Recommendation Service** | Serve precomputed personalized playlists | Read from cache; offline pipeline does the heavy ML |
| **Play History Service** | Record play events, serve "recently played" | Kafka for ingestion; Cassandra for write-heavy time-series storage |
| **S3** | Durable audio file storage | Immutable blobs; 11 nines durability; cheap at PB scale |
| **CDN** | Edge-cache audio for low-latency delivery | Serves ~99% of audio traffic; reduces origin load by 100Ã— |
| **Elasticsearch** | Search index for songs, artists, albums | 100M docs; sub-100ms fuzzy search |
| **Metadata DB (PostgreSQL)** | Songs, artists, albums, playlists | ACID for playlist edits; relational joins for catalog queries |
| **Redis** | Cache metadata, recommendations, search suggestions | Sub-ms reads; reduces DB load |
| **Kafka** | Event bus for play events | Decouples ingestion from processing; durable, replayable |
| **Cassandra** | Play history storage | Write-optimized (LSM-tree); partitioned by user_id for time-range queries |
| **Spark (offline)** | Recommendation pipeline | Batch processing on full play history; outputs precomputed playlists |

---

## 6. Sync vs. Async Communication

| Hop | Protocol | Sync/Async | Why |
|-----|----------|------------|-----|
| Client â†’ Streaming Service (get stream URL) | HTTP POST | **Sync** | User is waiting to hear audio |
| Client â†’ CDN (fetch audio chunks) | HTTP GET (Range) | **Sync** | Audio must arrive to play |
| Client â†’ Search Service | HTTP GET | **Sync** | User is waiting for results |
| Client â†’ Playlist Service (add song) | HTTP POST | **Sync** | User needs confirmation |
| Play History Service â†’ Kafka | Kafka PRODUCE | **Async (fire-and-forget)** | Play event is a side effect; shouldn't block playback |
| Kafka â†’ Cassandra (play history write) | Kafka CONSUME | **Async** | Eventual storage; minor lag acceptable |
| Kafka â†’ Recommendation Pipeline | Kafka CONSUME | **Async** | Offline batch; hours of lag acceptable |
| Playlist Service â†’ Redis (invalidate cache) | Redis DEL | **Sync (fast, ~0.5ms)** | Ensure stale playlist not served |
| Metadata Service â†’ Elasticsearch (index update) | Async (outbox) | **Async** | Search index is eventually consistent with DB |

**Rule of thumb**: Anything the user is waiting on = sync. Side effects (analytics, recommendations, notifications) = async.

---

## 7. Play Song Flow (End-to-End)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PLAY SONG â€” STEP BY STEP                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  â‘  Client taps "Play" on "Bohemian Rhapsody"                       â”‚
â”‚     â”‚                                                                â”‚
â”‚     â–¼                                                                â”‚
â”‚  â‘¡ POST /api/v1/songs/abc123/play                                   â”‚
â”‚     Header: X-Network-Quality: high                                  â”‚
â”‚     â”‚                                                                â”‚
â”‚     â–¼                                                                â”‚
â”‚  â‘¢ Streaming Service:                                                â”‚
â”‚     â”œâ”€â”€ Check Redis cache for song metadata                         â”‚
â”‚     â”‚   Cache HIT â†’ song metadata (S3 paths, duration, etc.)       â”‚
â”‚     â”‚   Cache MISS â†’ query Metadata DB â†’ populate cache             â”‚
â”‚     â”œâ”€â”€ Select bitrate: high network â†’ 320 kbps                     â”‚
â”‚     â”œâ”€â”€ Build S3 path: /songs/abc123/320.mp3                        â”‚
â”‚     â”œâ”€â”€ Generate pre-signed CDN URL (1-hour expiry)                 â”‚
â”‚     â””â”€â”€ Return stream_url + song metadata to client                 â”‚
â”‚     â”‚                                                                â”‚
â”‚     â–¼                                                                â”‚
â”‚  â‘£ Client receives stream_url, starts fetching from CDN             â”‚
â”‚     GET https://cdn.../songs/abc123/320.mp3                         â”‚
â”‚     Range: bytes=0-65535  (first 64KB chunk)                        â”‚
â”‚     â”‚                                                                â”‚
â”‚     â–¼                                                                â”‚
â”‚  â‘¤ CDN edge server:                                                  â”‚
â”‚     â”œâ”€â”€ Cache HIT (popular song) â†’ serve immediately (~10ms)       â”‚
â”‚     â””â”€â”€ Cache MISS â†’ fetch from S3 origin â†’ cache â†’ serve (~100ms) â”‚
â”‚     â”‚                                                                â”‚
â”‚     â–¼                                                                â”‚
â”‚  â‘¥ Client decodes first chunk â†’ audio starts playing               â”‚
â”‚     â”œâ”€â”€ Prefetch next chunks in background (buffer 10-30s ahead)    â”‚
â”‚     â””â”€â”€ Monitor download speed â†’ adjust bitrate if needed           â”‚
â”‚     â”‚                                                                â”‚
â”‚     â–¼                                                                â”‚
â”‚  â‘¦ After ~30 seconds of playback, client fires play event          â”‚
â”‚     POST /api/v1/play-events                                        â”‚
â”‚     { "song_id": "abc123", "duration_ms": 30000, "bitrate": 320 }  â”‚
â”‚     â”‚                                                                â”‚
â”‚     â–¼                                                                â”‚
â”‚  â‘§ Play History Service â†’ Kafka "play-events" topic                â”‚
â”‚     â”‚                                                                â”‚
â”‚     â”œâ”€â”€â–º Cassandra (play history for "recently played")             â”‚
â”‚     â”œâ”€â”€â–º Recommendation pipeline (offline batch)                    â”‚
â”‚     â””â”€â”€â–º Royalty calculation service                                â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why Fire the Play Event After 30 Seconds?

A "play" counts for royalties and recommendations only if the user listened for â‰¥ 30 seconds. If they skip after 5 seconds, no play event is recorded. This prevents users from gaming the system and inflating play counts.

---

## 8. Search Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SEARCH â€” STEP BY STEP                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  â‘  User types "bohemian rhaps" in search bar                        â”‚
â”‚     â”‚                                                                â”‚
â”‚     â–¼                                                                â”‚
â”‚  â‘¡ Client sends: GET /api/v1/search?q=bohemian+rhaps&type=song     â”‚
â”‚     â”‚                                                                â”‚
â”‚     â–¼                                                                â”‚
â”‚  â‘¢ Search Service:                                                   â”‚
â”‚     â”œâ”€â”€ Check Redis for cached results (key: "search:bohemian rhaps")â”‚
â”‚     â”‚   Cache HIT (popular query) â†’ return immediately              â”‚
â”‚     â”‚   Cache MISS â†’ continue to Elasticsearch                      â”‚
â”‚     â”‚                                                                â”‚
â”‚     â–¼                                                                â”‚
â”‚  â‘£ Elasticsearch query:                                              â”‚
â”‚     {                                                                â”‚
â”‚       "multi_match": {                                               â”‚
â”‚         "query": "bohemian rhaps",                                   â”‚
â”‚         "fields": ["title^3", "artist^2", "album"],                 â”‚
â”‚         "type": "best_fields",                                       â”‚
â”‚         "fuzziness": "AUTO"                                          â”‚
â”‚       }                                                              â”‚
â”‚     }                                                                â”‚
â”‚     â”‚                                                                â”‚
â”‚     â”‚  Field weights: title (3Ã—) > artist (2Ã—) > album (1Ã—)        â”‚
â”‚     â”‚  Fuzziness: AUTO â†’ allows 1-2 character edits                 â”‚
â”‚     â”‚                                                                â”‚
â”‚     â–¼                                                                â”‚
â”‚  â‘¤ Elasticsearch returns candidates ranked by text relevance        â”‚
â”‚     â”‚                                                                â”‚
â”‚     â–¼                                                                â”‚
â”‚  â‘¥ Search Service re-ranks by:                                      â”‚
â”‚     final_score = 0.7 Ã— text_relevance + 0.3 Ã— popularity_score    â”‚
â”‚     (popularity = log(total_play_count))                             â”‚
â”‚     â”‚                                                                â”‚
â”‚     â–¼                                                                â”‚
â”‚  â‘¦ Cache results in Redis (TTL: 5 min) â†’ return to client          â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Autocomplete

For keystroke-by-keystroke suggestions (as the user types), we use a **prefix index** in Redis or Elasticsearch's `completion` suggester. The client debounces (waits 100-200ms after last keystroke) before sending the request.

### Keeping the Search Index in Sync

When a new song is added or metadata changes, we need to update Elasticsearch. Two approaches:

| Approach | How | Trade-off |
|----------|-----|-----------|
| **Transactional outbox** | Song write â†’ DB + outbox row â†’ CDC/poller â†’ Elasticsearch | Guaranteed eventual consistency; slight lag |
| **Direct update** | Song write â†’ DB, then Elasticsearch update | Simpler but if ES update fails, index drifts |

We use the **outbox approach** â€” same pattern as Splitwise/Uber/BookMyShow. Search results may lag a few seconds behind catalog updates, which is acceptable.

---

## 9. Data Model

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         DATA MODEL                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€ Metadata DB (PostgreSQL) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                                                                â”‚   â”‚
â”‚  â”‚  artists                                                       â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚   â”‚
â”‚  â”‚  â”‚ artist_id â”‚ name  â”‚ image_url â”‚ bio         â”‚              â”‚   â”‚
â”‚  â”‚  â”‚ (PK)      â”‚       â”‚           â”‚             â”‚              â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚   â”‚
â”‚  â”‚                                                                â”‚   â”‚
â”‚  â”‚  albums                                                        â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚   â”‚
â”‚  â”‚  â”‚ album_id â”‚ artist_id â”‚ title â”‚ release_year â”‚              â”‚   â”‚
â”‚  â”‚  â”‚ (PK)     â”‚ (FK)      â”‚       â”‚              â”‚              â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚   â”‚
â”‚  â”‚  artist_id is FK â†’ one-to-many: one artist has many albums.   â”‚   â”‚
â”‚  â”‚                                                                â”‚   â”‚
â”‚  â”‚  songs                                                         â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚  â”‚ song_id â”‚ album_id â”‚ title     â”‚ duration â”‚ genre      â”‚   â”‚   â”‚
â”‚  â”‚  â”‚ (PK)    â”‚ (FK)     â”‚           â”‚ (ms)     â”‚            â”‚   â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚   â”‚
â”‚  â”‚  â”‚ s_001   â”‚ alb_001  â”‚ Bohemian  â”‚ 354000   â”‚ ROCK       â”‚   â”‚   â”‚
â”‚  â”‚  â”‚         â”‚          â”‚ Rhapsody  â”‚          â”‚            â”‚   â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â”‚  album_id is FK â†’ one-to-many: one album has many songs.      â”‚   â”‚
â”‚  â”‚                                                                â”‚   â”‚
â”‚  â”‚  song_files (one song â†’ multiple bitrate files)                â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚   â”‚
â”‚  â”‚  â”‚ song_id â”‚ bitrate â”‚ s3_path                      â”‚         â”‚   â”‚
â”‚  â”‚  â”‚ (FK)    â”‚ (kbps)  â”‚                              â”‚         â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”‚   â”‚
â”‚  â”‚  â”‚ s_001   â”‚ 128     â”‚ /songs/s_001/128.mp3         â”‚         â”‚   â”‚
â”‚  â”‚  â”‚ s_001   â”‚ 256     â”‚ /songs/s_001/256.mp3         â”‚         â”‚   â”‚
â”‚  â”‚  â”‚ s_001   â”‚ 320     â”‚ /songs/s_001/320.mp3         â”‚         â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚   â”‚
â”‚  â”‚  Composite PK: (song_id, bitrate). One-to-many from songs.    â”‚   â”‚
â”‚  â”‚                                                                â”‚   â”‚
â”‚  â”‚  playlists                                                     â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚
â”‚  â”‚  â”‚ playlist_id â”‚ owner_id â”‚ name â”‚ is_collaborativeâ”‚ version â”‚ â”‚   â”‚
â”‚  â”‚  â”‚ (PK)        â”‚ (FK)     â”‚      â”‚                â”‚         â”‚ â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚
â”‚  â”‚  version column for optimistic locking on collaborative edits.â”‚   â”‚
â”‚  â”‚                                                                â”‚   â”‚
â”‚  â”‚  playlist_songs (many-to-many join table: playlists â†” songs)  â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚   â”‚
â”‚  â”‚  â”‚ playlist_id â”‚ song_id â”‚ position â”‚  (composite PK)         â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                         â”‚   â”‚
â”‚  â”‚  â”‚ pl_001      â”‚ s_001   â”‚ 0        â”‚                         â”‚   â”‚
â”‚  â”‚  â”‚ pl_001      â”‚ s_002   â”‚ 1        â”‚                         â”‚   â”‚
â”‚  â”‚  â”‚ pl_001      â”‚ s_003   â”‚ 2        â”‚                         â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚   â”‚
â”‚  â”‚  Many-to-many with payload (position for ordering).           â”‚   â”‚
â”‚  â”‚  Same pattern as Splitwise's expense_splits â€” join table      â”‚   â”‚
â”‚  â”‚  with extra data. See: database_fundamentals/02_DATABASE_LOGICâ”‚   â”‚
â”‚  â”‚                                                                â”‚   â”‚
â”‚  â”‚  users                                                         â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚   â”‚
â”‚  â”‚  â”‚ user_id â”‚ name  â”‚ email  â”‚ subscription â”‚                  â”‚   â”‚
â”‚  â”‚  â”‚ (PK)    â”‚       â”‚        â”‚ FREE/PREMIUM â”‚                  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚   â”‚
â”‚  â”‚                                                                â”‚   â”‚
â”‚  â”‚  idempotency_keys                                              â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚   â”‚
â”‚  â”‚  â”‚ idempotency_key  â”‚ response_payload â”‚ created_at  â”‚        â”‚   â”‚
â”‚  â”‚  â”‚ (PK)             â”‚                  â”‚             â”‚        â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚   â”‚
â”‚  â”‚                                                                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€ Play History (Cassandra) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                                                                â”‚   â”‚
â”‚  â”‚  play_events                                                   â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚   â”‚
â”‚  â”‚  â”‚ user_id â”‚ played_at         â”‚ song_id â”‚ duration â”‚bitrate â”‚â”‚   â”‚
â”‚  â”‚  â”‚(part.key)â”‚ (clustering, DESC)â”‚         â”‚ (ms)     â”‚        â”‚â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚   â”‚
â”‚  â”‚  â”‚ u_001   â”‚ 2025-02-15 12:00  â”‚ s_001   â”‚ 240000   â”‚ 320    â”‚â”‚   â”‚
â”‚  â”‚  â”‚ u_001   â”‚ 2025-02-15 11:45  â”‚ s_002   â”‚ 195000   â”‚ 256    â”‚â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚   â”‚
â”‚  â”‚  Partition key: user_id â†’ all plays for one user on same node.â”‚   â”‚
â”‚  â”‚  Clustering key: played_at DESC â†’ recent plays first.         â”‚   â”‚
â”‚  â”‚  Write-heavy (46K/s) â†’ Cassandra's LSM-tree handles this.    â”‚   â”‚
â”‚  â”‚                                                                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€ Search Index (Elasticsearch) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                                                                â”‚   â”‚
â”‚  â”‚  songs_index                                                   â”‚   â”‚
â”‚  â”‚  {                                                             â”‚   â”‚
â”‚  â”‚    "song_id": "s_001",                                        â”‚   â”‚
â”‚  â”‚    "title": "Bohemian Rhapsody",                              â”‚   â”‚
â”‚  â”‚    "artist_name": "Queen",                                    â”‚   â”‚
â”‚  â”‚    "album_title": "A Night at the Opera",                     â”‚   â”‚
â”‚  â”‚    "genre": "rock",                                           â”‚   â”‚
â”‚  â”‚    "play_count": 2500000000,                                  â”‚   â”‚
â”‚  â”‚    "release_year": 1975                                       â”‚   â”‚
â”‚  â”‚  }                                                             â”‚   â”‚
â”‚  â”‚  Synced from Metadata DB via transactional outbox + CDC.      â”‚   â”‚
â”‚  â”‚                                                                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Relationship Summary

| Relationship | Type | How Modeled |
|-------------|------|-------------|
| artist â†’ albums | One-to-many | `albums.artist_id` FK |
| album â†’ songs | One-to-many | `songs.album_id` FK |
| song â†’ song_files | One-to-many | `song_files.song_id` FK |
| playlists â†” songs | Many-to-many (with position) | `playlist_songs` join table |
| user â†’ playlists | One-to-many | `playlists.owner_id` FK |
| user â†’ play_events | One-to-many (time-series) | Cassandra partition key = `user_id` |

### Redis Cache Data Model

```
Metadata cache:
  Key:   song:{song_id}
  Value: JSON (title, artist, album, duration, s3_paths)
  TTL:   1 hour

Playlist cache:
  Key:   playlist:{playlist_id}
  Value: JSON (name, owner, songs with positions)
  TTL:   5 minutes (shorter because playlists change more often)

Search cache:
  Key:   search:{normalized_query}
  Value: JSON array of results
  TTL:   5 minutes

Recommendations cache:
  Key:   reco:{user_id}:daily-mix
  Value: JSON array of song_ids
  TTL:   24 hours (regenerated daily by offline pipeline)
```

---

## 10. Concurrency

### Collaborative Playlist Edits

Two users edit the same collaborative playlist at the same time. User A adds a song; User B reorders songs.

```
Problem:
  User A reads playlist (version=5), adds song at position 3
  User B reads playlist (version=5), moves song from position 1 to position 5
  Both submit with version=5 â†’ one will overwrite the other's change

Solution: Optimistic Locking
  UPDATE playlists SET version = version + 1, ...
  WHERE playlist_id = ? AND version = ?

  If version has changed (another user edited first), the UPDATE matches 0 rows.
  Service detects this â†’ returns 409 Conflict â†’ client re-fetches and retries.
```

Why optimistic (not pessimistic)?
- Playlist edits are **infrequent and low-contention** â€” two users rarely edit the same playlist in the same second.
- Pessimistic locking (SELECT FOR UPDATE) would hold a DB lock, blocking other reads. Overkill for this use case.
- Contrast with Splitwise's balance updates â€” those ARE high-contention (same balance rows), so pessimistic locking makes sense there.

### Play Count Aggregation

Play counts are updated 46K times/second globally. We do NOT update a `play_count` column in the songs table in real-time â€” that would create a massive write hotspot on popular songs.

Instead:
1. Play events flow through Kafka â†’ Cassandra (raw events).
2. A periodic batch job (every few minutes) aggregates counts and updates `play_count` in the Metadata DB.
3. The Elasticsearch index picks up updated counts via CDC.

This means play counts are **eventually consistent** (lagging a few minutes), which is perfectly acceptable.

---

## 11. Consistency and Reliability

| Data | Consistency Level | Why |
|------|------------------|-----|
| Song metadata (title, artist, album) | **Strong** (DB write) | Source of truth for catalog. Incorrect metadata = wrong song info shown. |
| Playlist state (songs, ordering) | **Strong** (DB write + optimistic lock) | Users expect their edits to persist immediately. |
| Search results | **Eventual** (outbox â†’ Elasticsearch) | Brief lag between catalog update and search index. Acceptable. |
| Play history | **Eventual** (Kafka â†’ Cassandra) | Minor lag for "recently played." Acceptable. |
| Recommendations | **Eventual** (batch pipeline, regenerated daily) | Users don't expect real-time recommendation updates. |
| Play count | **Eventual** (batch aggregation) | Displaying "2.5B plays" vs "2.500001B plays" doesn't matter. |
| Cache (Redis) | **Eventual** (invalidated on write, TTL safety net) | Brief stale window. Acceptable. |
| Audio files (S3) | **Strong** (immutable, 11 nines durability) | Once uploaded, audio files never change. No consistency concern. |

**Cache invalidation**: Done by application code (`redis.delete(key)`) right after DB transaction commits. Same approach as Splitwise â€” all writes go through the app, and a short TTL acts as safety net.

### Kafka Topics

| Topic | Partition Key | Value | Producer | Consumer |
|-------|-------------|-------|----------|----------|
| `play-events` | `user_id` | `{user_id, song_id, duration_ms, bitrate, timestamp}` | Play History Service | Cassandra writer, Recommendation pipeline, Royalty service |
| `catalog-events` | `song_id` | `{event_type, song_id, metadata}` | Outbox publisher | Elasticsearch indexer |

Partitioned by `user_id` for play events (all plays for one user are ordered), by `song_id` for catalog events.

---

## 12. Failure Scenarios and Handling

| Failure | Risk | Mitigation | User impact |
|---------|------|------------|-------------|
| **CDN edge down** | Audio not served from that edge | CDN automatically routes to next-closest edge. Multiple CDN providers (multi-CDN). | Slight latency increase; seamless if multi-CDN. |
| **S3 origin down** | New audio not fetchable | S3 is multi-AZ. CDN cache covers popular songs. Only cold long-tail songs affected. | Rare songs temporarily unavailable. |
| **Metadata DB down** | Song info not readable | Redis cache serves reads during outage. Writes fail. | Read-only mode; playlist edits fail temporarily. |
| **Elasticsearch down** | Search unavailable | Fallback to DB-based LIKE query (slower, degraded). Cache serves popular queries. | Search slower/degraded, not broken. |
| **Kafka down** | Play events not ingested | Client buffers events locally; retries. Events are non-blocking for playback. | Playback unaffected. "Recently played" may lag. |
| **Pre-signed URL expired** | Client can't fetch audio | Client detects 403 from CDN â†’ requests new URL from Streaming Service. | Brief pause (~200ms) while new URL is fetched. |
| **Collaborative edit conflict** | Lost update on playlist | Optimistic locking â†’ 409 Conflict â†’ client re-fetches and retries. | User sees "playlist was updated, refreshing." |
| **Cassandra node down** | Play history writes fail for some partitions | Cassandra replication (RF=3). Writes go to other replicas. | No user impact if quorum is met. |

---

## 13. Scale and Sharding

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         SCALE STRATEGY BY COMPONENT                   â”‚
â”‚                                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Metadata DB      â”‚   â”‚ Cassandra       â”‚   â”‚ Elasticsearch   â”‚    â”‚
â”‚  â”‚ (PostgreSQL)     â”‚   â”‚ (Play History)  â”‚   â”‚ (Search)        â”‚    â”‚
â”‚  â”‚                  â”‚   â”‚                  â”‚   â”‚                  â”‚    â”‚
â”‚  â”‚ Shard by         â”‚   â”‚ Partition by     â”‚   â”‚ Shard by         â”‚    â”‚
â”‚  â”‚ song_id (catalog)â”‚   â”‚ user_id          â”‚   â”‚ song_id          â”‚    â”‚
â”‚  â”‚ playlist_id      â”‚   â”‚                  â”‚   â”‚                  â”‚    â”‚
â”‚  â”‚ (playlists)      â”‚   â”‚ 46K writes/s     â”‚   â”‚ 100M docs        â”‚    â”‚
â”‚  â”‚                  â”‚   â”‚ spread across     â”‚   â”‚ across 10-20     â”‚    â”‚
â”‚  â”‚ Read replicas    â”‚   â”‚ cluster           â”‚   â”‚ shards           â”‚    â”‚
â”‚  â”‚ for read-heavy   â”‚   â”‚                  â”‚   â”‚                  â”‚    â”‚
â”‚  â”‚ catalog queries  â”‚   â”‚ RF=3 for         â”‚   â”‚ Replicas for     â”‚    â”‚
â”‚  â”‚                  â”‚   â”‚ durability       â”‚   â”‚ read throughput   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Redis Cache      â”‚   â”‚ CDN             â”‚   â”‚ Services         â”‚    â”‚
â”‚  â”‚                  â”‚   â”‚                  â”‚   â”‚                  â”‚    â”‚
â”‚  â”‚ Cluster mode     â”‚   â”‚ 50+ global      â”‚   â”‚ STATELESS        â”‚    â”‚
â”‚  â”‚ Hash slot by key â”‚   â”‚ edge PoPs       â”‚   â”‚ â†’ auto-scale     â”‚    â”‚
â”‚  â”‚                  â”‚   â”‚                  â”‚   â”‚ behind LB        â”‚    â”‚
â”‚  â”‚ Eviction: LRU    â”‚   â”‚ Top 1% songs    â”‚   â”‚                  â”‚    â”‚
â”‚  â”‚                  â”‚   â”‚ cached at edge   â”‚   â”‚ Each instance    â”‚    â”‚
â”‚  â”‚ Hot key:         â”‚   â”‚                  â”‚   â”‚ is identical     â”‚    â”‚
â”‚  â”‚ replicate across â”‚   â”‚ Long-tail from   â”‚   â”‚                  â”‚    â”‚
â”‚  â”‚ multiple slots   â”‚   â”‚ origin           â”‚   â”‚                  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| Component | Shard/Scale Strategy | Why |
|-----------|---------------------|-----|
| **Metadata DB** | Shard by `song_id` (catalog), `playlist_id` (playlists). Read replicas for catalog reads. | Catalog is read-heavy (70K/s); replicas offload reads. Playlists sharded by ID for write distribution. |
| **Cassandra** | Partition by `user_id`. RF=3. | All play history for one user on same partition â†’ efficient "recently played" query. Write-heavy workload spread across cluster. |
| **Elasticsearch** | 10-20 shards for 100M song docs. Replicas for read throughput. | Parallel search across shards. Replicas handle 12K search QPS. |
| **Redis** | Cluster mode, hash by key. Hot keys (popular song metadata) replicated. | Prevents single-node bottleneck. Hot songs don't overload one slot. |
| **CDN** | 50+ global edge PoPs. Top 1% songs (by play count) pre-warmed at edges. | 1M songs cover 80% of plays (power law). Edge caching reduces origin traffic by 100Ã—. |
| **Services** | Stateless â†’ horizontal auto-scale behind LB. | No in-memory state. Scale up/down based on QPS. |

### Hot Song Problem

When a new hit drops (e.g., Taylor Swift album), millions of users play the same songs simultaneously. This creates:
1. **CDN hotspot**: Solved by CDN's multi-tier caching. Popular content replicated across all edges.
2. **Metadata hotspot**: Redis cache absorbs repeated reads for the same song. TTL prevents thundering herd on cache miss (use locking or request coalescing).
3. **Play event flood**: Kafka partitioned by `user_id`, not `song_id`, so writes for one popular song spread across all partitions.

---

## 14. Final Architecture (Putting It All Together)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SPOTIFY â€” COMPLETE ARCHITECTURE                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                                â”‚
â”‚  â”‚  Client   â”‚                                                               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                                                                â”‚
â”‚        â”‚ â‘ â‘¡â‘¢                                                                 â”‚
â”‚        â–¼                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                            â”‚
â”‚  â”‚ API Gateway   â”‚                                                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                                            â”‚
â”‚    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚    â”‚             â”‚              â”‚                â”‚               â”‚           â”‚
â”‚    â–¼             â–¼              â–¼                â–¼               â–¼           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚Streaming â”‚ â”‚ Search   â”‚ â”‚ Playlist  â”‚ â”‚ Recommend â”‚ â”‚Play History â”‚     â”‚
â”‚  â”‚Service   â”‚ â”‚ Service  â”‚ â”‚ Service   â”‚ â”‚ Service   â”‚ â”‚Service      â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚       â”‚             â”‚             â”‚             â”‚              â”‚             â”‚
â”‚       â”‚ â‘£           â”‚ â‘¤           â”‚ â‘¥           â”‚ â‘§            â”‚ â‘¦          â”‚
â”‚       â–¼             â–¼             â–¼             â–¼              â–¼             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  Redis   â”‚ â”‚Elastic-  â”‚ â”‚Metadata  â”‚ â”‚  Redis   â”‚  â”‚  Kafka   â”‚         â”‚
â”‚  â”‚ (meta    â”‚ â”‚search    â”‚ â”‚DB (PG)   â”‚ â”‚ (reco    â”‚  â”‚(play-    â”‚         â”‚
â”‚  â”‚  cache)  â”‚ â”‚          â”‚ â”‚          â”‚ â”‚  cache)  â”‚  â”‚ events)  â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜         â”‚
â”‚       â”‚ miss                                                 â”‚              â”‚
â”‚       â–¼                                                      â”‚              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                â”‚              â”‚
â”‚  â”‚Metadata  â”‚                                          â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚DB (PG)   â”‚                                          â”‚            â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â–¼            â–¼       â”‚
â”‚                                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚Cassandra â”‚ â”‚ Spark   â”‚  â”‚
â”‚  â”‚ Audio Delivery Path (separate)       â”‚        â”‚(history) â”‚ â”‚(offline)â”‚  â”‚
â”‚  â”‚                                      â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â”‚
â”‚  â”‚  Client â”€â”€â‘£â”€â”€â–º CDN â”€â”€missâ”€â”€â–º S3      â”‚                          â”‚       â”‚
â”‚  â”‚                 â”‚                    â”‚                          â”‚       â”‚
â”‚  â”‚           edge cache                 â”‚               â‘¨ write reco      â”‚
â”‚  â”‚           (popular songs)            â”‚                          â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â–¼       â”‚
â”‚                                                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚                                                              â”‚  Redis   â”‚  â”‚
â”‚                                                              â”‚ (reco)   â”‚  â”‚
â”‚                                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Numbered Flow Summary

| Step | What Happens | From â†’ To | Protocol | Infra |
|------|-------------|-----------|----------|-------|
| â‘  | User taps Play / searches / opens playlist | Client â†’ API Gateway | HTTPS | |
| â‘¡ | Gateway authenticates, rate limits, routes | API Gateway â†’ Service | HTTP | |
| â‘¢ | Service processes request | Service | Internal | |
| â‘£ | Streaming: check cache for song metadata; generate pre-signed CDN URL. Client fetches audio from CDN. | Streaming Service â†’ Redis/DB; Client â†’ CDN â†’ S3 | HTTP | Redis, CDN, S3 |
| â‘¤ | Search: query Elasticsearch (fuzzy + popularity re-rank) | Search Service â†’ Elasticsearch | HTTP | ES cluster |
| â‘¥ | Playlist: read/write playlist state in DB; invalidate cache | Playlist Service â†’ Metadata DB + Redis | SQL + Redis | PG, Redis |
| â‘¦ | Play event recorded (after 30s of playback) | Play History Service â†’ Kafka | Kafka PRODUCE | Kafka |
| â‘§ | Recommendations served from cache (precomputed) | Recommendation Service â†’ Redis | Redis GET | Redis |
| â‘¨ | Offline pipeline reads history, computes recommendations, writes to cache | Spark â†’ Cassandra â†’ Redis | Batch | Spark |

---

## 15. Trade-off Summary

| Decision | Chosen | Alternative | Why |
|----------|--------|------------|-----|
| **Audio delivery** | Pre-signed CDN URL (server never touches audio) | Server-proxied streaming | CDN handles bandwidth at scale; proxying would require 1Tbps server capacity |
| **Bitrate selection** | Client-side adaptive | Server decides | Client knows its own network better; can switch mid-song without round-trip |
| **Search engine** | Elasticsearch | DB LIKE queries / Solr | ES built for fuzzy full-text search at scale; LIKE doesn't scale to 100M rows |
| **Play history storage** | Cassandra | PostgreSQL | 46K writes/s, append-only, time-series â†’ Cassandra's LSM-tree is ideal; PG would struggle |
| **Playlist concurrency** | Optimistic locking | Pessimistic locking | Low contention; pessimistic would hold DB locks unnecessarily |
| **Recommendations** | Offline batch (Spark) â†’ precomputed | Real-time ML per request | Real-time ML at 200M DAU is too expensive; daily batch is good enough |
| **Search index sync** | Transactional outbox + CDC | Direct ES update after DB write | Guaranteed consistency; no lost index updates if ES is temporarily down |
| **Play count updates** | Batch aggregation (every few minutes) | Real-time counter increment | Real-time updates on a hot counter (popular songs) would create write hotspot |
| **Cache strategy** | App-level invalidation + TTL | CDC-based invalidation | Simple; all writes go through app. TTL as safety net. |

---

## 16. Common Mistakes to Avoid

| Mistake | Why It's Wrong | Correct Approach |
|---------|----------------|------------------|
| Streaming audio through your servers | Your servers become the bandwidth bottleneck (1 Tbps!) | Pre-signed CDN URLs. Server only issues the URL; CDN delivers audio. |
| Storing audio in the database | Databases are not designed for large binary blobs at PB scale | Object storage (S3) for audio; DB only stores metadata. |
| Updating play_count in real-time per play | Hot song = millions of concurrent writes to same row | Batch aggregation via Kafka â†’ periodic DB update. |
| Using SQL LIKE for search | LIKE '%query%' can't use indexes; full table scan on 100M rows | Elasticsearch with inverted index and fuzzy matching. |
| Single bitrate for all users | Users on 3G get buffering; users on WiFi get unnecessary low quality | Pre-encode at multiple bitrates; client selects adaptively. |
| No idempotency on playlist add | Network retry adds the same song twice | Client-generated idempotency key; DB stores it in same transaction. |
| Pessimistic locking on playlists | Holds DB lock during the entire edit; blocks other readers | Optimistic locking (version column). Playlist edits are low-contention. |
| Storing play history in PostgreSQL | 46K writes/s of append-only time-series data | Cassandra with partition by user_id, clustering by timestamp DESC. |

---

## 17. Interview Talking Points

### "Walk me through the architecture"

> Five core services: Streaming Service generates pre-signed CDN URLs (never touches audio bytes), Search Service queries Elasticsearch with fuzzy matching and popularity re-ranking, Playlist Service manages CRUD with optimistic locking for collaborative edits, Play History Service ingests events through Kafka into Cassandra, and Recommendation Service serves precomputed personalized playlists from Redis cache. Audio lives in S3, delivered via CDN. Metadata in PostgreSQL, cached in Redis. Play events flow through Kafka to feed Cassandra (history), Spark (recommendations), and royalty calculation.

### "How does streaming work?"

> User taps play â†’ Streaming Service looks up song metadata, selects bitrate based on client's network quality, generates a pre-signed CDN URL with a 1-hour expiry, and returns it. The client fetches audio directly from the CDN using HTTP Range requests â€” our servers never proxy audio bytes. The client buffers 10-30 seconds ahead and monitors download speed to adapt bitrate mid-song if needed. CDN edge servers cache popular songs; cache miss goes to S3 origin.

### "How do you handle a viral song that millions play simultaneously?"

> Three layers handle this: (1) CDN replicates popular content across all edge PoPs â€” the song is served from edge cache, not origin. (2) Song metadata is cached in Redis, so 70K metadata reads/s don't hit the DB. (3) Play events are partitioned by user_id in Kafka, not song_id, so writes for one popular song spread evenly across all Kafka partitions. Play count is updated via batch aggregation, not per-play increment, avoiding a write hotspot.

### "Why not real-time recommendations?"

> At 200M DAU, running an ML model per user per request is prohibitively expensive. Instead, we run a Spark batch job that reads play history from Cassandra, computes collaborative filtering (users who listened to X also listened to Y), and writes precomputed playlists to Redis. Recommendations refresh daily â€” users don't expect them to change in real-time. The trade-off: if a user's taste changes drastically today, Discover Weekly won't reflect it until tomorrow.

### "How do collaborative playlists handle concurrent edits?"

> Optimistic locking with a version column. Each edit reads the current version, applies the change, and updates with a `WHERE version = read_version` condition. If another user edited in between, the version won't match, the update affects 0 rows, and we return 409 Conflict. The client re-fetches the playlist and retries. This works because playlist edits are low-contention â€” two users rarely edit the same playlist in the same second. Contrast with Splitwise's balance updates where pessimistic locking is needed due to higher contention.

### "How do you keep Elasticsearch in sync with the catalog DB?"

> Transactional outbox pattern â€” same as we use in Splitwise, Uber, and BookMyShow. When a new song is inserted into PostgreSQL, an outbox row is written in the same transaction. A CDC process (or poller) reads the outbox and publishes to Kafka. An Elasticsearch indexer consumes from Kafka and updates the search index. If Elasticsearch is temporarily down, events queue in Kafka and are processed when it recovers. Search results may lag a few seconds behind catalog updates.

### "Why Cassandra for play history instead of PostgreSQL?"

> Play events are append-only, write-heavy (46K writes/s), and queried by user + time range ("show me Alice's last 50 plays"). Cassandra's LSM-tree is optimized for sequential writes, and partitioning by user_id with clustering by timestamp DESC gives efficient time-range queries. PostgreSQL could handle this with partitioning, but at 4B events/day, Cassandra's horizontal scalability and tunable consistency (we only need eventual for play history) make it the better fit.

### "What if the CDN is down?"

> Multi-CDN strategy â€” we contract with 2-3 CDN providers. If one CDN's edge is unreachable, DNS-level or client-level failover routes to another. For a regional outage, the CDN's own anycast routing sends traffic to the next-closest healthy PoP. If all edges fail for a song, the client falls back to fetching from S3 origin (higher latency but functional). Popular songs are replicated across so many edges that a total cache miss is extremely rare.
