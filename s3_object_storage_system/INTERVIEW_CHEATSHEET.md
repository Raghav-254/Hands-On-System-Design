# ğŸ—„ï¸ S3-like Object Storage - Interview Cheatsheet

> Based on Alex Xu's System Design Interview Volume 2 - Chapter 9

## Quick Reference Card

| Component | Purpose | Key Points |
|-----------|---------|------------|
| **API Service** | Handle HTTP requests (PUT, GET, DELETE) | Stateless, behind load balancer |
| **Identity & Access Mgmt** | Auth & authorization | Validates who can access which bucket/object |
| **Metadata Service** | Store/query object metadata | Where is this object? What versions exist? |
| **Metadata DB** | Persist metadata | Sharded database (bucket, object name, UUID, versions) |
| **Data Routing Service** | Route data to correct nodes | Consults Placement Service for node selection |
| **Placement Service** | Manage cluster topology | Virtual cluster map, heartbeats, node health |
| **Data Nodes** | Store actual bytes | Primary + Secondaries, replication across DCs |

---

## The Story: Building an S3-like Object Storage

Object storage is fundamentally different from file systems or block storage.
There are no directories â€” just flat key-value pairs: a **key** (like `photos/cat.jpg`)
maps to a **blob** of bytes. Let me walk you through designing one at 100 PB scale.

The entire system boils down to two questions:
1. **Where are the bytes?** â†’ Data Store (actual data on disk)
2. **What do I know about the object?** â†’ Metadata Store (name, size, location)

Every design decision flows from keeping these two concerns cleanly separated.

---

## 1. What Are We Building? (Requirements)

### Functional Requirements

- **Bucket creation** â€” top-level container for objects (globally unique name)
- **Object upload (PUT)** â€” store any blob of bytes under a key
- **Object download (GET)** â€” retrieve bytes by bucket + key
- **Object versioning** â€” keep multiple versions of the same key
- **Listing objects** â€” list objects in a bucket (like `aws s3 ls`)

### Non-Functional Requirements

- **100 PB** of storage capacity
- **6 nines durability** (99.9999%) â€” lose at most 1 object per million
- **4 nines availability** (99.99%) â€” ~52 minutes downtime per year
- **Storage efficiency** â€” minimize cost while maintaining durability

### Back-of-the-Envelope Estimation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Storage capacity:       100 PB                              â”‚
â”‚  Durability:             99.9999% (6 nines)                  â”‚
â”‚  Availability:           99.99% (4 nines)                    â”‚
â”‚                                                              â”‚
â”‚  Object size distribution:                                   â”‚
â”‚    20% small  (<1MB,  median 0.5MB)                          â”‚
â”‚    60% medium (1-64MB, median 32MB)                          â”‚
â”‚    20% large  (>64MB, median 200MB)                          â”‚
â”‚                                                              â”‚
â”‚  Weighted average size:                                      â”‚
â”‚    0.2 Ã— 0.5 + 0.6 Ã— 32 + 0.2 Ã— 200 = 59.3 MB             â”‚
â”‚                                                              â”‚
â”‚  At 40% storage utilization:                                 â”‚
â”‚    (100PB Ã— 0.4) / 59.3 MB â‰ˆ 0.68 billion objects           â”‚
â”‚                                                              â”‚
â”‚  Metadata per object: ~1 KB                                  â”‚
â”‚  Total metadata: 0.68B Ã— 1KB = 0.68 TB                      â”‚
â”‚                                                              â”‚
â”‚  â†’ Metadata is TINY (0.68 TB) vs actual data (100 PB)       â”‚
â”‚  â†’ Bottleneck is DISK CAPACITY and DISK IOPS, not metadata  â”‚
â”‚  â†’ SATA 7200 rpm disk: 100-150 random IOPS                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. API Design

Object storage APIs follow a simple REST pattern. The object is addressed
by `bucket_name` + `object_name` (the key).

### Bucket APIs

```
PUT /bucket-to-share
  â†’ Creates a new bucket named "bucket-to-share"
  â†’ Bucket names are globally unique

DELETE /bucket-to-share
  â†’ Deletes the bucket (must be empty)
```

### Object APIs

```
PUT /{bucket_name}/{object_name}
  Body: raw bytes of the object
  Headers: Content-Type, Content-Length
  â†’ Uploads (or overwrites) the object
  â†’ Returns: { object_id (UUID), version_id, etag (checksum) }

GET /{bucket_name}/{object_name}
  â†’ Downloads the object (latest version)
  â†’ Returns: raw bytes + metadata headers

GET /{bucket_name}/{object_name}?versionId=3
  â†’ Downloads a specific version

DELETE /{bucket_name}/{object_name}
  â†’ Non-versioned: permanently deletes
  â†’ Versioned: adds a "delete marker" (soft delete)

GET /{bucket_name}?prefix=photos/&maxKeys=1000
  â†’ Lists objects in the bucket with optional prefix filter
  â†’ Returns: list of { object_name, size, last_modified }
```

---

## 3. The Big Picture (High-Level Architecture)

The system has two cleanly separated halves: **Metadata Store** (what/where)
and **Data Store** (actual bytes). They're separated because metadata is tiny
(0.68 TB) and needs fast queries, while data is massive (100 PB) and needs
raw disk throughput.

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              S3-LIKE OBJECT STORAGE - HIGH-LEVEL DESIGN                      â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                               â•‘
â•‘                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                     â•‘
â•‘                         â”‚    Client    â”‚                                     â•‘
â•‘                         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                     â•‘
â•‘                                â”‚                                              â•‘
â•‘                         â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”                                     â•‘
â•‘                         â”‚Load Balancer â”‚                                     â•‘
â•‘                         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                     â•‘
â•‘                                â”‚                                              â•‘
â•‘         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â•‘
â•‘         â”‚                      â”‚                      â”‚                      â•‘
â•‘         â–¼                      â–¼                      â–¼                      â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â•‘
â•‘  â”‚  Identity &  â”‚â—€â”€â”€â”€â–¶â”‚  API Service â”‚â”€â”€â”€â”€â–¶â”‚   Data Store     â”‚             â•‘
â•‘  â”‚  Access Mgmt â”‚     â”‚  (stateless) â”‚     â”‚                  â”‚             â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚             â•‘
â•‘                              â”‚              â”‚ â”‚Data Routing  â”‚â”‚             â•‘
â•‘                              â”‚              â”‚ â”‚Service       â”‚â”‚             â•‘
â•‘                              â–¼              â”‚ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜â”‚             â•‘
â•‘                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚        â”‚         â”‚             â•‘
â•‘                       â”‚  Metadata    â”‚     â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”â”‚             â•‘
â•‘                       â”‚  Store       â”‚     â”‚ â”‚ Placement    â”‚â”‚             â•‘
â•‘                       â”‚              â”‚     â”‚ â”‚ Service      â”‚â”‚             â•‘
â•‘                       â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚     â”‚ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜â”‚             â•‘
â•‘                       â”‚ â”‚Metadata  â”‚ â”‚     â”‚        â”‚         â”‚             â•‘
â•‘                       â”‚ â”‚Service   â”‚ â”‚     â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”â”‚             â•‘
â•‘                       â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â”‚     â”‚ â”‚ Data Nodes   â”‚â”‚             â•‘
â•‘                       â”‚      â”‚       â”‚     â”‚ â”‚(Primary +    â”‚â”‚             â•‘
â•‘                       â”‚ â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â” â”‚     â”‚ â”‚ Secondaries) â”‚â”‚             â•‘
â•‘                       â”‚ â”‚Metadata  â”‚ â”‚     â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚             â•‘
â•‘                       â”‚ â”‚   DB     â”‚ â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â•‘
â•‘                       â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚                                      â•‘
â•‘                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                      â•‘
â•‘                                                                               â•‘
â•‘  Metadata Store: SMALL (0.68 TB)        Data Store: HUGE (100 PB)           â•‘
â•‘  "Where is this object?"                "Here are the actual bytes"          â•‘
â•‘                                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## 4. Deep Dive: Uploading an Object

This involves **two separate API calls** from the client. The bucket must
exist before uploading objects to it.

### Call 1: Create the Bucket (one-time setup)

> **Why PUT and not POST?** PUT is idempotent (safe to retry) and means "create
> at this exact URL" â€” the client chooses the bucket name, not the server.

```
User: PUT /bucket-to-share
      â”‚
      â–¼
â‘  Load Balancer â†’ API Service
      â”‚
      â–¼
â‘¡ API Service â†’ Identity & Access Management
   "Does this user have permission to create a bucket?"
      â”‚
      â–¼
â‘¢ API Service â†’ Metadata Service
   Store: bucket_name="bucket-to-share", owner, creation_time, ACLs
      â”‚
      â–¼
â‘£ Return success: bucket created
```

### Call 2: Upload an Object

```
User: PUT /bucket-to-share/script.txt  (with file bytes)
      â”‚
      â–¼
â‘  Load Balancer â†’ API Service
      â”‚
      â–¼
â‘¡ API Service â†’ Identity & Access Management
   "Does this user have permission to PUT objects in 'bucket-to-share'?"
   â†’ YES: continue
   â†’ NO: 403 Forbidden
      â”‚
      â–¼
â‘¢ API Service â†’ Data Store (upload the actual bytes)
   â”‚
   â”œâ”€â”€â–¶ Data Routing Service
   â”‚    "Where should I store this?"
   â”‚         â”‚
   â”‚         â–¼
   â”‚    Placement Service
   â”‚    "Use node-1 (DC-1) as Primary, node-2 (DC-2) and node-3 (DC-3) as Secondaries"
   â”‚         â”‚
   â”‚         â–¼
   â”‚    Write to Primary Data Node â†’ Replicate to Secondaries
   â”‚    Returns: object_id (UUID) = "a1b2c3d4-..."
   â”‚
      â–¼
â‘£ API Service â†’ Metadata Service
   "Store the metadata for this object"
   Record: bucket_name="bucket-to-share", object_name="script.txt",
           object_id="a1b2c3d4-...", size=4096, checksum="sha256:abcd..."
      â”‚
      â–¼
â‘¤ Return success to user: { object_id, etag }
```

> **Why two separate calls?** Creating a bucket and uploading an object are
> different operations â€” like creating a folder vs putting a file in it.
> The bucket is created once; then you upload millions of objects to it.
> Each call goes through auth independently (standard REST practice).

> **Why data BEFORE metadata?** If we stored metadata first and then the data
> upload failed, we'd have a metadata entry pointing to non-existent bytes
> (a dangling pointer). By storing data first, the worst case is orphaned data
> with no metadata â€” which garbage collection can clean up later (Section 12).

---

## 5. Deep Dive: Downloading an Object

Now the reverse â€” fetching `script.txt` from `bucket-to-share`.

```
User: GET /bucket-to-share/script.txt
      â”‚
      â–¼
â‘  Load Balancer â†’ API Service
      â”‚
      â–¼
â‘¡ API Service â†’ Identity & Access Management
   "Does this user have read access to 'bucket-to-share'?"
      â”‚
      â–¼
â‘¢ API Service â†’ Metadata Service
   "Where is 'bucket-to-share/script.txt'?"
   â†’ Query Metadata DB
   â†’ Returns: object_id = "a1b2c3d4-...", size = 4096, nodes = [node-1, node-2, node-3]
      â”‚
      â–¼
â‘£ API Service â†’ Data Store
   "Give me the bytes for object_id = a1b2c3d4-..."
   â”‚
   â”œâ”€â”€â–¶ Data Routing Service â†’ tries Primary (node-1) first
   â”‚    â†’ If Primary is down â†’ fallback to Secondary (node-2)
   â”‚
      â–¼
â‘¤ Return bytes to user + metadata headers (Content-Type, ETag, etc.)
```

> **Key insight:** The download path uses the `object_id` (UUID) from the
> Metadata DB to locate the actual bytes in the Data Store. The Metadata
> Store is the "index" and the Data Store is the "filing cabinet."

---

## 6. Deep Dive: Data Store Internals

Now let's zoom into the Data Store â€” the part that actually persists bytes to disk.
This is where durability and performance live.

### Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Data Store                                                        â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”‚
â”‚  â”‚ Data Routing     â”‚ â† Receives data from API Service           â”‚
â”‚  â”‚ Service          â”‚ â† Consults Placement Service for routing   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                            â”‚
â”‚           â”‚                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”‚
â”‚  â”‚ Placement Serviceâ”‚ â† Knows the virtual cluster map            â”‚
â”‚  â”‚                  â”‚ â† Receives heartbeats from all data nodes  â”‚
â”‚  â”‚                  â”‚ â† Decides: which node is Primary, which    â”‚
â”‚  â”‚                  â”‚   are Secondaries for each new object      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                            â”‚
â”‚           â”‚                                                       â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚     â–¼     â–¼                  â–¼                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚  â”‚Node 1â”‚ â”‚Node 2â”‚   â”‚Node 3â”‚                                    â”‚
â”‚  â”‚(DC-1)â”‚ â”‚(DC-2)â”‚   â”‚(DC-3)â”‚                                    â”‚
â”‚  â”‚Primaryâ”‚ â”‚Sec.  â”‚   â”‚Sec.  â”‚                                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚                                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Virtual Cluster Map

The Placement Service maintains a hierarchical view of the cluster:

```
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        Root:         â”‚ Default â”‚
                      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                     â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
        Datacenter:  â”‚            â”‚
                  â”Œâ”€â”€â”´â”€â”€â”    â”Œâ”€â”€â”€â”´â”€â”€â”
                  â”‚DC-1 â”‚    â”‚DC-2  â”‚
                  â””â”€â”€â”¬â”€â”€â”˜    â””â”€â”€â”¬â”€â”€â”€â”˜
                 â”Œâ”€â”€â”€â”´â”€â”€â”€â”   â”Œâ”€â”€â”´â”€â”€â”€â”
        Host:    â”‚       â”‚   â”‚      â”‚
              â”Œâ”€â”€â”´â”€â”  â”Œâ”€â”´â”€â”€â” â”Œâ”´â”€â”€â” â”Œâ”´â”€â”€â”
              â”‚H-1 â”‚  â”‚H-2 â”‚ â”‚H-3â”‚ â”‚H-4â”‚
              â””â”€â”¬â”€â”€â”˜  â””â”€â”¬â”€â”€â”˜ â””â”€â”¬â”€â”˜ â””â”€â”¬â”€â”˜
        Part:  P1,P2  P3,P4  P5,P6  P7,P8
```

> **Why this hierarchy?** When placing replicas, the Placement Service ensures
> they go to **different data centers** (or at least different hosts). This way,
> if an entire DC goes down, replicas in other DCs keep the data available.

### Data Persistence Flow

```
API Service
    â”‚
    â”‚ â‘  Write data
    â–¼
Data Routing Service
    â”‚
    â”‚ â‘¡ "Where should this go?"
    â–¼
Placement Service â—€â”€â”€â”€â”€ Heartbeats from all Data Nodes
    â”‚
    â”‚ "Primary = node-1 (DC-1), Secondaries = node-2 (DC-2), node-3 (DC-3)"
    â”‚
    â”‚ â‘¢ Send data to Primary
    â–¼
Data Node Primary (node-1)
    â”‚
    â”‚ â‘£ Replicate to Secondaries
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Data Node Secondary (node-2)
    â”‚                    â‘£ Replicate
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Data Node Secondary (node-3)
    â”‚
    â”‚ â‘¤ All replicas confirmed â†’ return object_id (UUID) to API Service
    â–¼
API Service receives object_id
```

---

## 7. Deep Dive: Data Organization on Disk

How do data nodes actually store objects on disk? Not as individual files â€” that would
be terrible for small objects.

### The Problem with One-File-Per-Object

```
If we store each object as a separate file on disk:
  â€¢ 0.68 billion objects = 0.68 billion files
  â€¢ Each file has inode overhead (~256 bytes)
  â€¢ Linux ext4 limit: ~4 billion inodes per filesystem
  â€¢ Tons of small files â†’ filesystem metadata bloat
  â€¢ Random I/O for small files kills disk performance
```

### The Solution: Pack Small Objects into Large Files

```
Each data node has a local file system with:

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Read-only    â”‚ â”‚ Read-only    â”‚ â”‚ Read-write File   â”‚
  â”‚ File         â”‚ â”‚ File         â”‚ â”‚                   â”‚
  â”‚              â”‚ â”‚              â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
  â”‚ (sealed,     â”‚ â”‚ (sealed,     â”‚ â”‚ â”‚ object 1      â”‚ â”‚ â† append
  â”‚  immutable)  â”‚ â”‚  immutable)  â”‚ â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
  â”‚              â”‚ â”‚              â”‚ â”‚ â”‚ object 2      â”‚ â”‚ â† append
  â”‚              â”‚ â”‚              â”‚ â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ object 3      â”‚ â”‚ â† append
                                     â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
                                     â”‚ â”‚ object 4      â”‚ â”‚ â† NEW write
                                     â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
                                     â”‚ â”‚ (empty space)  â”‚ â”‚
                                     â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  How it works:
  â‘  New objects are APPENDED to the current read-write file
    (like a WAL â€” write-ahead log, sequential writes = fast)
  â‘¡ When the read-write file reaches a size limit (e.g., a few GB)
    â†’ it's sealed and becomes read-only
    â†’ a new read-write file is created
  â‘¢ Metadata records: object_id â†’ (file_name, offset, size)
    So to read object 2: seek to offset in the file, read 'size' bytes

  Why "read-write" vs "read-only"?
  â€¢ Read-write file: the ACTIVE file currently accepting new writes (appends).
    Only ONE read-write file exists at a time per data node.
  â€¢ Read-only files: SEALED files that are full and no longer accept writes.
    They only serve read requests. Immutable = safe to cache, replicate.

  Lifecycle:  New (read-write) â†’ Full â†’ Sealed (read-only) â†’ Compacted eventually

  Why?
  â€¢ Sequential writes (append-only) are 100Ã— faster than random writes
  â€¢ Fewer files = less filesystem metadata overhead
  â€¢ Small objects packed together = efficient disk utilization
```

---

## 8. Deep Dive: Durability

We promised 6 nines (99.9999%) durability. How do we ensure objects aren't lost?

### Option 1: Replication (Simple, More Storage)

```
Store 3 copies of every object across different data centers:

  Object "a1b2c3" is stored on:
    node-1 (DC-1) â† Primary
    node-2 (DC-2) â† Secondary
    node-3 (DC-3) â† Secondary

  If node-1 dies â†’ still have 2 copies
  If DC-1 burns down â†’ still have copies in DC-2 and DC-3

  Storage overhead: 3Ã— (store 300 PB for 100 PB of data)
  
  Pros: Simple, fast reads (read from any replica)
  Cons: 3Ã— storage cost â€” expensive at petabyte scale
```

### Option 2: Erasure Coding (Complex, Less Storage)

```
Instead of storing 3 full copies, split data into chunks and add parity:

  What are parity chunks?
    Parity chunks are mathematical "summaries" computed from the data chunks.
    They contain enough information to reconstruct any missing data chunk.

    Simple analogy (XOR parity):
      Data:   A = 5,  B = 3
      Parity: P = A + B = 8
      If A is lost â†’ A = P - B = 8 - 3 = 5 (reconstructed!)

    Reed-Solomon is more complex math, but the same idea â€” parity lets
    you recover lost data without storing full copies.

  Example: 4+2 erasure coding (Reed-Solomon)
    Original object â†’ split into 4 data chunks (d1, d2, d3, d4)
    Compute 2 parity chunks (p1, p2) from the data chunks
    Store all 6 chunks on 6 different nodes

    d1 â†’ node-1 (DC-1)
    d2 â†’ node-2 (DC-1)
    d3 â†’ node-3 (DC-2)
    d4 â†’ node-4 (DC-2)
    p1 â†’ node-5 (DC-3)
    p2 â†’ node-6 (DC-3)

  Can tolerate ANY 2 node failures:
    Lost d1, d3? â†’ Reconstruct from d2, d4, p1, p2
    Lost d1, p1? â†’ Reconstruct from d2, d3, d4, p2

  Storage overhead: 6/4 = 1.5Ã— (vs 3Ã— for replication)
  Saves 50% storage compared to 3-way replication!

  Pros: Storage efficient (1.5Ã— vs 3Ã—)
  Cons: Complex reconstruction, higher read latency (need multiple nodes),
        CPU overhead for encoding/decoding
```

### Comparison

| | Replication (3 copies) | Erasure Coding (4+2) |
|---|---|---|
| **Storage overhead** | 3Ã— (300 PB for 100 PB) | 1.5Ã— (150 PB for 100 PB) |
| **Fault tolerance** | Survives 2 node failures | Survives 2 node failures |
| **Read speed** | Fast (read from any copy) | Slower (may need to reconstruct) |
| **Write speed** | Fast (just copy bytes) | Slower (compute parity) |
| **Complexity** | Simple | Complex (Reed-Solomon math) |
| **Best for** | Hot data (frequently accessed) | Cold data (rarely accessed, large) |

> **In practice:** Use replication for hot/small objects (fast reads),
> erasure coding for cold/large objects (save storage costs).

---

## 9. Deep Dive: Correctness Verification

How do we know the bytes we read back are the same bytes that were written?
Disks can have silent data corruption (bit rot).

```
Where is the checksum stored?
  â†’ In the METADATA DB (objects table), as a column alongside object_id, size, etc.
  â†’ NOT in the data file itself. The data file just has raw bytes.

Who computes and compares?
  â†’ The API Service / Data Node computes it. Metadata DB stores it.

Write path (during upload):
  â‘  API Service receives bytes from client
  â‘¡ API Service computes checksum: sha256(bytes) = "abcd1234..."
  â‘¢ Data Node stores raw bytes in the read-write file
  â‘£ API Service stores checksum in Metadata DB:
     object_id â†’ checksum = "sha256:abcd1234..."

Read path (during download):
  â‘  Data Node reads raw bytes from the data file
  â‘¡ Data Node recomputes checksum: sha256(bytes_read) = ???
  â‘¢ Compare with checksum from Metadata DB
     â†’ Match: data is good âœ“ â†’ return to client
     â†’ Mismatch: data is CORRUPT!
       â†’ Read from a different replica
       â†’ Re-replicate the good copy to replace the corrupt one

Background:
  Periodic "scrubbing" â€” read all data and verify checksums.
  Detects bit rot before a second failure makes it unrecoverable.
```

---

## 10. Deep Dive: Metadata Data Model

The metadata is small but critical â€” it's the "index" that makes everything findable.

### Schema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ buckets table                                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ bucket_name (PK) â”‚ "bucket-to-share"                             â”‚
â”‚ owner_id         â”‚ "user-alice"                                  â”‚
â”‚ versioning       â”‚ true/false                                    â”‚
â”‚ created_at       â”‚ timestamp                                     â”‚
â”‚ acl              â”‚ access control list                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ objects table                                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ bucket_name (PK) â”‚ "bucket-to-share"                             â”‚
â”‚ object_name (PK) â”‚ "script.txt"                                  â”‚
â”‚ version_id  (PK) â”‚ 3 (for versioned buckets)                    â”‚
â”‚ object_id        â”‚ "a1b2c3d4-..." (UUID â€” points to Data Store) â”‚
â”‚ size             â”‚ 4096 bytes                                    â”‚
â”‚ content_type     â”‚ "text/plain"                                  â”‚
â”‚ checksum         â”‚ "sha256:abcd1234..."                          â”‚
â”‚ is_delete_marker â”‚ false                                         â”‚
â”‚ created_at       â”‚ timestamp                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ PK: (bucket_name, object_name, version_id)                       â”‚
â”‚                                                                   â”‚
â”‚ The object_id (UUID) is the bridge between Metadata and Data     â”‚
â”‚ Store. Metadata says "script.txt is UUID a1b2c3d4", and the     â”‚
â”‚ Data Store knows where UUID a1b2c3d4's bytes live on disk.       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Store Internal Mapping

The Data Store maintains its own mapping from `object_id` to the physical
location on disk. This is stored locally on each Data Node.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Data Node local index (stored on each node)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ object_id     â”‚ "a1b2c3d4-..."                                   â”‚
â”‚ file_name     â”‚ "data_file_0042.dat" (which packed file)         â”‚
â”‚ offset        â”‚ 8192 (byte offset within the file)               â”‚
â”‚ size          â”‚ 4096 (bytes to read from that offset)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                   â”‚
â”‚ To read object "a1b2c3d4":                                       â”‚
â”‚   Open "data_file_0042.dat", seek to offset 8192, read 4096 bytesâ”‚
â”‚                                                                   â”‚
â”‚ This index is small (object_id + file + offset + size â‰ˆ 100 bytesâ”‚
â”‚ per entry) and kept in memory or in a local embedded DB (RocksDB)â”‚
â”‚ on each Data Node for fast lookups.                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

So the full lookup chain is:

  Client: "Give me bucket-to-share/script.txt"
    â”‚
    â–¼
  Metadata DB: (bucket, object_name) â†’ object_id = "a1b2c3d4"
    â”‚
    â–¼
  Data Node local index: object_id â†’ (file_name, offset, size)
    â”‚
    â–¼
  Disk: read file at offset â†’ return raw bytes
```

### Which Database for Metadata?

```
Metadata is 0.68 TB â€” small enough for a traditional relational DB.

Why MySQL/PostgreSQL (with sharding)?
  â€¢ Metadata operations need ACID transactions
    (e.g., versioning: atomically add new version + update "latest" pointer)
  â€¢ Access patterns are simple: point queries and range scans by URI
  â€¢ 0.68 TB fits comfortably in a sharded MySQL setup (e.g., Vitess)
  â€¢ Well-understood, battle-tested for this scale

Why NOT NoSQL (Cassandra/DynamoDB)?
  â€¢ We need strong consistency for metadata (two uploads to the same key
    must not create conflicting entries)
  â€¢ Listing objects in a bucket needs range scans â€” relational DBs
    handle this well with proper indexing
```

### Sharding the Metadata DB

```
The queries we need to support efficiently:

  Query 1: Find object by URI (used by GET, PUT, DELETE)
    SELECT * FROM objects
    WHERE bucket_name = 'my-bucket' AND object_name = 'photos/cat.jpg'
    â†’ Most frequent query â€” every upload, download, delete uses this

  Query 2: List objects in a bucket (used by aws s3 ls)
    SELECT * FROM objects
    WHERE bucket_name = 'my-bucket' AND object_name LIKE 'photos/%'
    â†’ Less frequent â€” browsing / listing operations

  Query 3: Get all versions of an object (used by version listing)
    SELECT * FROM objects
    WHERE bucket_name = 'my-bucket' AND object_name = 'cat.jpg'
    ORDER BY version_id DESC

The shard key must optimize for Query 1 (by far the most common).

At 0.68 TB, a single instance could handle the storage.
But for high availability and throughput, we shard.

Three sharding options:

  âŒ Option 1: Shard by bucket_name
     All objects in the same bucket â†’ same shard.
     Problem: One bucket can have BILLIONS of objects â†’ hotspot shard!
     A single popular bucket overwhelms one shard.

  âŒ Option 2: Shard by object_id (UUID)
     Evenly distributed â€” no hotspots.
     Problem: Most API operations use the URI, NOT the object_id.

     URI vs UUID â€” they are completely different:
       URI  = the human-readable path the client uses in the URL
              e.g., /my-bucket/photos/cat.jpg  (bucket_name + object_name)
       UUID = the internal ID the server generates after upload
              e.g., a1b2c3d4-e5f6-7890-...  (object_id, client never sees this)

     The client only knows the URI. For example:
       PUT /my-bucket/photos/cat.jpg  â†’ we know bucket + object name
       GET /my-bucket/photos/cat.jpg  â†’ we know bucket + object name
     With object_id sharding, we'd need to query ALL shards to find
     which shard has this object's metadata. That's a scatter-gather
     across every shard â€” terrible for latency.

  âœ… Option 3: Shard by hash(bucket_name, object_name)
     Shard key matches the access pattern!

     Why it works:
       Every API call includes bucket_name + object_name in the URL.
       hash("my-bucket", "photos/cat.jpg") â†’ shard 7
       â†’ Route directly to shard 7. No scatter-gather.

       PUT /my-bucket/photos/cat.jpg â†’ hash â†’ shard 7 â†’ write metadata
       GET /my-bucket/photos/cat.jpg â†’ hash â†’ shard 7 â†’ read metadata

     Why no hotspot?
       Objects from the same bucket are SPREAD across shards
       (hash distributes them). Unlike Option 1, no single shard
       gets all objects from a popular bucket.

     Example: 6 objects in "my-photos" bucket, 3 shards

       Upload â€” each object hashed to a shard:
         hash("my-photos", "beach.jpg")  â†’ shard 1
         hash("my-photos", "sunset.jpg") â†’ shard 3
         hash("my-photos", "cat.jpg")    â†’ shard 2
         hash("my-photos", "dog.jpg")    â†’ shard 1
         hash("my-photos", "party.jpg")  â†’ shard 3
         hash("my-photos", "food.jpg")   â†’ shard 2

       GET (Query 1 â€” fast, direct routing):
         GET /my-photos/cat.jpg
         â†’ hash("my-photos", "cat.jpg") â†’ shard 2
         â†’ Go directly to shard 2, get metadata. One shard. Done.

       LIST (Query 2 â€” scatter-gather):
         aws s3 ls s3://my-photos/
         â†’ We need ALL objects in "my-photos"
         â†’ But they're spread across shard 1, 2, and 3!
         â†’ Must query ALL shards in parallel:
           Shard 1 returns: beach.jpg, dog.jpg
           Shard 2 returns: cat.jpg, food.jpg
           Shard 3 returns: sunset.jpg, party.jpg
         â†’ Combine results â†’ return to client

     Tradeoff:
       GET/PUT is fast (direct to one shard), LIST is slower (all shards).
       But this is acceptable because:
       â€¢ Listing is much less frequent than GET/PUT
       â€¢ Scatter-gather queries run in parallel across shards
       â€¢ We optimized for the MOST COMMON operation (direct routing)
```

---

## 11. Deep Dive: Object Versioning

When versioning is enabled, uploading the same key creates a new version
instead of overwriting.

```
PUT script.txt (v1: "hello")    â†’ object_id = UUID-aaa, version_id = 1
PUT script.txt (v2: "hello v2") â†’ object_id = UUID-bbb, version_id = 2
PUT script.txt (v3: "hello v3") â†’ object_id = UUID-ccc, version_id = 3

Metadata DB now has 3 rows for "script.txt":
  (bucket, "script.txt", v1) â†’ UUID-aaa
  (bucket, "script.txt", v2) â†’ UUID-bbb
  (bucket, "script.txt", v3) â†’ UUID-ccc

GET script.txt         â†’ returns v3 (latest)
GET script.txt?v=1     â†’ returns v1

DELETE script.txt (versioned bucket):
  â†’ Does NOT delete any data!
  â†’ Adds a "delete marker" as v4:
    (bucket, "script.txt", v4) â†’ DELETE_MARKER

  GET script.txt       â†’ 404 (latest version is a delete marker)
  GET script.txt?v=2   â†’ still returns v2 (old versions preserved!)

  To permanently delete: DELETE script.txt?v=2
  â†’ Actually removes the row and the data for v2
```

> **Why delete markers?** They preserve the version history. An accidental
> delete doesn't destroy data â€” just hides it. You can "undelete" by
> removing the delete marker.

---

## 12. Deep Dive: Multipart Upload (Large Files)

A 5 GB file uploaded as a single HTTP request is fragile â€” any network hiccup
means starting over. Multipart upload solves this.

```
Flow:
  â‘  Client: POST /bucket/large-file.zip?uploads
     â†’ Server returns: upload_id = "UP-12345"

  â‘¡ Client uploads parts in parallel (separate PUT call per part):
     PUT /bucket/large-file.zip?uploadId=UP-12345&partNumber=1  (5 MB)
     PUT /bucket/large-file.zip?uploadId=UP-12345&partNumber=2  (5 MB)
     PUT /bucket/large-file.zip?uploadId=UP-12345&partNumber=3  (3 MB)
     â†’ Each part returns: etag (checksum of that part)
     â†’ Parts can be uploaded IN PARALLEL from multiple threads/machines

  â‘¢ Client sends "complete" call (separate POST):
     POST /bucket/large-file.zip?uploadId=UP-12345
     Body: { parts: [{partNumber: 1, etag: "..."}, {2, "..."}, {3, "..."}] }
     â†’ Server verifies all parts received (etags match)
     â†’ Assembles parts into final object
     â†’ Creates single metadata entry pointing to all parts

  Why are â‘¡ and â‘¢ separate calls? Why doesn't the server auto-assemble?
  â€¢ The CLIENT split the file, so only the client knows how many parts
    to expect. The server has no idea if 3 parts or 100 parts are coming.
  â€¢ Parts can arrive out of order, days apart, from different machines.
    The server can't guess when "all parts are done."
  â€¢ The client tracks its own uploads â€” once all PUTs return success,
    it knows it's done and sends the "complete" call (â‘¢).
  â€¢ This also lets the client ABORT mid-way (skip â‘¢ â†’ server cleans up
    parts) or retry a single failed part without re-uploading everything.

  Benefits:
  â€¢ Parts upload in parallel â†’ faster
  â€¢ If part 2 fails â†’ retry ONLY part 2 (not the whole file)
  â€¢ Can pause and resume uploads
  â€¢ Recommended for objects > 100 MB
```

---

## 13. Deep Dive: Garbage Collection

Over time, the Data Store accumulates orphaned or deleted data that needs cleanup.

### What Needs Garbage Collection?

```
â‘  Orphaned data (data stored but metadata write failed)
   â†’ Data exists in Data Store, but no metadata points to it
   â†’ Caused by crash between step â‘¥ and â‘¦ in upload flow

â‘¡ Deleted objects (non-versioned bucket)
   â†’ Metadata removed, but data bytes still on disk
   â†’ GC scans for unreferenced object_ids and reclaims space

â‘¢ Deleted versions (versioned bucket, permanent delete)
   â†’ Specific version's data bytes need cleanup

â‘£ Compaction of data files
   â†’ After many deletes, data files have "holes" (deleted objects)
   â†’ Compaction rewrites live objects into new files, reclaims space
   â†’ Similar to LSM-tree compaction
```

### Compaction Process

```
Before compaction:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ obj1 â”‚ DELETED â”‚ obj3 â”‚ DELETED â”‚ obj5 â”‚ (empty) â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†’ 40% of file is wasted space

After compaction:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ obj1 â”‚ obj3 â”‚ obj5 â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†’ Tight packing, no wasted space
  â†’ Old file deleted, metadata updated to point to new file
```

> **When to compact?** When a data file's dead-object ratio exceeds a threshold
> (e.g., 30% deleted). Compaction runs in the background, never blocking reads.

---

## 14. Listing Objects in a Bucket

How listing routes across shards is covered in Section 10 (scatter-gather).
Here we cover the additional details: prefix filtering and pagination.

```
Prefix filtering:
  GET /my-photos?prefix=vacation/&maxKeys=1000

  Object names like "vacation/beach.jpg", "vacation/sunset.jpg"
  are just flat strings â€” there are no real directories in object storage.
  Prefix filtering = string matching on the object_name column.
  Each shard runs: WHERE object_name LIKE 'vacation/%'

Pagination:
  If > 1000 results, response includes a continuation_token.
  Next request: GET /my-photos?prefix=vacation/&continuation_token=xyz
  The token encodes the last object_name seen, so each shard can
  resume from where it left off.
```

---

## 15. Why These Choices? (Key Design Decisions)

### Decision #1: Separate Metadata Store from Data Store

**Why?** Metadata is tiny (0.68 TB) and needs fast lookups and transactions.
Data is massive (100 PB) and needs raw sequential disk throughput.
Combining them would force one system to do both poorly.

### Decision #2: Append-Only Data Files (Not One-File-Per-Object)

**Why?** Billions of small files would exhaust filesystem inodes and cause
random I/O. Packing objects into large append-only files gives sequential
writes (100Ã— faster) and reduces filesystem overhead.

### Decision #3: Replication + Erasure Coding (Hybrid)

**Why?** Replication is simple and fast but 3Ã— storage cost. Erasure coding
saves 50% storage but is complex. Use replication for hot data,
erasure coding for cold data â€” best of both worlds.

### Decision #4: Data Before Metadata (Upload Order)

**Why?** If metadata is written first and data upload fails, we have a
dangling pointer (metadata pointing to nothing). Orphaned data (data
without metadata) is safe â€” garbage collection cleans it up.

### Decision #5: Placement Across Data Centers

**Why?** Placing replicas in different DCs ensures that an entire DC failure
doesn't lose all copies. The Placement Service's virtual cluster map
enforces this topology-aware placement.

---

## 16. Interview Pro Tips

### Opening Statement
"S3-like object storage is a two-part system: a Metadata Store (tiny, ~TB scale, needs fast queries) and a Data Store (massive, ~PB scale, needs disk throughput). Objects are addressed by bucket+key, stored with UUID linking metadata to data. Durability comes from replication for hot data and erasure coding for cold data. Data is organized as append-only files on disk for sequential write performance."

### Key Talking Points
1. **Two halves:** Metadata Store (what/where) vs Data Store (actual bytes)
2. **Upload:** Data first, then metadata (avoid dangling pointers)
3. **Download:** Metadata lookup (UUID) â†’ Data Store fetch
4. **Data organization:** Append-only files, not one-file-per-object
5. **Durability:** Replication (3Ã—, simple, hot data) + Erasure coding (1.5Ã—, cold data)
6. **Versioning:** New version = new UUID, delete = delete marker
7. **Multipart upload:** Parallel parts, retry individual parts, assemble at end
8. **Garbage collection:** Clean orphaned data, compact files with holes

### Common Follow-ups

**Q: How does the Placement Service decide which nodes to use?**
A: It maintains a virtual cluster map (hierarchy: DC â†’ Host â†’ Partition). When placing replicas, it ensures they're in different DCs. It monitors node health via heartbeats and avoids unhealthy nodes. If a node goes down, it triggers re-replication to maintain the replica count.

**Q: What happens if a Data Node dies permanently?**
A: The Placement Service detects missing heartbeats. It identifies all objects that had a replica on that node (from the replica map) and triggers re-replication from surviving replicas to a new node, restoring the target replica count.

**Q: How do you handle concurrent uploads to the same key?**
A: Last-writer-wins. If two clients upload to the same key simultaneously, both complete, but the last one to finish becomes the latest version. With versioning enabled, both versions are preserved.

**Q: Why not use HDFS or a distributed file system?**
A: HDFS is optimized for large sequential reads/writes (batch analytics). Object storage needs to handle both tiny (50 KB) and huge (5 GB) objects efficiently, with HTTP API access, per-object permissions, and flat key-value semantics â€” different access patterns.

---

## 17. Visual Architecture Summary

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                S3-LIKE OBJECT STORAGE - COMPLETE FLOW                        â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                               â•‘
â•‘  UPLOAD:  Client â†’ LB â†’ API Service                                         â•‘
â•‘           â†’ Auth (IAM)                                                        â•‘
â•‘           â†’ Data Store: Routing â†’ Placement â†’ Primary â†’ Replicas             â•‘
â•‘           â†’ Metadata Store: Record (bucket, key, UUID, size, checksum)       â•‘
â•‘           â†’ Return success                                                    â•‘
â•‘                                                                               â•‘
â•‘  DOWNLOAD: Client â†’ LB â†’ API Service                                        â•‘
â•‘            â†’ Auth (IAM)                                                       â•‘
â•‘            â†’ Metadata Store: Lookup UUID by (bucket, key)                    â•‘
â•‘            â†’ Data Store: Fetch bytes by UUID                                 â•‘
â•‘            â†’ Return bytes                                                     â•‘
â•‘                                                                               â•‘
â•‘  DATA STORE INTERNALS:                                                        â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘
â•‘  â”‚ Data Routing Service â†’ Placement Service â†’ Data Nodes                  â”‚  â•‘
â•‘  â”‚                                                                         â”‚  â•‘
â•‘  â”‚ Placement Service:                                                      â”‚  â•‘
â•‘  â”‚   Virtual cluster map: Root â†’ DC â†’ Host â†’ Partition                    â”‚  â•‘
â•‘  â”‚   Heartbeats from all nodes, topology-aware placement                  â”‚  â•‘
â•‘  â”‚                                                                         â”‚  â•‘
â•‘  â”‚ Data Nodes:                                                             â”‚  â•‘
â•‘  â”‚   Append-only files (small objects packed into large files)             â”‚  â•‘
â•‘  â”‚   Primary writes â†’ replicate to Secondaries across DCs                 â”‚  â•‘
â•‘  â”‚   Checksum verification on every read                                  â”‚  â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘
â•‘                                                                               â•‘
â•‘  KEY DESIGN DECISIONS:                                                        â•‘
â•‘  â€¢ Metadata Store (0.68 TB) separate from Data Store (100 PB)                â•‘
â•‘  â€¢ Data before metadata on upload (orphaned data > dangling pointers)        â•‘
â•‘  â€¢ Append-only files on disk (sequential writes, no inode exhaustion)        â•‘
â•‘  â€¢ Replication for hot data, erasure coding for cold data                    â•‘
â•‘  â€¢ Versioning via delete markers (safe delete, history preserved)            â•‘
â•‘  â€¢ Multipart upload for large files (parallel, resumable)                    â•‘
â•‘  â€¢ Garbage collection: compact files, clean orphans                          â•‘
â•‘                                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```
