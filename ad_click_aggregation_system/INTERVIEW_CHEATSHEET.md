# ğŸ“Š Ad Click Event Aggregation - Interview Cheatsheet

> Based on Alex Xu's System Design Interview Volume 2 - Chapter 6

## Quick Reference Card

| Component | Purpose | Storage | Key Points |
|-----------|---------|---------|------------|
| **Log Watcher** | Collects click events from ad servers | N/A | Runs on every ad-serving server |
| **Message Queue (Kafka)** | Buffers events between services | Disk (partitions) | Decouples ingestion from processing |
| **Aggregation Service** | MapReduce on streaming data | In-memory + checkpoints | Map â†’ Aggregate â†’ Reduce |
| **Raw Data DB** | Stores every raw click event | Cassandra | Source of truth for recalculation |
| **Aggregation DB** | Stores pre-computed results | Cassandra | Serves the query/dashboard API |
| **Query Service** | Serves dashboard API | N/A | Reads from Aggregation DB only |
| **Recalculation Service** | Replays raw data to fix results | N/A | Used when aggregation logic had a bug |

---

## The Story: Building an Ad Click Event Aggregation System

Let me walk you through how we'd build a system to aggregate billions of ad clicks in real-time for billing and analytics.

---

## 1. What Are We Building? (Requirements)

### Functional Requirements

- Aggregate the number of clicks for an `ad_id` in the last M minutes
- Return the **top 100 most clicked** `ad_id` every minute
- Support aggregation **filtering by attributes** (country, device, ad format)
- Handle dataset at Facebook/Google scale

### Non-Functional Requirements

- **Correctness:** Aggregation results must be accurate (used for billing and RTB)
- **Handle delayed/duplicate events:** Late-arriving and duplicate clicks must be handled properly
- **Robustness:** Resilient to partial failures
- **Latency:** End-to-end latency should be a few minutes at most

### Back-of-the-Envelope Estimation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DAU:                1 billion                               â”‚
â”‚  Clicks per user/day: 1                                      â”‚
â”‚  Daily ad clicks:    1 billion                               â”‚
â”‚                                                              â”‚
â”‚  Avg QPS:            1B / 100K seconds = ~10,000 QPS         â”‚
â”‚  Peak QPS:           5x average = ~50,000 QPS                â”‚
â”‚                                                              â”‚
â”‚  Event size:         ~0.1 KB                                 â”‚
â”‚  Daily storage:      0.1 KB Ã— 1B = 100 GB                   â”‚
â”‚  Monthly storage:    ~3 TB                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. API Design

### Query APIs (read from Aggregation DB)

```
GET /ads/{ad_id}/aggregated_count
  Params: from, to (minute timestamps)
  Returns: { ad_id, count }

  Example: GET /ads/ad123/aggregated_count?from=2024-01-01T10:00&to=2024-01-01T10:05
  Response: { "ad_id": "ad123", "count": 4523 }


GET /ads/popular_ads
  Params: window_size (minutes), top_n (default 100)
  Returns: [ { ad_id, count }, ... ]

  Example: GET /ads/popular_ads?window_size=5&top_n=100
  Response: [ { "ad_id": "ad3", "count": 12000 }, { "ad_id": "ad1", "count": 9500 }, ... ]


GET /ads/{ad_id}/aggregated_count?filter_id=country:US
  Params: from, to, filter_id
  Returns: { ad_id, count, filter_id }
```

---

## 3. Data Model

### Raw Data (every click event stored as-is)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Raw Click Event                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ad_id       â”‚  "ad_001"                               â”‚
â”‚  timestamp   â”‚  2024-01-01 10:01:35                    â”‚
â”‚  user_id     â”‚  "user_789"                             â”‚
â”‚  ip          â”‚  "203.0.113.42"                         â”‚
â”‚  country     â”‚  "US"                                   â”‚
â”‚  device      â”‚  "mobile"                               â”‚
â”‚  ad_format   â”‚  "banner"                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Storage: Cassandra (write-heavy, append-only)
Partition key: ad_id
Clustering key: timestamp (DESC)
```

### Aggregated Data (pre-computed results)

```
Table: ad_click_counts
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ad_id   â”‚  click_minute â”‚  count â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ad_001  â”‚  1704103260   â”‚  352   â”‚
â”‚  ad_002  â”‚  1704103260   â”‚  198   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Table: ad_click_counts_filtered
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ad_id   â”‚  click_minute â”‚  count â”‚  filter_id  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ad_001  â”‚  1704103260   â”‚  210   â”‚  country:US â”‚
â”‚  ad_001  â”‚  1704103260   â”‚  142   â”‚  country:UK â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Table: most_clicked_ads
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  window_size â”‚  update_time_minuteâ”‚  most_clicked_adsâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1           â”‚  1704103260        â”‚  [ad3, ad1, ad2] â”‚
â”‚  5           â”‚  1704103260        â”‚  [ad1, ad3, ad5] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> **Why store aggregated data separately?**
> Querying raw data for "clicks in last 5 minutes" at 10K QPS would mean scanning
> billions of rows every request. Pre-computed aggregates make queries O(1) lookups.

---

## 4. The Big Picture (High-Level Architecture)

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         AD CLICK EVENT AGGREGATION - HIGH-LEVEL DESIGN                           â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                                   â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â•‘
â•‘  â”‚ Log Watcher  â”‚â”€â”€â”€â”€â–¶â”‚  Message     â”‚â”€â”€â”€â”€â–¶â”‚  Data Aggregation   â”‚               â•‘
â•‘  â”‚ (on ad       â”‚  â‘   â”‚  Queue       â”‚  â‘¡  â”‚  Service            â”‚               â•‘
â•‘  â”‚  servers)    â”‚     â”‚  (Kafka)     â”‚     â”‚  (MapReduce)        â”‚               â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â•‘
â•‘                              â”‚                        â”‚                           â•‘
â•‘                              â”‚ â‘¢                      â”‚ â‘£                        â•‘
â•‘                              â–¼                        â–¼                           â•‘
â•‘                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â•‘
â•‘                     â”‚  DB Writer     â”‚     â”‚  Message Queue    â”‚                  â•‘
â•‘                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  (Kafka)          â”‚                  â•‘
â•‘                              â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â•‘
â•‘                              â–¼                      â”‚ â‘¤                          â•‘
â•‘                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â–¼                             â•‘
â•‘                     â”‚  Raw Data DB   â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â•‘
â•‘                     â”‚  (Cassandra)   â”‚     â”‚  DB Writer     â”‚                    â•‘
â•‘                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                    â•‘
â•‘                              â†‘                      â”‚                             â•‘
â•‘                              â”‚ â‘¦                    â–¼                            â•‘
â•‘                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â•‘
â•‘                     â”‚ Recalculation  â”‚     â”‚ Aggregation DB â”‚                    â•‘
â•‘                     â”‚ Service        â”‚     â”‚ (Cassandra)    â”‚                    â•‘
â•‘                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                    â•‘
â•‘                                                     â”‚                             â•‘
â•‘                                                     â–¼                             â•‘
â•‘                                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â•‘
â•‘                                            â”‚ Query Service  â”‚                    â•‘
â•‘                                            â”‚ (Dashboard)    â”‚                    â•‘
â•‘                                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â•‘
â•‘                                                                                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Flow:
â‘  Log Watcher collects click events â†’ publishes to Kafka
â‘¡ Aggregation Service consumes events â†’ runs MapReduce
â‘¢ DB Writer stores raw events in Raw Data DB (for recalculation)
â‘£ Aggregation results published to downstream Kafka topic
â‘¤ DB Writer consumes aggregated results â†’ writes to Aggregation DB
â‘¥ Query Service reads from Aggregation DB â†’ serves dashboard
â‘¦ Recalculation Service reads raw data â†’ replays through aggregation
```

> **Why two Kafka queues?** The first Kafka buffers raw events (high throughput ingestion).
> The second Kafka buffers aggregation results (decouples aggregation from DB writes).
> This ensures the aggregation service isn't blocked by slow DB writes.

---

## 5. Deep Dive: Aggregation Pipeline

Now let's walk through how a click event flows through the system, step by step.

**The journey of a click event:**
> â‘  Event arrives â†’ â‘¡ Assign to time window (Section 6) â†’ â‘¢ Aggregate with MapReduce (below)
> â†’ â‘£ Apply filters (Section 10) â†’ â‘¤ Handle late/duplicate events (Section 8)
> â†’ â‘¥ Commit results exactly-once (Section 7) â†’ â‘¦ If something was wrong, recalculate (Section 9)

### Step 1: How Do We Aggregate? (MapReduce)

#### Use Case 1: Aggregate Click Count Per Ad

```
Input: All click events in a 1-minute window

         Inputs                Map              Aggregate           Output
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Click events â”‚    â”‚ Group by     â”‚    â”‚ Count per    â”‚    â”‚ ad3: 4 clicksâ”‚
    â”‚ (mixed ads)  â”‚â”€â”€â”€â–¶â”‚ ad_id        â”‚â”€â”€â”€â–¶â”‚ ad_id        â”‚â”€â”€â”€â–¶â”‚ ad1: 3 clicksâ”‚
    â”‚              â”‚    â”‚              â”‚    â”‚              â”‚    â”‚ ad2: 2 clicksâ”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Example:
  Events: [ad3, ad1, ad3, ad2, ad1, ad3, ad1, ad2, ad3]

  Map:    ad1 â†’ [click, click, click]
          ad2 â†’ [click, click]
          ad3 â†’ [click, click, click, click]

  Aggregate: ad1 = 3, ad2 = 2, ad3 = 4
```

#### How Are Events Distributed to Nodes?

```
Kafka partitions determine which node gets which events:

  Raw events published to Kafka with key = ad_id
  â†’ hash(ad_id) % num_partitions â†’ partition assignment
  â†’ Each aggregation node consumes one or more partitions

  Example (6 partitions, 3 nodes):
    Node 1 consumes: Partition 0, 1  â†’ gets events for ad3, ad6, ad9, ad12, ad15
    Node 2 consumes: Partition 2, 3  â†’ gets events for ad1, ad4, ad7, ad10, ad13
    Node 3 consumes: Partition 4, 5  â†’ gets events for ad2, ad5, ad8, ad11, ad14

  Key property: Same ad_id ALWAYS goes to the same node
  â†’ Per-ad count is computed entirely within one node (no cross-node shuffle!)
  â†’ Only top-N requires merging across nodes (reduce step)
```

#### Use Case 2: Top N Most Clicked Ads (with Reduce)

```
Input: All click events in a 1-minute window, distributed across nodes

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Kafka Topic â”‚
    â”‚ (partitioned â”‚
    â”‚  by ad_id)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           Each node consumes its
    â–¼      â–¼              â–¼           assigned Kafka partitions
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Node 1 (P0, P1)         â”‚ â”‚ Node 2 (P2, P3)         â”‚ â”‚ Node 3 (P4, P5)         â”‚
â”‚                          â”‚ â”‚                          â”‚ â”‚                          â”‚
â”‚ MAP (in-memory):         â”‚ â”‚ MAP (in-memory):         â”‚ â”‚ MAP (in-memory):         â”‚
â”‚   group by ad_id         â”‚ â”‚   group by ad_id         â”‚ â”‚   group by ad_id         â”‚
â”‚   ad3â†’[click,click,..]  â”‚ â”‚   ad1â†’[click,click,..]  â”‚ â”‚   ad2â†’[click,click,..]  â”‚
â”‚   ad6â†’[click,..]        â”‚ â”‚   ad4â†’[click,..]        â”‚ â”‚   ad5â†’[click,..]        â”‚
â”‚                          â”‚ â”‚                          â”‚ â”‚                          â”‚
â”‚ AGGREGATE (in-memory):   â”‚ â”‚ AGGREGATE (in-memory):   â”‚ â”‚ AGGREGATE (in-memory):   â”‚
â”‚   ad3:12, ad6:5, ad9:3  â”‚ â”‚   ad1:9, ad4:4, ad7:3   â”‚ â”‚   ad2:8, ad5:4, ad8:3   â”‚
â”‚                          â”‚ â”‚                          â”‚ â”‚                          â”‚
â”‚ LOCAL TOP-3 (min-heap):  â”‚ â”‚ LOCAL TOP-3 (min-heap):  â”‚ â”‚ LOCAL TOP-3 (min-heap):  â”‚
â”‚   ad3:12, ad6:5, ad9:3  â”‚ â”‚   ad1:9, ad4:4, ad7:3   â”‚ â”‚   ad2:8, ad5:4, ad8:3   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                            â”‚                            â”‚
             â”‚    â† No network hop above (Map + Aggregate on same node)
             â”‚    â† Only THIS step below requires network (tiny top-3 lists)
             â”‚                            â”‚                            â”‚
                â”‚  â† Only THIS step requires network!
                â”‚    Each node sends its tiny top-3 list
                â”‚    (3 entries, not millions of events)
                â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ REDUCE     â”‚                REDUCE: merge 3 local top-3 lists
        â”‚ Global     â”‚                (9 candidates total) into a
        â”‚ Top 3:     â”‚                MIN-HEAP of size 3 â†’ final answer
        â”‚ ad3: 12    â”‚
        â”‚ ad1: 9     â”‚
        â”‚ ad2: 8     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> **No shuffle needed (unlike Hadoop MapReduce):**
> In Hadoop, there's a costly "shuffle" phase that redistributes data between
> map and reduce nodes over the network. Here, Kafka's partitioning by ad_id
> eliminates this entirely â€” all events for an ad_id are already on one node.
>
> **Data structure â€” Min-Heap of size N:**
> - Each node maintains a min-heap of size N (e.g., 3)
> - For every ad_id count: if count > heap.min(), replace the minimum
> - Memory: O(N) per node â€” only stores top-N, not all ad counts
> - Reduce merges K heaps of size N â†’ O(KÃ—N) candidates â†’ final heap of size N

---

## 6. Step 2: Which Time Window Does This Event Belong To?

Before we can aggregate, we need to decide: which minute bucket does each click belong to?

### Event Time vs Processing Time

```
Event time:      When the click actually happened (timestamp on the device)
Processing time: When the aggregation service processes the event

Example:
  User clicks ad at 10:01:00 (event time)
  Network delay...
  Event arrives at aggregation service at 10:03:30 (processing time)

  Which minute bucket does this click belong to?
  - Event time:      minute 10:01 âœ“ (correct â€” reflects reality)
  - Processing time: minute 10:03 âœ— (wrong â€” inflates 10:03, misses 10:01)
```

**We use event time.** It reflects when the click actually happened, which is what billing and analytics care about.

| | Event Time | Processing Time |
|---|---|---|
| **Accuracy** | Correct â€” reflects real world | Skewed by delays |
| **Late events** | Needs watermarking (events may arrive after window closes) | No late events (everything is "on time" by definition) |
| **Complexity** | Higher â€” must handle out-of-order events | Simpler â€” just process as they arrive |
| **Use case** | Billing, analytics (accuracy matters) | Monitoring, alerting (speed matters more than precision) |

> **Tradeoff:** Event time is harder (needs watermarking, out-of-order handling)
> but essential for correctness. Processing time is simpler but gives wrong results
> when events are delayed â€” unacceptable for ad billing.

### Tumbling Window vs Sliding Window

```
Tumbling Window (non-overlapping, fixed size):

  Time:  |  min 0  |  min 1  |  min 2  |  min 3  |
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  Window: [0,1)      [1,2)     [2,3)     [3,4)
  
  Each event belongs to exactly ONE window.
  Simple: just bucket by minute.


Sliding Window (overlapping, "last M minutes"):

  Time:  |  min 0  |  min 1  |  min 2  |  min 3  |
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  "Last 3 minutes" at minute 3:  [1, 2, 3)
  "Last 3 minutes" at minute 4:  [2, 3, 4)

  Trick: Build from tumbling windows!
  sliding_count(last 3 min at minute 3) = tumbling[1] + tumbling[2] + tumbling[3]
```

> **Why tumbling first?** Store 1-minute tumbling window results in the DB.
> To answer "last M minutes," just SUM the last M tumbling buckets.
> This avoids re-aggregating raw events for every sliding window query.

---

## 7. Step 3: How Do We Guarantee Correctness? (Exactly-Once)

We've aggregated the events and assigned them to windows. But what if the aggregation service crashes mid-processing? We need to guarantee every click is counted exactly once â€” this is critical for billing.

### The Problem

If the aggregation service crashes mid-processing, we either:
- **Lose data** (if we committed offset before processing) â€” wrong billing!
- **Double count** (if we process but crash before committing offset) â€” wrong billing!

### The Solution: Atomic Commit

```
Upstream      Aggregation        HDFS/S3       Downstream
(Kafka)       Service                           (Kafka)
   â”‚               â”‚                â”‚               â”‚
   â”‚  1. Poll      â”‚                â”‚               â”‚
   â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                â”‚               â”‚
   â”‚  2. Consume   â”‚                â”‚               â”‚
   â”‚  from offset  â”‚                â”‚               â”‚
   â”‚  100          â”‚                â”‚               â”‚
   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚                â”‚               â”‚
   â”‚               â”‚  3.1 Verify    â”‚               â”‚
   â”‚               â”‚  offset        â”‚               â”‚
   â”‚               â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚               â”‚
   â”‚               â”‚                â”‚               â”‚
   â”‚               â”‚  3. Aggregate  â”‚               â”‚
   â”‚               â”‚  events        â”‚               â”‚
   â”‚               â”‚  100 to 110    â”‚               â”‚
   â”‚               â”‚                â”‚               â”‚
   â”‚               â”‚   â”Œâ”€ Distributed Transaction â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚               â”‚   â”‚                â”‚               â”‚   â”‚
   â”‚               â”‚   â”‚ 4. Send result â”‚               â”‚   â”‚
   â”‚               â”‚   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶   â”‚
   â”‚               â”‚   â”‚                â”‚               â”‚   â”‚
   â”‚               â”‚   â”‚ 5. Save offset â”‚               â”‚   â”‚
   â”‚               â”‚   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚               â”‚   â”‚
   â”‚               â”‚   â”‚                â”‚               â”‚   â”‚
   â”‚               â”‚   â”‚ 6. Ack back    â”‚               â”‚   â”‚
   â”‚               â”‚   â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   â”‚
   â”‚               â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚               â”‚                â”‚               â”‚
   â”‚  7. Ack with  â”‚                â”‚               â”‚
   â”‚  new offset   â”‚                â”‚               â”‚
   â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                â”‚               â”‚
   â”‚  110          â”‚                â”‚               â”‚

Steps 4, 5, 6 happen as a DISTRIBUTED TRANSACTION:
  - Send aggregation result to downstream Kafka
  - Save the new offset to HDFS/S3
  - If ANY step fails â†’ rollback â†’ restart from offset 100
  - If ALL succeed â†’ commit â†’ next batch starts from 110

Why distributed transaction?
  Without it, two things can go wrong:

  Case 1: Result sent (step 4) âœ“, but offset NOT saved (step 5) âœ—
    â†’ On restart, offset is still 100 â†’ reprocesses 100-110 â†’ DUPLICATE counts

  Case 2: Offset saved (step 5) âœ“, but result NOT sent (step 4) âœ—
    â†’ On restart, offset is 110 â†’ skips 100-110 â†’ LOST counts

  Both are unacceptable for billing. The distributed transaction ensures
  either BOTH happen or NEITHER happens â€” no partial state.
```

> **Why save offset to HDFS/S3 instead of Kafka?**
> Storing offset externally alongside the aggregation checkpoint allows
> us to verify consistency: "Did I already process offset 100-110?"
> On restart, check HDFS â†’ if offset 110 is saved, skip to 110.

---

## 8. Step 4: What About Late & Duplicate Events?

We chose event time for windowing (Section 6). But this creates a problem: events can arrive *after* their time window has already closed. And network retries can send the same event twice. How do we handle these?

### Late Events (Watermarking)

```
Problem: Events can arrive after their time window has closed.

  Real time:     min 0    min 1    min 2    min 3    min 4
                  â”‚        â”‚        â”‚        â”‚        â”‚
  Event arrives:  â”‚        â”‚        â”‚  â† Event from min 0 arrives here!
                  â”‚        â”‚        â”‚     (2 minutes late)

Solution: Watermark = "I believe all events up to time T have arrived"

  Watermark delay = 2 minutes

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Event time vs Watermark:                                  â”‚
  â”‚                                                            â”‚
  â”‚  Event â‰¥ watermark          â†’ ON TIME (process normally)  â”‚
  â”‚  Event â‰¥ watermark - delay  â†’ LATE but accepted           â”‚
  â”‚  Event < watermark - delay  â†’ TOO LATE, dropped           â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  Tradeoff: Longer watermark delay â†’ fewer dropped events
                                   â†’ higher end-to-end latency
```

> **Watermark does NOT solve everything.** Events with very long delays (e.g., device
> was offline for hours) will still be dropped. But designing a complex system for
> these low-probability events isn't worth the ROI. Instead, we correct the tiny
> inaccuracy with **end-of-day reconciliation** â€” a batch job that compares real-time
> aggregation results against raw data and fixes any drift (see Section 12).

### Duplicate Events (Deduplication)

```
Problem: Same click event sent twice (network retry, producer retry)

Solution options:
  1. Exact dedup: Store event_id in a short-lived cache (e.g., Redis)
     - Check "have I seen this event_id?" before counting
     - Expensive at scale (10K QPS Ã— event IDs to track)

  2. Approximate dedup: Use idempotent processing
     - aggregation is "count per ad_id per minute"
     - If same event counted twice in same window â†’ count is slightly off
     - Acceptable for analytics, NOT for billing

  3. Exactly-once via Kafka transactions (preferred)
     - Kafka producer dedup: producer_id + sequence_number
     - Combined with atomic commit â†’ end-to-end exactly-once
```

---

## 9. Step 5: What If We Got It Wrong? (Recalculation)

Even with exactly-once processing, the aggregation *logic itself* could have a bug â€” maybe we miscounted, applied wrong filters, or had a code defect. We need a way to recompute historical results. This is why we store raw events.

### Why Recalculation?

```
Scenario: A bug in the aggregation logic caused wrong counts for the last 3 days.
          Need to recompute all aggregated results.

  This is why we store raw events in a separate database!
```

### How It Works

```
Normal path (real-time):
  Log Watcher â†’ Kafka â†’ Aggregation Service â†’ Kafka â†’ Aggregation DB

Recalculation path (batch):
  Raw Data DB â†’ Recalculation Service â†’ Kafka â†’ Aggregation DB (overwrite)
       â”‚                 â”‚
       â”‚     â‘  Read raw events for affected time range
       â”‚     â‘¡ Replay through aggregation logic (fixed version)
       â”‚     â‘¢ Write corrected results to downstream Kafka
       â”‚     â‘£ DB Writer overwrites stale aggregation data

  The recalculation service is a separate instance of the
  aggregation service, running the corrected logic on historical data.
```

> **Key design decision:** Always store raw events even though they're expensive (100 GB/day).
> Without them, recalculation is impossible â€” you'd have to ask users to click again!

---

## 10. Step 6: Filtering by Attributes

So far we've been counting total clicks per ad. But advertisers also want breakdowns: "How many clicks from the US? From mobile devices?" This runs alongside the main aggregation pipeline.

### How Filtering Works

```
Raw event has attributes: country, device_type, ad_format

Instead of pre-computing ALL possible filter combinations (explosion!),
we use a "star schema" approach:

  Unfiltered:    ad_id + minute â†’ count
  Filter by 1:   ad_id + minute + filter_id â†’ count

  filter_id examples:
    "country:US"
    "device:mobile"
    "format:banner"

Aggregation Service computes:
  For each minute window:
    1. Total count per ad_id (unfiltered)
    2. Count per ad_id per country
    3. Count per ad_id per device
    4. Count per ad_id per format

This is O(N Ã— F) where F = number of filter dimensions (small, ~3-5)
```

> **Tradeoff:** Pre-computing filters increases processing and storage
> but makes queries instant (O(1) lookup instead of scanning raw data).

---

## 11. Scaling

### Message Queue (Kafka)

- Partition by `ad_id` hash â†’ events for same ad go to same partition
- Add more partitions for higher throughput
- Consumer group: one aggregation node per partition

**Topic Physical Sharding:**

A single Kafka topic may not be enough at scale. Split into multiple topics by:

```
By geography:     topic_north_america, topic_europe, topic_asia
By business type: topic_web_ads, topic_mobile_ads, topic_video_ads
```

| | Pros | Cons |
|---|---|---|
| **Sharded topics** | Higher throughput, faster consumer rebalancing (fewer consumers per topic) | Extra complexity, higher maintenance cost |
| **Single topic** | Simpler to manage | Rebalancing is slow with many consumers, single topic bottleneck |

### Aggregation Service

- Horizontally scalable â€” each node processes a subset of partitions
- Stateless processing (state is in Kafka offsets + checkpoints)
- Scale by adding more aggregation nodes + Kafka partitions

### Database

- **Raw Data DB (Cassandra):** Partition by `ad_id`, clustering key `timestamp` (DESC)
  - Write-heavy (100 GB/day), append-only â†’ Cassandra's sweet spot
  - TTL-based cleanup (retain 2-4 weeks for recalculation)

- **Aggregation DB (Cassandra):** Partition by `ad_id`, clustering key `click_minute`
  - Read pattern is simple **point queries**: `SELECT count WHERE ad_id = X AND minute = Y`
  - No complex joins, no full-table scans â†’ Cassandra handles point reads well
  - Same technology as Raw Data DB â†’ simpler operations (one DB to manage)
  - **Alternative:** Could use Redis for faster reads if latency is critical,
    but Cassandra already serves point queries in single-digit ms

### Hotspot Issue

```
Problem: A viral ad gets millions of clicks â†’ single partition overloaded

Solutions:
  1. Add extra partitions: Break hot ad_id across sub-partitions
     Key: ad_id + random_suffix (e.g., "ad123_0", "ad123_1", ... "ad123_9")
     â†’ Spreads load across 10 partitions
     â†’ Query must aggregate across all sub-partitions (slightly more complex)

  2. Dedicated aggregation node: Route hot ad_ids to beefier machines
  
  3. Pre-aggregation at Log Watcher: Batch clicks locally before
     sending to Kafka (reduces QPS for hot ads)
```

---

## 12. What Can Go Wrong? (Failure Handling)

### Aggregation Service Crash

**Scenario:** Aggregation node crashes mid-processing
**Solution:**
- Restart from last committed offset (stored in HDFS/S3)
- Atomic commit ensures no partial results are visible
- Another node in the consumer group picks up the orphaned partitions

### Kafka Failure

**Scenario:** Kafka broker goes down
**Solution:**
- Kafka replication (ISR) ensures data is not lost
- Producers retry; consumers reconnect to new leader broker
- See Distributed Message Queue cheatsheet for details

### Database Write Failure

**Scenario:** Aggregation DB is temporarily unavailable
**Solution:**
- Results stay in downstream Kafka topic (buffered)
- DB Writer retries when database recovers
- No data loss because Kafka retains messages

### Incorrect Aggregation (Bug)

**Scenario:** Aggregation logic had a bug
**Solution:**
- Recalculation Service replays raw data with fixed logic
- Overwrites incorrect results in Aggregation DB
- This is why raw data storage is critical

### Monitoring & Reconciliation

**Why reconciliation?** Real-time streaming can silently produce wrong results â€”
dropped events, watermark cutoffs, race conditions, or subtle bugs. Without
verification, you'd never know billing is off by 2%.

**How it works:** Run a batch job (end-of-day) that re-aggregates from raw data
and compares against the real-time results. If they differ â†’ alert and fix.

```
Example: Reconciliation for ad_001 on Jan 1st, minute 10:00

  Real-time aggregation result:     ad_001, minute 10:00 â†’ 4,523 clicks
  Batch reconciliation (from raw):  ad_001, minute 10:00 â†’ 4,531 clicks
                                                            â”€â”€â”€â”€â”€
                                                   Diff:     8 clicks missing!

  Why? 8 events arrived after the watermark cutoff and were dropped.

  Action:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Diff < threshold (e.g., 0.1%)?                                 â”‚
  â”‚    YES â†’ Log it, acceptable drift, no action                    â”‚
  â”‚    NO  â†’ Alert! Investigate cause. Options:                     â”‚
  â”‚          1. Patch the 8 missing clicks into Aggregation DB      â”‚
  â”‚          2. Trigger full recalculation for the affected window  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  Schedule: Run reconciliation daily (or every few hours for critical data)
  Compare: real-time counts vs batch counts for every (ad_id, minute) pair
```

> **This is the safety net.** Watermarking handles most late events, exactly-once
> handles crashes, but reconciliation catches everything else â€” the final
> guarantee that billing numbers are correct.

---

## 13. Lambda vs Kappa Architecture

### The Problem: Real-Time Alone Isn't Always Enough

```
Scenario: Your real-time streaming aggregation has been running for months.
One day, you discover a bug in the filtering logic â€” country "GB" was
being counted as "UK" for the last 3 days. All filtered counts are wrong.

  Options:
  A. Reprocess 3 days of raw data through the SAME streaming pipeline (Kappa)
  B. Have a SEPARATE batch pipeline that periodically reprocesses everything (Lambda)
```

### Lambda Architecture

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚      Batch Layer         â”‚
              â”Œâ”€â”€â”€â”€â–¶â”‚  (Batch Engine: Spark)   â”‚â”€â”€â”€â”€â”
              â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
              â”‚                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”´â”€â”€â”€â”                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”
â”‚Eventsâ”‚â”€â–¶â”‚ Kafka â”‚                          â”‚ Serving  â”‚â”€â”€â”€â”€â–¶â”‚ Query â”‚
â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”˜                          â”‚ Backend  â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                                     â–²
              â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
              â””â”€â”€â”€â”€â–¶â”‚    Streaming Layer       â”‚â”€â”€â”€â”€â”˜
                    â”‚  (Flink / Spark Stream)  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚ Raw Data â”‚          â”‚ Results  â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**How it works (with example):**

```
Two parallel paths running ALL the time:

  Path 1 â€” Streaming (real-time, approximate):
    Click at 10:01 â†’ Flink aggregates â†’ "ad_001: 4,523 clicks" â†’ Dashboard shows instantly
    May miss late events, may have small inaccuracies

  Path 2 â€” Batch (every few hours, accurate):
    Spark reads ALL raw data from 00:00 to now â†’ recomputes from scratch
    "ad_001: 4,531 clicks" â†’ overwrites the streaming result

  Serving layer: Uses streaming result for recent data (fast),
                 switches to batch result once batch catches up (accurate)

  Pros: Batch corrects streaming errors automatically
  Cons: TWO codebases doing the same thing (Flink code + Spark code)
        Must ensure both produce identical results â€” hard to maintain
```

### Kappa Architecture (Preferred)

```
â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”
â”‚Eventsâ”‚â”€â–¶â”‚ Kafka â”‚â”€â–¶â”‚    Streaming Layer       â”‚â”€â–¶â”‚ Serving  â”‚â”€â–¶â”‚ Query â”‚
â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  (Flink / Spark Stream)  â”‚  â”‚ Backend  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚ Raw Data â”‚          â”‚ Results  â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**How it works (with example):**

```
Single path â€” streaming only:

  Normal:       Click at 10:01 â†’ Flink aggregates â†’ Dashboard (same as Lambda)

  Bug found:    "Country GB was counted as UK for last 3 days!"

  Recalculate:  Read 3 days of raw data from Raw Data DB
                â†’ Feed it through the SAME Flink pipeline (with fixed code)
                â†’ Overwrite incorrect results in Aggregation DB

  Same code handles both real-time AND recalculation.
  No separate batch codebase to maintain.

  Pros: Single codebase, no sync issues
  Cons: Replaying 3 days through streaming can be slow
        (but recalculation is rare â€” only on bugs)
```

| | Lambda | Kappa |
|---|---|---|
| **Codebases** | Two (batch + stream) â€” must produce same results | One (stream only) |
| **Accuracy** | Batch auto-corrects streaming drift | Relies on reconciliation to catch drift |
| **Recalculation** | Always running (batch reprocesses periodically) | On-demand (replay raw data when needed) |
| **Complexity** | High â€” two systems to maintain | Lower â€” single pipeline |
| **Best for** | Systems where batch is already needed (ML training + serving) | Systems where recalculation is rare (our case) |

> **For this system:** Kappa is preferred. Recalculation is rare (only on bugs),
> and we already store raw data for replay. Reconciliation catches drift.
> No need for a permanent batch layer running 24/7.

---

## 14. Why These Choices? (Key Design Decisions)

### Decision #1: Aggregate Then Store (Not Raw Query)

**Problem:** How to serve "clicks in last 5 min" at 10K QPS?

**Why pre-aggregate:** Querying billions of raw rows per request is impossible.
Pre-compute minute-level counts, then SUM last M buckets for sliding window.
Query becomes O(M) lookups instead of scanning billions of rows.

### Decision #2: Two Kafka Queues (Not One)

**Problem:** Can we use a single Kafka between log watcher and DB writer?

**Why two:** The aggregation service is CPU-intensive (MapReduce). If it writes
directly to the DB, a slow DB blocks aggregation. Two Kafkas decouple:
- First Kafka: raw event buffering (high throughput)
- Second Kafka: aggregation result buffering (decouples from DB writes)

### Decision #3: Cassandra for Both Databases

**Problem:** Which database for raw data and aggregation results?

**Why Cassandra:**
- **Raw Data DB:** Write-heavy (100 GB/day append-only), time-series-like, TTL support
- **Aggregation DB:** Write from batch, read from query service, simple key-value lookups
- Both benefit from Cassandra's linear scalability and tunable consistency

### Decision #4: Kappa Over Lambda

**Problem:** How to handle both real-time and batch recalculation?

**Why Kappa:** Single codebase (stream processing) handles both. Recalculation
replays raw data through the same stream pipeline. Lambda would require
maintaining two separate codebases (batch + stream) that must produce identical results.

### Decision #5: Store Raw Data Despite Cost

**Problem:** Raw data is 100 GB/day (3 TB/month). Why not discard after aggregation?

**Why keep:** Recalculation is impossible without raw data. If aggregation logic
has a bug, we'd lose billing accuracy permanently. 3 TB/month is cheap
compared to incorrect ad billing at scale.

---

## 15. Interview Pro Tips

### Opening Statement
"An ad click aggregation system ingests billions of click events through Kafka, runs MapReduce-style aggregation in minute-level windows, and stores pre-computed results for fast dashboard queries. The key challenges are exactly-once processing for billing accuracy, handling late-arriving events via watermarking, and supporting data recalculation when bugs are discovered."

### Key Talking Points
1. **Two Kafkas:** Decouple ingestion from aggregation from DB writes
2. **MapReduce:** Map (group by ad_id) â†’ Aggregate (count) â†’ Reduce (top N)
3. **Tumbling â†’ Sliding:** Store 1-min buckets, SUM for sliding windows
4. **Exactly-Once:** Atomic commit (result + offset together)
5. **Watermarking:** Handle late events with configurable delay tolerance
6. **Recalculation:** Replay raw data through fixed logic
7. **Hotspot:** Sub-partition hot ad_ids to spread load

### Common Follow-ups

**Q: Why not use a stream processing framework like Flink directly?**
A: We would! Flink or Spark Streaming would implement the MapReduce logic. The architecture describes the overall system; Flink is the implementation choice for the Aggregation Service.

**Q: How do you handle a viral ad that gets 100x normal clicks?**
A: Sub-partition the hot ad_id (e.g., "ad123_0" through "ad123_9"), spread across 10 partitions. Pre-aggregate at the log watcher level to batch clicks before sending to Kafka.

**Q: Why Cassandra and not a time-series DB like InfluxDB?**
A: Either would work for the aggregation DB. Cassandra is preferred because it handles both raw data (write-heavy) and aggregated data (read-heavy) with a single technology stack, simplifying operations.

**Q: What if the recalculation takes too long?**
A: Parallelize â€” partition raw data by time range, run multiple recalculation instances. Since data is in Kafka/Cassandra, it can be read in parallel.

**Q: How do you ensure the dashboard shows consistent data during recalculation?**
A: Write recalculated results to a shadow table first. Once complete, atomically swap the shadow table with the live table (blue-green deployment for data).

---

## 16. Visual Architecture Summary

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              AD CLICK AGGREGATION - COMPLETE FLOW                                â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                                   â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â•‘
â•‘  â”‚   Log    â”‚â”€â”€â–¶â”‚  Kafka  â”‚â”€â”€â–¶â”‚ Aggregation  â”‚â”€â”€â–¶â”‚  Kafka  â”‚â”€â”€â–¶â”‚  DB Writer  â”‚ â•‘
â•‘  â”‚ Watcher  â”‚   â”‚  (raw)  â”‚   â”‚  Service     â”‚   â”‚ (agg)   â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â”‚ (MapReduce)  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚        â•‘
â•‘                       â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â–¼        â•‘
â•‘                       â”‚                                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘
â•‘                       â–¼                                       â”‚ Aggregation  â”‚  â•‘
â•‘                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                 â”‚ DB           â”‚  â•‘
â•‘                 â”‚ DB Writer â”‚                                 â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘
â•‘                 â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                                        â”‚          â•‘
â•‘                       â”‚                                              â–¼          â•‘
â•‘                       â–¼                                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘
â•‘                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚   Query      â”‚  â•‘
â•‘                 â”‚ Raw Data  â”‚â—€â”€â”€â”€â”€â”‚  Recalculation   â”‚        â”‚   Service    â”‚  â•‘
â•‘                 â”‚ DB        â”‚     â”‚  Service          â”‚        â”‚  (Dashboard) â”‚  â•‘
â•‘                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘
â•‘                                                                                   â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                                   â•‘
â•‘  KEY FLOWS:                                                                       â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                                       â•‘
â•‘  â‘  Real-time: LogWatcher â†’ Kafka â†’ Aggregation â†’ Kafka â†’ DB Writer â†’ Agg DB    â•‘
â•‘  â‘¡ Query:     Dashboard â†’ Query Service â†’ Aggregation DB                         â•‘
â•‘  â‘¢ Recalc:    Raw Data DB â†’ Recalculation Service â†’ Kafka â†’ Agg DB (overwrite)  â•‘
â•‘                                                                                   â•‘
â•‘  CRITICAL DESIGN DECISIONS:                                                       â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                       â•‘
â•‘  â€¢ Two Kafkas: decouple ingestion â†’ aggregation â†’ DB writes                      â•‘
â•‘  â€¢ MapReduce: Map(ad_id) â†’ Aggregate(count) â†’ Reduce(top N)                     â•‘
â•‘  â€¢ Tumbling windows: 1-min buckets, SUM for sliding window queries               â•‘
â•‘  â€¢ Exactly-once: atomic commit (result + offset)                                 â•‘
â•‘  â€¢ Watermark: accept late events within tolerance, drop the rest                 â•‘
â•‘  â€¢ Raw data stored: enables recalculation when bugs found                        â•‘
â•‘  â€¢ Kappa architecture: single streaming path, replay for batch                   â•‘
â•‘                                                                                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```
